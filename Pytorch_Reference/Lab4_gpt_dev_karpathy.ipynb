{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpXpmjEYC_T"
      },
      "source": [
        "## Building a GPT by Andrej Karpathy\n",
        "\n",
        "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT.\n",
        "\n",
        "See [his youtube channel](https://www.youtube.com/@AndrejKarpathy) for live coding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5hjCcLDr2WC",
        "outputId": "48d12f8b-9b92-43a9-f33c-cd2431b19a26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-08-14 11:41:32--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-08-14 11:41:32 (46.9 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "9c6713d2-6c28-4e26-9c43-10048297a812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t22tBLJSMzO",
        "outputId": "2bd7f291-1001-465e-b294-fb7d29cf1421"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('F',\n",
              " 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text[0], text[0:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "b2383f2e-35b6-449d-9cc7-b70bb7344c25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "25e58247-da07-4082-e94b-828ff1277776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ],
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "d6de8074-479f-4929-c651-b894e30ec099"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evWsqmbUSrcy",
        "outputId": "c89d5165-a08f-4fc3-a8b0-d1e3705a9588"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(46, 'h')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stoi[\"h\"], itos[46]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "9011d3fb-4b57-4443-bc8c-46fd832ae88a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ],
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "outputs": [],
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n] # 90% train data\n",
        "val_data = data[n:] # 10% val data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jx_6PygqTXvp",
        "outputId": "4db9ceb3-344e-418c-fae4-97d0897aa719"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1003854"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tsszr20TZ9r",
        "outputId": "07baac1d-fff1-41ec-e203-e44586d7d439"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([1003854]), tensor([18, 47, 56,  ..., 43, 56, 43]))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.shape, train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "da710b9b-0e34-46f6-aab7-236d38a6db2b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "fcda44f6-29b8-4b11-df5d-9b50b67d3dab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "b0015c67-0499-4ce4-ed37-b22581d77285"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start tensor([ 76049, 234249, 934904, 560986]) tensor([24, 44, 52, 25])\n",
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # batch_size 크기만큼 문자열 start 크기 지정\n",
        "    print(\"start\", ix, data[ix])\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]) # torch.stack -> tensor간에 연결을 해줌\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyiG09cKXn-z",
        "outputId": "14e07a68-1fa9-43d3-aa8f-e5810131e018"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1003854"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybjpoZuaXhhu",
        "outputId": "c1270d3f-1eca-416c-d20b-0097e30f49c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([971401, 579495, 193625, 348340])\n"
          ]
        }
      ],
      "source": [
        "ix = torch.randint(len(train_data) - block_size, (batch_size,))\n",
        "print(ix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "a21472e9-34ee-49cd-cca1-8c18a54b1148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ],
      "source": [
        "print(xb) # our input to the transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "7c53f982-472c-49ba-a007-e7f86f3a942c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "tensor([[[ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,\n",
            "           0.0643,  0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398,\n",
            "          -0.9211,  1.5433,  1.3488, -0.1396,  0.2858,  0.9651, -2.0371,\n",
            "           0.4931,  1.4870,  0.5910,  0.1260, -1.5627, -1.1601, -0.3348,\n",
            "           0.4478, -0.8016,  1.5236,  2.5086, -0.6631, -0.2513,  1.0101,\n",
            "           0.1215,  0.1584,  1.1340, -1.1539, -0.2984, -0.5075, -0.9239,\n",
            "           0.5467, -1.4948, -1.2057,  0.5718, -0.5974, -0.6937,  1.6455,\n",
            "          -0.8030,  1.3514, -0.2759, -1.5108,  2.1048,  2.7630, -1.7465,\n",
            "           1.4516, -1.5103,  0.8212, -0.2115,  0.7789,  1.5333,  1.6097,\n",
            "          -0.4032, -0.8345]]], grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "S\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "        # B : Batch Size\n",
        "        # T : Sequence Length\n",
        "        # C : Embedding Dimenssion\n",
        "        #print(\"debug\")\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            print(logits)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "            break\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "u-PEHj7uvu6x"
      },
      "outputs": [],
      "source": [
        "idx = torch.zeros((1, 1), dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDUyrilivwyI",
        "outputId": "f96e63df-0b2b-4e22-b16d-6c6bfe214776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0]])\n"
          ]
        }
      ],
      "source": [
        "print(idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start tensor([450509,  54109, 795768, 802784, 132415, 641849,  46397, 609971,  80982,\n",
            "        191857, 347982, 818779, 804295,  28053, 874504, 916748, 184513, 640951,\n",
            "         27907, 585404, 972303, 684922, 886902,  58869, 272999, 667308, 830228,\n",
            "        554272, 549829, 499783, 611871, 610733]) tensor([58,  1, 43, 59, 53, 59, 59, 46,  1, 59, 39, 59,  1,  1,  1, 42, 60, 58,\n",
            "        43, 39, 47, 47, 63, 46,  1, 41, 39,  0, 39, 57,  1, 40])\n",
            "start tensor([  5190, 821228, 854432, 957903, 861616, 824029, 276429, 420942, 894612,\n",
            "        424333, 782494, 812821, 851312, 897037, 495620,  99384, 776911, 831423,\n",
            "        780652, 159331, 841507, 676302,  15903, 705711, 949596, 897172, 158065,\n",
            "        309484,  46972, 251593, 467457, 451129]) tensor([61, 59, 42,  0, 57, 57,  1, 47, 42, 20, 35, 52,  1,  1,  1, 57, 53,  1,\n",
            "        33, 47, 46, 18, 21, 19, 57, 51,  1, 44, 58, 63, 43,  6])\n",
            "start tensor([999102, 490526, 354820, 906229, 405035, 102725, 371502, 246375,  76094,\n",
            "        882855, 909730, 894586, 537349, 695641, 233463, 942950, 851935, 325715,\n",
            "        974442, 523927, 140824, 429737, 683570, 760474, 768202, 967840,   3655,\n",
            "         75238, 278320, 384924,  33561, 477400]) tensor([61, 45, 59, 54,  1, 43, 43, 42, 54, 52, 63, 59, 56, 47, 63, 57, 57, 41,\n",
            "         1, 42, 18,  0, 57, 52, 43, 43, 61,  1,  1, 57, 43,  1])\n",
            "start tensor([514421, 782990,  10109, 880792, 620608, 758123, 761955, 483803,  61808,\n",
            "        476059, 726263, 656633, 674426, 241399, 395150, 298543, 938016, 254737,\n",
            "        952644,  24901, 772494, 628396, 556307, 319255,  38208, 849969, 394140,\n",
            "        919551, 316361, 359631, 121698, 567966]) tensor([40, 51, 57, 43, 53, 32, 46, 51, 58, 58,  1,  8,  1, 46, 46,  0,  0, 53,\n",
            "        53, 41, 39, 53, 28, 52, 39, 31, 63,  6, 46, 49, 39, 39])\n",
            "start tensor([743553, 884616, 849510, 793375, 924645,  86369, 919544, 299140, 227344,\n",
            "        860300, 216165, 551383, 555439, 641236, 435602, 759848, 700082, 918994,\n",
            "        468585, 273700,  95491, 765171, 359861, 736914, 144022, 105516, 477200,\n",
            "        784451, 896134, 934681, 376322, 502277]) tensor([35,  1, 53,  0, 58, 43, 62, 63, 63,  1, 53, 44, 39, 39, 46, 25,  1, 43,\n",
            "        51,  1,  1, 53,  1, 42, 41,  1,  1, 17, 59, 59, 54, 47])\n",
            "start tensor([ 204528,  202156,  543499,  800823,  930616,  153375,  834582,  860265,\n",
            "         296227,  993134,  806983,   98006,  172503,  194830,  353142,  954475,\n",
            "         301736,  781978, 1002223,  143971,  716105,  174172,  148687,  362485,\n",
            "         354704,  925089,  290609,  108944,  926805,  989259,  608356,  416002]) tensor([ 1, 31, 43, 63, 58,  2, 53, 43, 12, 46, 39, 43, 42, 56, 56, 46, 53, 53,\n",
            "        53, 46, 57, 21, 42, 63, 53, 53, 50, 56, 56, 43, 52,  1])\n",
            "start tensor([157300, 833640, 657117, 680430, 266960, 331912, 406615, 677171,  61696,\n",
            "        506942, 766551, 981292, 147889, 266774, 862035, 603096, 553078, 738498,\n",
            "        962424, 437998, 126441, 349794, 261665, 175719, 213589, 244115, 431387,\n",
            "        590558, 719326, 462052, 295611, 463623]) tensor([35, 43,  6, 46, 55, 53,  1, 42, 47, 39, 43,  1, 57, 47,  0,  6,  0, 54,\n",
            "        54, 53, 45, 27, 53, 51,  1, 63, 51, 30, 51, 53, 45,  1])\n",
            "start tensor([213216, 266343, 268166, 156407, 475683, 592933, 338218, 876527, 829282,\n",
            "        888254, 248968, 633776, 167546,  83501, 236419, 951212, 583918, 625225,\n",
            "        973616, 201135, 660186, 437321, 769547, 642680, 954330, 231823,  35428,\n",
            "        626574, 815010, 771323, 674051,  53631]) tensor([23, 21, 53, 10, 58, 19, 57, 63,  6, 47, 56, 21,  1, 63, 59, 51, 50, 46,\n",
            "        40, 58,  1, 51, 42, 56, 57, 57, 43, 58, 43, 53, 58, 47])\n",
            "start tensor([837493, 992538, 695904,  88751, 584216, 368832, 157590, 554612, 241879,\n",
            "        901854, 121138, 768224, 868342, 100862, 467341, 286745, 847209, 835563,\n",
            "         84405, 621744, 453023, 785984, 626166, 490613, 430440, 288076, 253589,\n",
            "        432120, 742944, 992506, 535015, 505529]) tensor([53, 43, 40, 47, 37,  1,  1, 42, 52, 63, 47, 39, 46, 17,  1, 52, 43, 59,\n",
            "        47,  1,  1, 59, 61, 56, 20, 56, 21, 52, 42, 46, 57, 59])\n",
            "start tensor([ 20040,  73777,  39029, 741485, 803341,  80592,  65748, 669302,  31787,\n",
            "        198391, 128563, 758707, 874333, 205979, 868572, 491937, 389567, 891829,\n",
            "        771507, 389895, 505459,   2092, 500753, 125475, 385021, 570354, 298509,\n",
            "        505276, 332012, 748881, 250527, 978752]) tensor([59, 59,  1, 52, 53,  1, 41, 43, 63, 42,  1, 50,  1,  1, 57,  1, 56, 53,\n",
            "        56, 39,  1,  1, 61, 47,  1, 59, 39, 63, 57, 53,  1, 58])\n",
            "start tensor([ 77785, 888649, 385053, 871498, 438579, 506909, 982796, 518693, 339779,\n",
            "        145004, 469067, 501844, 957414, 111439,  37424, 522166, 296778, 153346,\n",
            "        622374, 567431, 420269, 762093,   5914,  69130, 585623, 317665, 606753,\n",
            "        722305, 281222, 859789, 242325, 614128]) tensor([61, 58, 50, 63, 21, 27,  1,  0, 56, 52, 58, 53,  1, 46, 39, 13,  0, 24,\n",
            "        52,  1,  1, 21,  1,  1, 59, 39, 42, 47, 52, 53,  0, 52])\n",
            "start tensor([778900, 800589,  28717, 900908, 700791, 520674, 869468, 268606, 653686,\n",
            "        218969, 632670, 125498, 103671,  86398, 674125, 293347, 311892, 612919,\n",
            "        842239, 727511, 432463,   8001, 135831, 699726, 247388, 485963, 388182,\n",
            "        161406, 332513, 693109, 863378, 272701]) tensor([ 1, 15, 40, 12, 56, 50, 53, 58, 35, 58,  1,  1, 43, 45, 44, 63, 52, 56,\n",
            "        50, 50, 39, 13, 43, 46, 50,  1, 61, 59, 59,  6,  0, 43])\n",
            "start tensor([ 885316,  476357,  315031,   27345,  487567,  456530,  866607,  565563,\n",
            "         678534,  285052, 1003133,  444520,  369214,  174572,  639566,  247427,\n",
            "         279708,  129607,  807697,  858322,  945685,  487519,  886420,  187816,\n",
            "         643196,  616927,  100949,   42432,  943059,   30618,  600259,  581395]) tensor([ 1,  1, 58, 58, 42, 50, 46, 39, 52,  1, 32, 53, 58, 21, 58,  1, 58, 53,\n",
            "        57, 43, 12, 61, 60, 20, 10, 58, 53, 59, 56, 33, 43, 35])\n",
            "start tensor([491981, 770119, 846151, 420013, 785963, 565124, 600895, 338042, 842203,\n",
            "        726597, 851871, 508856, 978213, 624059, 914823, 557813, 918916, 576290,\n",
            "        738025, 631723, 130699, 225137, 801262,  31308, 859893,     64, 908534,\n",
            "        849922, 515772, 949432, 641208, 489423]) tensor([ 1,  1, 46, 51, 46, 47, 39,  1,  1,  6, 46, 30, 50, 39, 42, 13,  1, 57,\n",
            "        54, 61, 49, 43,  0, 47, 39, 50,  1,  1,  6, 43, 53, 57])\n",
            "start tensor([320319, 356277, 453112, 457125, 299301, 814363, 761903, 134862, 606534,\n",
            "         94704, 934793, 900617, 302547, 556556, 307317, 111496,  42627, 900172,\n",
            "        780813, 815899, 608809, 192717, 116502, 859904, 779783, 110336, 424484,\n",
            "        673727, 757478, 456702, 169210, 332420]) tensor([47,  1, 47, 47, 51, 43,  0,  1, 13, 50, 42, 43, 56, 39, 57, 43,  1, 57,\n",
            "         1, 46,  1, 56, 43, 58, 53, 58, 58, 61, 43, 25,  1,  1])\n",
            "start tensor([408494, 552819, 107657, 418682, 136084, 443096, 555019, 534379, 994077,\n",
            "        782129, 938875, 241948, 540577, 285610, 501168, 928516, 928465, 133499,\n",
            "        455488, 611137, 398322,  41482, 993931, 698438, 627794, 817546, 861773,\n",
            "         89163, 234655, 889972, 194075, 737666]) tensor([24,  1, 57, 53,  6, 25, 41, 58, 56, 43, 47, 56,  0, 63, 39,  1, 58, 56,\n",
            "        58,  1, 46,  0,  1, 47, 40, 63, 53, 58,  7, 35, 41, 39])\n",
            "start tensor([844534, 522200, 443461,  65586, 431794, 616384, 411977, 263736, 774649,\n",
            "        499407, 322881, 757997, 608502, 893152, 716830, 166664, 707455, 768693,\n",
            "        523253, 308484, 605042, 268596, 924110, 234354, 256047, 340536, 219372,\n",
            "          8368, 989564, 821639, 627115, 106169]) tensor([39, 42,  0, 63, 60, 42, 57, 44,  1, 57, 42, 51, 52, 50,  0, 60, 58, 43,\n",
            "         0, 59,  1, 46, 59,  0,  1, 52, 26,  1, 50, 53, 40, 56])\n",
            "start tensor([653086, 959094, 323711, 907170,  65447, 655546, 811984, 907943, 986008,\n",
            "        456005, 827067,  86635, 643112, 617120, 388714, 918634, 951006, 955258,\n",
            "        896737, 548973, 480827, 939949, 169906, 627579, 907274,  91625, 924084,\n",
            "        521620, 128250,   1679, 902035, 175296]) tensor([49, 42, 58, 63, 52,  1, 43, 43, 50, 46, 41, 42, 43, 45, 52,  1,  1, 57,\n",
            "        16, 46, 50, 58, 58, 53, 50, 43, 63, 39, 42, 58, 58,  1])\n",
            "start tensor([946284, 632336, 914190, 903158, 977241,  70546, 998793, 858347, 828694,\n",
            "        902133,   8871,  29760, 946390, 437753, 557193, 381035, 634023, 206457,\n",
            "        607766, 313980, 161994, 450339, 506165, 317427, 580592, 110190, 483301,\n",
            "        298030, 476023, 741746, 898782, 617827]) tensor([ 1, 41,  1, 42,  1, 56,  1, 43, 17, 41, 53, 52, 58, 63, 57, 39, 52, 53,\n",
            "        61,  6, 53, 58,  1,  1,  1, 43, 57, 59, 53, 39, 51, 52])\n",
            "start tensor([340751, 319776, 321115,  20489, 427595, 581436, 699240, 560537, 168103,\n",
            "        770243, 493882, 256497, 147645,  65672, 630117, 997490, 488793, 541389,\n",
            "        143839, 862245, 785419, 695637, 116417, 759263, 565571, 432609, 153607,\n",
            "        659915, 508830, 151336, 988663, 987802]) tensor([ 0, 47, 39, 58, 31, 21, 50, 58, 47, 40, 39, 53,  1, 57, 42, 44, 50, 46,\n",
            "        39, 58, 41, 53, 53,  1, 39, 52, 43, 57, 61, 41, 59, 57])\n",
            "start tensor([229921, 727613, 760528,   9707, 786237,  35057, 274031, 540088, 168936,\n",
            "        749747, 840988,  87499,  52202, 736176, 694973, 115594, 370553,  86968,\n",
            "        835405,  89064, 456100, 129147, 658896,  61010, 835618, 610643, 562741,\n",
            "        108987, 350520, 501273, 699591, 846729]) tensor([58, 58, 43, 46, 17, 57, 42, 47, 56,  6, 42, 50, 39,  1, 58, 43, 43, 46,\n",
            "        41, 39, 61, 52,  1,  1, 47,  1, 41, 43, 27, 52, 47,  1])\n",
            "start tensor([143125, 562040, 784266,  36783,  98402, 403870, 264362, 106346, 184730,\n",
            "        752496, 688656, 173393, 611738, 607503, 146447, 912252, 955965, 828767,\n",
            "        422573, 705598, 118992, 477552,  40838, 766579, 913566, 971864, 769837,\n",
            "        325474, 651630, 762273, 567623, 289365]) tensor([44,  0, 59,  0,  1, 45, 52,  1,  1, 51,  6, 50, 61,  1, 11, 58, 43, 56,\n",
            "        46,  1,  1, 56, 56, 53,  1, 50, 61, 51, 50, 42, 47,  1])\n",
            "start tensor([946527,  53227, 611723, 338858, 592140, 811502, 911431, 411405, 430416,\n",
            "        850637, 333248,  83940, 524532, 624723, 962332, 978960,  90581, 142983,\n",
            "        596294, 357691, 189364, 763543, 152338, 236973, 322672, 206672, 760186,\n",
            "        352462, 568448,  29412, 479904, 757083]) tensor([59, 41, 34, 43, 58, 43, 52, 56, 47, 50, 39, 52, 57, 43, 53, 40,  0, 47,\n",
            "        53, 43, 39,  1,  0,  1, 46, 14, 56, 39, 24, 53,  0, 43])\n",
            "start tensor([602312, 989135, 481948, 921330, 632237, 916952, 314848, 349112, 526916,\n",
            "        568695,  95919, 858943, 792718, 756121, 331583, 560618,  61634, 331016,\n",
            "        633490, 624292, 520078, 517542, 243354, 544700, 135802, 267803, 668252,\n",
            "        282899, 955739, 821963, 577105, 430125]) tensor([ 1, 41, 21, 63, 60, 47, 53, 21, 45,  6, 47,  1, 43,  6, 58, 56, 49,  1,\n",
            "         1, 46, 41,  1, 35, 58, 43, 50, 52, 26, 39, 43,  1, 10])\n",
            "start tensor([956182, 367306, 576146,  88485, 779838, 351285, 898819, 336628,  39861,\n",
            "        457574, 313296, 393366, 656329, 468923, 243479, 622368, 669338, 545644,\n",
            "        472761, 511009, 431881, 755856, 957021, 106273, 982568, 123446, 278885,\n",
            "        353265, 935277, 814586,  60641, 236650]) tensor([50, 61, 57,  6, 44, 31, 39, 39, 25, 58,  1, 13, 61,  0, 39, 23, 32, 52,\n",
            "        61, 40, 46, 47, 53,  0,  6, 47, 53,  1, 53, 53, 53, 54])\n",
            "start tensor([ 66416,  78087,  13988, 354313, 322545, 500632, 171516, 200094, 708131,\n",
            "        258571, 161853, 360458, 791143, 821352,  31494, 570008, 610092,  60182,\n",
            "        704647,  95531, 436581, 956084,  18129, 538254, 662539, 828516, 638635,\n",
            "        288754, 192054, 707919, 101340, 701745]) tensor([43, 58, 50, 45, 41,  1,  1, 57, 52, 58, 57, 43, 57, 45, 18, 60, 56, 63,\n",
            "        44, 54, 56, 39, 52, 39, 43, 52, 39, 19, 39, 24,  5,  1])\n",
            "start tensor([ 341626,  788801,  465464,  140469,  350654,  440636,  471824,  365631,\n",
            "         109180,  699606,  316614,   95839,  874840,  356131,  870027, 1002859,\n",
            "         762139,  978748,  966965,  280219,  615835,  914515,   95340,  206566,\n",
            "          46811,  118231,  598445,  500469,  988932,   74519,  264236,  804532]) tensor([46, 52, 61,  0, 32, 58, 61, 46, 52, 47, 58, 63, 43, 61, 25,  1, 60, 42,\n",
            "         0, 51, 56, 54, 57, 47, 39,  1, 43, 10, 39,  1, 46, 56])\n",
            "start tensor([ 89269, 503394, 197637, 580441, 276884, 677464, 937273, 750455, 219887,\n",
            "        260421, 843146, 557995, 738957, 127203, 410215, 933498, 116226, 895366,\n",
            "        692122, 875151,  71904,  36391, 710290,  52638, 922297, 366915, 317178,\n",
            "        892275, 161628, 638178, 909541, 426141]) tensor([52, 46, 43, 46,  1, 59, 10, 46, 10, 60, 46, 49, 50, 52, 60, 56, 57, 52,\n",
            "         1, 54, 53, 25, 53, 44,  1, 43, 43, 43, 52, 56,  1, 16])\n",
            "start tensor([ 968423,  629177,  590141,  760658,  425152, 1002645,  180298,   13423,\n",
            "         100163,  385942,  325260,  315899,  466861,  476199,  337737,  519673,\n",
            "         247499,  116592,  888291,  909004,  479972,  357740,  647441,  975081,\n",
            "         171982,  209035,  345373,  162663,  879973,  597503,  966514,  913924]) tensor([59, 43, 50,  1,  1, 43, 17, 46,  1,  1,  6,  0, 40, 57, 58, 41, 43, 50,\n",
            "        24, 58, 60, 41, 61, 50, 53,  1, 43, 60, 56, 41, 49,  0])\n",
            "start tensor([979306, 515616, 509882, 780997, 956860, 183693,  25313, 603289, 151458,\n",
            "        658995, 231813, 457434, 347069, 684804, 670800, 613830, 581453, 682300,\n",
            "        530331, 275686,  12420, 996978, 148658, 416410, 195347, 615140, 751299,\n",
            "        555351, 342776, 228944, 706992, 199058]) tensor([47, 43, 58, 52, 41, 39, 39, 25,  1, 43,  1, 61,  1, 53,  0, 39, 57,  1,\n",
            "         1, 61, 57, 53,  1,  1, 47, 53, 53, 47, 47, 39, 46,  1])\n",
            "start tensor([ 26552, 479450, 722420, 231927, 531518, 711700, 935466, 126042, 354994,\n",
            "        129162, 145250, 357988, 636606, 103144, 238203, 359448, 107616, 405623,\n",
            "        265204, 352775, 219570, 313416, 254960, 635347, 602923, 866133, 381917,\n",
            "        862443, 959199, 864805, 188172,   2921]) tensor([25, 43, 58, 50, 53,  1,  0, 52, 59,  1, 58,  0, 58, 52, 40, 62,  1, 43,\n",
            "        53,  0, 61,  1, 56, 47, 59,  1, 39, 56,  1, 56, 19, 61])\n",
            "start tensor([854909, 374496, 649291, 279282, 287934,  29487, 842253, 457431, 900279,\n",
            "        877099, 278793, 769641,  41449, 572534, 732805,  60299, 928023, 954721,\n",
            "        804215, 908661, 200781, 766246, 555936, 829284, 197231, 902478, 624436,\n",
            "        909985,  69371, 900555, 965500, 355015]) tensor([50, 53,  1,  1, 61, 50, 58,  1, 57,  1, 43, 63, 33, 50, 57, 59, 53, 21,\n",
            "        51,  1, 47,  0,  6, 52, 50, 50, 51,  1, 10, 43,  5, 58])\n",
            "start tensor([477320, 258787, 252028, 351657, 900707, 771411, 989677, 207945, 243305,\n",
            "        238505, 616732, 439335,  63462, 414192, 547337, 639947, 684533,  64276,\n",
            "        204534, 413383, 562178, 345803, 380896, 267160, 366814,  30347, 100873,\n",
            "        227126, 762136, 649402, 923884, 106634]) tensor([49, 43, 57, 52, 56, 40, 25, 32, 43, 52, 50, 47, 47, 56,  6, 59, 58, 53,\n",
            "        52, 63, 47, 58, 42, 53,  1,  1,  1,  1, 46, 56, 51, 43])\n",
            "start tensor([626961,  13498, 556175, 154938, 664184, 155926, 372263,  13612, 519220,\n",
            "        599586, 519389, 922934, 119066, 463832, 124620, 493333, 335418, 271775,\n",
            "        552091, 659571, 420023, 290932, 924563, 858110, 574201, 482012, 731742,\n",
            "        627434, 291121, 992423, 344976, 360656]) tensor([42, 47,  0, 53, 45, 58, 20, 50,  1, 37, 58, 60,  0, 58,  5, 40, 27,  1,\n",
            "        53, 57,  0, 53, 60, 43, 58, 45, 45,  1, 21, 51, 51, 50])\n",
            "start tensor([259724, 434145,  90968, 419387, 577179, 329684, 181107, 221874,  89340,\n",
            "        653178,  88088, 495961, 668687, 224574, 738581, 473655, 170675, 786169,\n",
            "        191963, 429672,  97937, 682384, 160608,  13464, 321816, 450768,  60607,\n",
            "        885729, 767759, 476797, 455086, 660269]) tensor([46, 39, 46,  1,  1, 50,  1, 39, 63, 58,  1, 50, 58, 43, 57, 21, 43, 42,\n",
            "        41,  0,  8,  1, 59, 47, 43, 43, 43,  1,  1, 53,  6,  1])\n",
            "start tensor([819166,  14861, 577510, 857585, 336169,  36390, 564252, 611596, 135760,\n",
            "        830088, 578399, 446581,  92416, 880100, 606894, 814648, 223522,  44738,\n",
            "        972666, 379573, 239204, 748007, 473235, 391449, 310742, 962645, 567220,\n",
            "        431155, 952006, 351320, 400114, 951482]) tensor([ 6, 43, 53, 56, 58,  1, 47, 57, 53, 50, 56,  1, 46, 42, 40,  1,  0, 54,\n",
            "        57, 13, 47,  6, 39, 39, 59, 42, 54, 58, 58, 53, 52, 43])\n",
            "start tensor([119382, 983812, 395732, 235274, 433722, 634036,  85965, 685082, 721059,\n",
            "        426101, 542668, 759152,  36226, 987464, 657686, 824593, 762243,  96215,\n",
            "        682483, 784623, 679979, 679503, 363307, 468351, 412841,  73940, 589832,\n",
            "        912226,  62645, 584835, 858276, 297236]) tensor([57, 46, 49, 11, 43, 16,  1, 18, 11, 12,  1, 53,  1, 46, 52,  0, 53, 39,\n",
            "         1, 43,  1, 58, 56, 56, 39, 52, 53, 43, 43,  0, 50, 47])\n",
            "start tensor([844864, 492138, 630689,   7349, 710349, 356369, 989376, 495559, 415611,\n",
            "        607915, 558156, 575233, 409594, 693883, 748544, 362712, 972424, 818845,\n",
            "        657479, 227963, 498656, 647523, 164719, 277028, 121742, 983456, 400722,\n",
            "        883042, 756905, 886456, 388153, 776946]) tensor([58, 59, 50, 58, 39, 58, 59, 63, 27,  1,  1, 52, 14, 57,  6, 53, 47, 43,\n",
            "        46, 53, 58, 27, 43, 56, 53, 43, 50, 43, 58, 43, 46, 43])\n",
            "start tensor([  32552, 1000885,   32203,  996088,  788597,  699388,  562752,  585104,\n",
            "         431918,  144620,  777071,  576656,  268806,  293897,  145811,    2139,\n",
            "         200075,  624300,  904158,  517942,  491437,  231783,  393901,  370562,\n",
            "         715092,  266829,  733781,   40890,  277341,  587519,  330729,  252219]) tensor([32, 59,  1,  1, 60, 43, 50, 26, 58, 43,  1, 39, 50,  1, 56, 43, 44,  1,\n",
            "        20, 61,  1, 43, 50, 44, 43, 58, 43, 53, 55, 59, 42, 13])\n",
            "start tensor([ 75134, 707614, 851797, 166349, 228423, 426301, 787126, 637741, 834317,\n",
            "        739581, 725706, 586339, 110911, 127237, 843574, 994907, 117728, 854267,\n",
            "        291896,  33762, 711597, 744874, 385146, 961603, 524418, 368377, 283195,\n",
            "         48521, 768639, 930645, 819232, 928771]) tensor([ 1, 20, 43, 43,  1,  1, 53, 43, 56,  1, 47, 50, 52, 53, 41, 43, 58, 46,\n",
            "        10, 58,  1,  1,  1,  0, 58,  1, 56, 51,  1, 53, 43, 61])\n",
            "start tensor([342753, 671405, 145152, 801470, 945675, 774958, 836024, 360785, 979141,\n",
            "        622527, 399694, 962891,  69929, 215184, 528254, 718641, 135556, 454703,\n",
            "        335926, 369001, 768419, 157252, 964321, 517477, 585926, 298559,  71630,\n",
            "        404792, 783262,  91567, 958391, 230098]) tensor([43, 45, 57,  1, 50,  1, 53,  1, 41, 50, 43, 39, 52, 52, 53, 51, 52,  1,\n",
            "        57, 39, 47, 43, 57, 57,  1, 56, 59, 43, 53, 16, 52,  1])\n",
            "start tensor([514099, 275172, 348759, 827289, 350367, 818084, 250310,  42052, 621410,\n",
            "        906055,  71089, 840247, 512364, 849993, 341985, 699216, 105176, 492722,\n",
            "        194406, 666741, 406653, 212434, 316680, 101957, 622311, 461417, 691883,\n",
            "        424433, 318211, 917027,  82989, 976270]) tensor([50, 58, 47, 57, 41, 47, 13, 57, 41, 43, 63, 56,  1,  0,  1, 43, 47, 61,\n",
            "        63, 43, 50, 51,  1, 39, 47, 26, 13, 52, 57, 15, 52, 46])\n",
            "start tensor([122573, 877880,  41642,  27187, 752844,  88427, 126836,  57266, 422957,\n",
            "        283129,  37293, 137124, 403983, 486322, 615515, 458476, 733669, 514274,\n",
            "        191676, 939155, 368484, 166574, 495225, 112649, 238221, 918913, 105170,\n",
            "        159283,  76156, 417255, 184839, 206661]) tensor([42,  1, 12, 56, 53, 53, 58, 45, 52, 21, 43, 57, 45, 44, 47, 42, 43,  1,\n",
            "        52, 46, 63, 43,  0, 43, 30,  1, 56, 41, 47,  1, 43, 29])\n",
            "start tensor([290642, 215796, 954127, 968073, 872740, 396464, 159562, 856690, 648347,\n",
            "        956586, 495506, 513614, 756086, 324874, 870330, 347145, 294008, 506224,\n",
            "        502109, 639343, 239956, 605932, 609164, 367420, 255870, 421877, 252996,\n",
            "        146538, 535968, 458660, 966299, 840985]) tensor([53, 46, 51, 43, 20, 47, 47, 50, 63, 53, 50, 39, 52, 32, 47, 57, 59, 53,\n",
            "         1, 52, 39, 56, 43,  1,  6, 39, 43, 44, 52, 58,  1, 52])\n",
            "start tensor([1003764,  579422,   91102,  612094,  959076,  231427,  667216,  600841,\n",
            "         302926,  517557,  358228,  377487,  305249,  751013,  131648,  608429,\n",
            "         202678,  961106,  974819,  264972,  438153,  405935,  895262,   63746,\n",
            "         518809,  849761,  884488,  827305,  161125,  843344,   18838,  266915]) tensor([42,  1, 57, 44, 46, 40, 53,  1, 52, 58,  1, 46, 15,  1, 40, 39,  1,  1,\n",
            "        57, 46,  1, 43, 53, 50, 47, 15, 53, 52, 49, 39, 56, 57])\n",
            "start tensor([877293, 968217, 964822, 676541, 633188, 462704, 111927,  83158,  90904,\n",
            "        198071, 906863, 179214, 583593,  93079, 933797, 409130, 639856, 674740,\n",
            "        869918, 620964, 772157, 891893,  33418, 517242, 175796, 650324, 598729,\n",
            "        933620, 438375, 126541, 787652, 699835]) tensor([43, 39,  1, 52, 43, 54, 21, 27, 41, 39, 58, 59, 39, 57,  1,  0, 52, 47,\n",
            "        59, 56, 57, 61, 43, 43, 26, 35, 39, 46, 54, 50, 53,  1])\n",
            "start tensor([197029, 585476, 304237, 111273, 449235, 558719,  78518, 144039, 374147,\n",
            "        772735, 338325, 866794,  12421, 582301, 965329, 641910, 729662, 513963,\n",
            "        669259, 822305, 851701, 601863, 196880,  96595, 899305, 186746,   6346,\n",
            "        279059, 281475, 909525, 557021, 337731]) tensor([51, 10, 42, 43, 57, 32, 51, 47, 43, 53, 53, 56, 47, 60, 43,  1,  1, 51,\n",
            "         0, 43,  1, 43,  1, 46, 58, 58, 46, 26, 30, 32, 45, 58])\n",
            "start tensor([566827, 755942, 202523, 770888, 749526, 296392,  69455, 595077, 620912,\n",
            "        746890, 704151, 822681, 680315, 269756, 640650, 254190, 271680, 784234,\n",
            "        913893, 584950, 403690, 255410, 113995, 354250, 972825,  62373, 471277,\n",
            "        515089, 750209, 370036, 771236,  44284]) tensor([43,  7, 47, 58, 46, 33,  1, 16, 53,  8, 35, 46,  1, 10, 59, 43, 63, 46,\n",
            "        50, 57, 43, 37, 42, 39, 53,  6, 63,  1, 33, 26, 43,  1])\n",
            "start tensor([736319, 221322, 671014, 951939,  95514, 887080, 725626, 269618,  62183,\n",
            "        882899, 424144, 109543, 285956, 194455, 215525, 350234,  92214, 870067,\n",
            "        627192,  61191, 546438,   5483, 233757, 406826, 644486, 995835, 971510,\n",
            "        999212, 319392, 699216, 340633, 350544]) tensor([57, 56,  1, 46, 51, 63, 57, 58, 50, 53, 51,  2, 30, 50, 51, 40, 57, 40,\n",
            "         6, 47, 58,  1,  0, 43, 50, 59, 47,  0, 58, 43, 23, 52])\n",
            "start tensor([644400, 531334, 296320, 479451, 336238, 389503, 430763, 223794, 221516,\n",
            "        349661, 278786, 503350, 399902,  75926, 421380, 507010, 260076, 650322,\n",
            "        114208, 837371, 842385,  33149, 731129, 659470, 516968,  38707, 205773,\n",
            "        732571,  86348, 778166, 409185,   4760]) tensor([11, 49,  0,  1, 13, 40,  1, 50,  1, 50, 53, 40, 46, 39, 51, 43, 43,  6,\n",
            "        51, 43,  1, 43,  0,  1, 39, 30, 50, 58,  1, 43, 30, 52])\n",
            "start tensor([106676, 491291,   3477, 123253, 594136, 566772, 764484, 111931, 564281,\n",
            "        908014, 320689, 248837, 961532, 333126, 374676, 272566, 481585, 315813,\n",
            "        775971, 603466, 199940, 820696, 648304, 817503, 561526, 250703, 962066,\n",
            "        529319,  26668, 277081, 701096, 742281]) tensor([32,  1, 57,  1, 43, 47, 27, 43, 13, 43, 60,  1, 43, 57, 47, 56, 41, 52,\n",
            "        53, 46, 50, 59, 39, 53, 47, 43, 46, 58, 49,  1, 52, 57])\n",
            "start tensor([ 810761,  636292,  515221,   25339,  304621,  526136,  751867,  438141,\n",
            "          34001,  309666,  996563,  737672,  803185,  903854,  433190,  141477,\n",
            "         487690,  143802,  387940,  226425,  595233,  310913,  596517,  513582,\n",
            "         972610,  374493,  263055,  166018, 1003201,  354468,  843970,  450204]) tensor([44,  1, 43,  0, 47, 35, 46, 56, 43, 30, 47, 63, 43, 52, 50, 53, 53, 58,\n",
            "        45, 57, 43, 56, 21, 28, 51, 47, 57,  1, 42, 56, 43, 57])\n",
            "start tensor([164771, 190302, 823465,  31175, 756942, 783628, 179012, 158177,  84932,\n",
            "          7280, 871717, 134276, 316030,  96530,  24757, 719985, 166072, 503829,\n",
            "        708116, 644139, 166403, 508998, 102341,  23882, 408590, 897831, 432368,\n",
            "        200888, 698831, 991400, 597409, 460498]) tensor([34, 50, 57, 43,  0, 52, 43, 32, 50,  0, 59, 39, 58, 59,  1, 50, 53, 43,\n",
            "        50, 15, 56, 43, 42, 52, 59, 59, 58,  1, 44, 51,  1,  5])\n",
            "start tensor([ 268778,    5081,  101125,  526934,  543518, 1000487,  179360,  904882,\n",
            "          23318,  382348,  743673,  751908,  976211,  311243,  625982,  267126,\n",
            "         751720,  624913,  280264,  454485,  744109,  743420,  978677,  172006,\n",
            "         520909,  441384,   38918,  311021,  739566,  817129,   98561,  543293]) tensor([ 6, 53, 43, 58, 39, 39,  1, 50, 43, 42, 39, 57, 41, 46, 63,  1, 43, 40,\n",
            "        63, 53, 58, 50, 43, 43,  1,  1, 57, 52, 60, 52, 52, 58])\n",
            "start tensor([556055,   4607, 306362, 928280, 303077, 859627,   7679, 521286, 499712,\n",
            "        188541, 688571, 383230, 512375, 868092, 620428, 241797, 272938, 607406,\n",
            "         39479, 359686, 271910, 687201, 298514, 494773, 215990,  44938, 116536,\n",
            "         66167, 666544,  80226, 952763, 225316]) tensor([56, 53,  1, 13, 52, 47,  1, 47,  5,  0, 47, 63, 58,  1, 41, 31, 15, 41,\n",
            "         1, 52, 20, 47, 53, 58,  1, 45, 10, 53,  1, 52, 58,  1])\n",
            "start tensor([674701,  14059, 151847, 994728, 989379, 399991, 202549, 545185,  53209,\n",
            "        956167, 512807, 673133,  96929, 762351,  79277, 279209, 937967, 460513,\n",
            "        530755, 989193, 778152,  73361, 749014,  59017,  45825, 821695, 727379,\n",
            "        864304,  38437, 375789, 992580, 838459]) tensor([43, 58,  0, 61, 53,  1, 43,  1, 52, 60, 43,  1, 27, 56, 43, 61, 61, 57,\n",
            "         1, 10,  1, 59, 59, 43, 33, 51, 43, 53, 49, 43, 53,  1])\n",
            "start tensor([787744, 941951, 767164,  83624, 857608, 298458, 232190, 560064, 216052,\n",
            "        414369, 451480, 798535, 469441, 188937, 387297,  96912, 302359, 925218,\n",
            "        700842, 572406, 291797, 267308, 942703, 963143, 187207, 276979, 448735,\n",
            "        708030, 530782, 326151, 581002, 284395]) tensor([ 1,  1, 58, 63, 43, 40, 56,  1, 33,  1, 34,  0, 46, 17,  0,  1,  0, 44,\n",
            "        57, 47, 40, 57, 53,  1, 45, 47, 51, 46,  0, 45,  6,  1])\n",
            "start tensor([ 72510, 528831, 609038, 922529, 595117, 725866, 454449, 170542, 905375,\n",
            "        822457, 725590, 385354, 675743, 843295, 184682, 774322, 362070, 183609,\n",
            "         43496, 177734, 585590, 156655, 260533, 217896, 406754, 767950, 812404,\n",
            "        161781, 547331, 295883, 908097, 457331]) tensor([ 1, 63, 57,  1,  1, 58, 51, 43,  1, 53, 58, 50,  1, 46, 47,  1, 53, 45,\n",
            "        42, 53, 34, 58, 63, 43, 39, 39,  1,  1,  8, 49, 40, 60])\n",
            "start tensor([347599, 762171, 375095, 256031, 591176, 431012, 838463, 870793, 435673,\n",
            "        382195,  40798, 355523, 453145, 150343, 729402, 506627, 710125, 837918,\n",
            "        894138, 345195, 774399, 409713, 829858,  28084, 950934, 868629, 862998,\n",
            "        441924,  87724, 402340, 451221, 948387]) tensor([47, 47, 46, 52, 42, 50, 46, 17, 58, 47, 47, 46, 39, 52, 53, 59,  1, 43,\n",
            "        52, 53, 61, 45, 53, 53,  1,  6, 43, 39, 53, 50, 47, 43])\n",
            "start tensor([ 202099,  738072,  904084,  623980,   57614,  756479,  130927,  536564,\n",
            "         825158,   85136,  132375,  145908,   55509,   82634,  165991,   48045,\n",
            "         838902,  361578,   67975, 1002561,  571016,  342330,  443122,  681371,\n",
            "         521383,  241320,  424188,  931328,  889615,  983383,  786171,  535125]) tensor([ 1,  1, 44, 53, 46, 39,  6, 47, 42, 47,  1, 58, 57, 50, 43, 57, 57, 51,\n",
            "        39, 56, 28,  8,  1,  1, 43, 58, 47, 47, 50,  1, 43,  1])\n",
            "start tensor([719008, 683043, 643083, 352482, 827069, 923520, 228360, 432783, 484378,\n",
            "        120534, 808973,  13896, 739627, 823450, 734152, 646375, 210930, 240430,\n",
            "        645755, 578222, 730970, 851003, 441156, 220423, 840420, 112184, 477439,\n",
            "        730570, 533955, 167584, 326239,  71909]) tensor([20,  1,  0, 58, 51, 60,  1, 43, 21, 39, 51, 41, 63, 43, 39, 46,  0, 53,\n",
            "        54, 42, 57, 47, 61,  6,  5,  1, 47, 53, 43,  0, 58, 57])\n",
            "start tensor([341044, 354045, 244185, 976847, 562877, 893405, 272118, 403205,  42400,\n",
            "        432952, 212331,  42587, 398650, 427700, 876679, 890989,  88717, 946064,\n",
            "        781059,  71373, 459287, 900350, 637187, 288466, 112331, 769064, 813446,\n",
            "        564700, 649827, 258695, 451148, 934020]) tensor([43, 59, 43, 59, 42, 57, 50, 57,  1, 59, 50, 39, 43, 58, 44, 63,  1, 31,\n",
            "        56, 56, 63,  1, 41,  6,  1, 45, 39,  1, 58, 51, 47, 43])\n",
            "start tensor([391550, 367827, 676455, 871781, 497535, 846019, 304646, 662329, 950544,\n",
            "        960029,  31452,  38044, 674245, 569745, 861940, 424725, 610905, 770175,\n",
            "        591046, 298439, 960837, 235566, 711393, 234747, 234183, 547850, 871767,\n",
            "        427871,  57820, 137975, 642602, 490787]) tensor([61, 42, 47, 58,  1, 41, 45, 15, 59, 47, 39,  0, 57, 47,  0, 45, 52, 58,\n",
            "        39, 30, 51,  1, 61, 58, 46, 58, 21, 53,  0, 50, 19,  1])\n",
            "start tensor([ 68325, 770387, 120267, 957463, 997925, 785910, 530731, 436121, 973440,\n",
            "        407951, 911295, 945897, 235355, 297477, 856394,  24075, 241893, 736625,\n",
            "        895363, 194535,  23039, 162355, 923670, 177636, 316834, 821278, 541606,\n",
            "        623184, 571383,  38440, 398960, 685176]) tensor([57,  6, 59, 44, 52, 39,  0, 56,  0, 43, 47, 56, 37,  8,  1, 10,  0, 61,\n",
            "        58, 52, 43,  1,  5, 43, 58, 47, 63,  1, 43, 51, 39, 17])\n",
            "start tensor([ 836438,  920116,   39476,  221479,  535897,  546923,  867244,  985127,\n",
            "        1003393,  153768,  906056,   63327,  388080,  109490,  149631,   22448,\n",
            "         755119,  223543,   45903,  356936,  829707,  181063,  381782,  238374,\n",
            "         115384,  766565,  366455,  954907,  308410,  319555,  265132,  906684]) tensor([39,  0,  1, 51, 24, 53, 40,  1, 61, 57,  1, 57, 39,  1, 53, 43, 28, 44,\n",
            "        47, 43, 59, 46,  1, 47, 39, 39, 62, 31, 42, 58, 50,  1])\n",
            "start tensor([154862, 246779, 843865, 855453, 842108, 461105, 893752, 595951, 285167,\n",
            "        514052, 597634, 241761, 756023, 501211,  83654, 662298, 326059, 962659,\n",
            "        199916, 171934, 335268, 874577, 200585, 944785, 113755, 952410, 303662,\n",
            "        878042, 606268, 822171, 553530, 788597]) tensor([52, 58,  1,  1,  1,  1,  1, 53, 31, 46, 58,  1, 15, 44, 33, 39, 50, 51,\n",
            "        46, 37, 58, 43, 56,  0, 44, 39, 50,  5,  0, 51, 52, 60])\n",
            "start tensor([408764, 893037, 627316, 465430, 183582, 860146, 514711,  54223, 477317,\n",
            "        808189, 273716, 853897, 603624, 765523, 522410, 695256, 510994, 335954,\n",
            "        311108, 949844, 797815, 374535, 215012, 660716, 375068, 835876, 988858,\n",
            "        367263, 904241, 821199, 146230,  97449]) tensor([ 1,  1,  6, 45, 56, 58,  5, 58, 54, 52, 13, 61, 40, 59, 39, 53, 12, 46,\n",
            "        56, 57,  1, 57, 43,  1, 53, 50, 53, 61, 51,  0, 43, 47])\n",
            "start tensor([ 995312,  331891,  913280,  940807,  851203,   47299,  279006,  965514,\n",
            "         426016,  454425,  297214,  328886,  964452,  303624,  556650,  229895,\n",
            "         350205,   11214,  657831,   75493,  740376,   17459,  115971,  932604,\n",
            "         327186,  416417,  265431, 1000213,  806876,  147331,  681274,  314844]) tensor([ 1, 42, 26, 61, 43, 56,  1, 43, 63, 51,  1, 43,  1, 52,  0,  6,  1, 56,\n",
            "        57, 17, 46, 43, 43, 58, 46,  8,  1, 47, 46, 50, 50,  0])\n",
            "start tensor([826143, 702155, 753417, 333663,  41969,  30309,  24425, 115337, 409269,\n",
            "         52776, 535287, 811403, 151247, 173143, 520222, 596045, 989695,  88063,\n",
            "        850870, 717345, 382397, 257042, 237224,  36418, 250903, 848939, 672484,\n",
            "        577403, 109774, 873438, 458420, 874864]) tensor([ 1, 41, 43, 25, 58, 53,  0, 57, 39, 58,  1, 58,  0, 43,  0,  6, 42, 56,\n",
            "        42, 39, 52, 57,  1, 30,  5, 43, 56, 63, 57,  8, 53, 63])\n",
            "start tensor([166415, 914920, 227431, 969312, 472746, 826187,  71043, 242551, 276632,\n",
            "        505079, 559141, 210447,  88542, 280210, 349720,  24363, 323354,  62931,\n",
            "        245391,  78740,  57285, 645483,  79520, 679072, 931143, 121235, 950256,\n",
            "        698555, 141510, 710922, 265273, 182068]) tensor([16, 43, 41, 59,  1, 56, 39, 43, 40, 52,  1, 61, 47, 50, 58, 60, 10, 53,\n",
            "         1,  1, 57,  0, 33, 39, 49, 46, 21, 26, 56, 42, 42, 46])\n",
            "start tensor([ 272514,  755971,  493453,  105778,  786057,  368959,  239941,  787224,\n",
            "         642279,  593940,  877889,  456776,  101026,  369602,  572885,   13668,\n",
            "         120027,  231629,  916804,  377813,   72631,  185621,  357008,  641215,\n",
            "          71356, 1002167,  113757,  671203,  465960,  662511,  733048,  136254]) tensor([42,  1, 53, 56,  1, 50, 56, 47, 43, 45, 53, 63, 47, 42,  0, 59, 43, 56,\n",
            "        26, 52, 53, 60, 63, 41,  1, 52, 58, 39, 57, 60, 41,  1])\n",
            "start tensor([107361, 962348, 883525, 947135, 304154, 806978, 190100,  97022, 107993,\n",
            "        256054, 653203, 891022, 493442, 719574, 811381, 612460, 232317, 658056,\n",
            "        780619, 453750, 506098, 854375, 332088, 671224, 166579, 940089, 169994,\n",
            "        768511, 326164, 455523, 261565,  68283]) tensor([39, 56,  6, 56,  1, 53, 14, 52, 34, 16,  1, 27, 52,  6, 41, 30, 43,  1,\n",
            "         1, 52, 59, 53, 46, 57, 56, 56, 44,  1,  5, 51, 43, 41])\n",
            "start tensor([ 21192, 397079, 602908, 248799, 598125, 534327, 527951, 325513, 107298,\n",
            "        728181, 131802, 575830, 424189, 883456, 690946,  72303, 203972, 624213,\n",
            "        171209, 919891,  42878, 344540, 106362, 397365, 580116, 706641, 599574,\n",
            "         88643, 209083, 390769, 824856, 948243]) tensor([53, 58, 63, 57, 43, 52, 53,  1, 41,  1, 52,  1, 41, 57,  1, 43, 58, 39,\n",
            "        41, 43,  7, 57, 46, 41,  0, 53, 13, 51, 59, 56, 57, 50])\n",
            "start tensor([421401, 320368, 144453, 988946, 912944, 691790, 139484,  17321, 835458,\n",
            "         34330, 812565, 350763, 402438, 440799, 544373, 335385, 765630, 877936,\n",
            "        975305, 288767, 903664,    187, 431835, 403055, 964884, 950373, 182320,\n",
            "        923093, 293115, 673512, 662638, 495675]) tensor([21, 43,  5, 63, 57, 42, 54, 46, 53, 63,  1,  0,  0,  6, 56,  1, 39, 44,\n",
            "        39, 53, 53, 52, 37, 45,  1, 56, 46, 43, 57, 40, 56, 46])\n",
            "start tensor([820166,  16948, 897095, 883967, 444509, 241915, 480748, 187218, 762387,\n",
            "        517079, 224147, 706825, 901472, 574801, 830230, 677840, 496135,  23131,\n",
            "        789361, 180971, 199346, 437776, 320287, 130261, 315052, 645455, 445726,\n",
            "        852234, 453664, 190647, 239923, 165058]) tensor([57, 61,  1, 57,  0,  6,  5, 43,  1, 52, 53, 46, 43,  1, 51, 58, 57,  1,\n",
            "        42, 53,  0, 47, 57, 64, 43, 56, 41, 58, 43,  0, 45,  1])\n",
            "start tensor([ 37005, 270137, 465234, 510563, 333369, 646547, 847471, 973986, 881016,\n",
            "        453796, 317036, 284495, 675077, 510495,  72887, 636262, 950005, 693010,\n",
            "        837698, 430869, 115919, 940366, 524020, 428463, 474074, 714788, 599073,\n",
            "        108342, 992077, 950620, 845176, 563622]) tensor([52, 20, 44, 44, 41,  6, 58, 58, 47,  1, 13,  1, 53, 43, 52, 43, 41, 58,\n",
            "        42, 61,  1, 47, 47, 41, 57, 26, 21, 50, 61, 43, 58,  1])\n",
            "start tensor([134397, 819554, 416660,  72053, 403108, 444069, 700343, 398139, 968815,\n",
            "        196108, 331465, 363931, 490948, 719781, 116618, 797661, 840334, 784728,\n",
            "         66067, 883649, 292296, 882116, 106154, 725363, 447507, 869293, 676131,\n",
            "        467318, 150057, 351493, 503908, 793464]) tensor([47,  1, 27, 43, 46, 26, 56, 42,  1,  1, 39, 44, 53, 53, 21, 52,  1, 42,\n",
            "        43, 52, 56, 56, 39, 53, 46, 15, 13, 42, 31, 43, 33, 53])\n",
            "start tensor([ 95251, 528184, 287680,  35543,  91569, 419387, 666048, 292588,  83373,\n",
            "        192059,  77311, 342115, 359613, 508145, 772805, 882668, 216743, 355425,\n",
            "           420, 183549, 372033, 336891, 420683, 627928, 789453, 294438, 688998,\n",
            "        979037, 896403, 612048, 808446, 616787]) tensor([39, 53,  5, 43, 39,  1, 50,  1, 43,  5, 57, 58, 59, 17, 59,  1,  0, 39,\n",
            "         0,  1, 58, 52, 47, 46, 39, 43, 14, 41, 46, 49,  1, 46])\n",
            "start tensor([915522, 390576, 510679, 281399, 659350, 652255, 712922, 381421, 428380,\n",
            "        329793, 854015, 784408, 543885, 995218, 134489, 744894, 370191, 613520,\n",
            "         42170, 383020,  89832, 124230, 958255, 119523, 430657, 545280, 676941,\n",
            "        711963, 419611, 220126,  65118, 788868]) tensor([58, 46, 52, 56, 46,  1,  1, 50,  1, 51,  1, 50, 47, 39,  7, 43, 33,  1,\n",
            "        43, 43, 43,  1,  1, 50,  1, 42,  0, 43, 52,  1, 58, 52])\n",
            "start tensor([553133,   1151, 444231, 982640, 102013, 944136, 329781, 398515, 990809,\n",
            "        607651, 950127, 782444, 835580, 827110, 531194, 973653, 320110,  13052,\n",
            "        490287,  63080, 742950, 194227, 802048,  48915, 947553, 449643, 595401,\n",
            "        371320, 718694, 597860, 248411, 220140]) tensor([47, 43, 43,  1, 52, 58,  1, 39, 27, 57,  0,  0, 39, 43, 60, 43,  1, 59,\n",
            "         1, 39, 46, 10, 53, 51, 57, 39, 41, 39, 42,  1, 61,  1])\n",
            "start tensor([831141, 618632, 731239, 765861, 824387, 662456, 796321, 393832, 214598,\n",
            "        180828, 663316, 103309, 847390, 466630, 466613, 830296, 840203, 928503,\n",
            "        410175, 363310, 519052, 346201, 899346, 789719, 600810, 561790, 312595,\n",
            "         67301, 988404, 714375, 754736, 128560]) tensor([43, 10,  0, 50, 45, 57, 47, 57, 53, 58, 52, 21, 44, 58,  1, 39, 57,  0,\n",
            "         6,  1, 63,  6, 58, 53,  1,  1, 46, 43, 57, 49,  1, 13])\n",
            "start tensor([ 915891,  495796,  475163,  495645,  799808, 1001158,  787455,  296464,\n",
            "         200061,  665582,  214686,  210704,  330868,  296943,  322207,  854829,\n",
            "         954877,  797982,  532433,  630223,  784028,  810162,  478947,  364369,\n",
            "         613581,  303119,   98704,  687311,  481465,  313397, 1002404,  548763]) tensor([58, 58,  1, 52, 12, 59, 47, 43, 56, 43, 47, 60,  0, 61, 54, 57,  1,  5,\n",
            "        46, 52,  6, 57, 45,  0, 52,  1, 42, 53,  0, 51, 49,  6])\n",
            "start tensor([154050, 754630, 150407, 728830, 281879, 479296,  85549, 166232, 662787,\n",
            "        880839, 924896, 166806, 597653, 289856, 548618, 634651, 174398, 741005,\n",
            "        873596, 208785, 369870, 894856, 482893, 740261, 435619, 896011, 436307,\n",
            "        627968, 217075, 548016, 347801, 749949]) tensor([ 1,  1, 58, 42, 58, 26, 45, 59, 43, 57, 39, 58, 58, 43, 57, 58, 57, 41,\n",
            "        43, 51, 31, 39, 39, 50, 51, 52, 52, 57,  0, 59,  1, 41])\n",
            "start tensor([970586, 570434, 800181, 131362, 974886, 677830, 238326, 606465, 463539,\n",
            "        247690, 326304, 972440, 563758, 538225, 229445, 997245, 405817, 354617,\n",
            "        803947, 676743, 883406, 148065, 408789, 201399, 205338, 219370, 358921,\n",
            "        731571, 246933, 560810, 700649, 456248]) tensor([13, 58, 44,  1, 50, 14, 10,  1, 51, 61, 53, 47, 52,  1, 43,  1, 43, 53,\n",
            "        46,  0, 26, 16,  1, 53, 52, 17, 30, 47, 46, 63, 56, 58])\n",
            "start tensor([711292,  27994, 905160, 130342, 165322, 729926, 730754, 670799, 805825,\n",
            "        482053, 853737, 244962, 342734, 878822, 419512, 917330, 336244, 591263,\n",
            "        937626, 669414, 375091, 204782, 399027, 694714, 350222, 678383, 582003,\n",
            "        931615, 366207, 765021, 931047, 741123]) tensor([ 1, 21,  1, 39, 43,  1, 39,  6, 46,  1, 58, 39, 39,  1, 53, 59, 10,  1,\n",
            "        32, 52, 35, 56, 57, 31, 56,  1, 56, 27, 46,  1, 39,  1])\n",
            "start tensor([226821, 146475, 857912, 117164, 911369, 567746, 468549,  72616, 450821,\n",
            "        132763, 639603,  58252, 980374, 725175, 663361, 607084,  23511,  17911,\n",
            "        536195,  23885, 103603,  37384, 761792, 398013, 543966,  42304, 701151,\n",
            "        560429, 279622, 946357, 395514, 334090]) tensor([53, 13, 58, 43,  5, 56,  1, 43, 53,  1, 11, 47, 58, 41, 42, 58, 43, 39,\n",
            "        52, 53, 57, 58, 58, 47, 58, 53,  1,  1, 56, 52, 42, 43])\n",
            "start tensor([306966, 215928,  42180,  85499, 691228, 104557, 657658, 865734, 259490,\n",
            "        489178, 610798,  21437,  14437, 284326, 680773, 725423, 632358, 807349,\n",
            "        306082, 790258, 403481, 453520, 907014, 985149, 582672, 194046, 124070,\n",
            "        597927, 276151, 703030, 887418, 442721]) tensor([21,  1, 43, 59,  8, 52, 45, 42, 55, 39,  1,  1, 53, 49,  8, 43, 56, 54,\n",
            "         1, 63,  1,  1, 58, 21,  1, 39, 53, 57, 61, 58, 58,  1])\n",
            "start tensor([661756, 239772, 324696, 220005,  33075,  77233, 307842, 485762, 346921,\n",
            "         36247, 583329, 718302, 515980, 496632, 156583, 658146, 792078, 262017,\n",
            "          3012, 711345, 799129, 785635, 579953, 653582, 669047, 945879, 319966,\n",
            "        217054, 714377, 241062, 153508, 662992]) tensor([46,  1,  1, 45, 51, 21, 61, 40, 52, 43, 54, 50, 63, 46, 48,  1, 41,  1,\n",
            "        47,  1, 59,  1, 43, 43, 56,  1, 43,  1, 46,  1,  0, 52])\n",
            "start tensor([848526, 287967, 833893, 552073,  31692, 673459, 182638, 867991,  32720,\n",
            "        177293, 787131, 342578, 648351, 387513, 511827, 241946, 569212, 996217,\n",
            "        410394, 112669, 361299, 362450, 301246, 916429, 867164, 308890, 621001,\n",
            "        650826, 151291, 577202, 669935, 336743]) tensor([ 0, 51, 40, 16, 58, 63, 58, 43, 39, 43, 10, 58, 53,  1,  1, 50, 43, 58,\n",
            "         1, 18, 47, 43, 58, 51, 47, 46, 39,  1, 31, 21, 46, 58])\n",
            "start tensor([132977, 655929, 512796, 145032, 617061, 151076, 658314,  10954, 464955,\n",
            "        507253, 647506, 795167, 877917, 262647, 169325, 166745,  76338, 507852,\n",
            "        701858, 574454, 947004, 303037, 372394, 251142, 817938, 718715, 111931,\n",
            "        800641, 359510, 495659, 175188,  61269]) tensor([ 1, 39, 43,  1, 46, 58, 53, 31,  6, 46, 52, 56, 59,  1, 39, 57,  1, 56,\n",
            "        58, 11, 52, 56, 59, 63,  1,  1, 43, 17, 17,  6, 56, 56])\n",
            "start tensor([  4775, 541091, 876699, 687610, 175290, 402729, 508404, 928529, 904556,\n",
            "        901879, 663667, 470589, 822852, 594073, 899045,  31535, 869606, 771961,\n",
            "        411100, 873760, 468287, 204703, 303311, 296281, 967109, 292684, 176992,\n",
            "        601253, 249713, 607643, 499756, 792300]) tensor([ 6, 43,  1, 47, 39, 47, 59, 52, 52, 50, 53, 47, 46, 18, 49, 50, 63, 50,\n",
            "        56, 50, 42,  1, 57, 52,  1, 21, 63, 46, 47, 61, 46,  1])\n",
            "start tensor([735198, 466695,  27354, 953011, 599303, 344293, 299674, 803299, 735481,\n",
            "        902688, 561910, 940901, 541183, 875346,  35579, 682881,  24569, 825683,\n",
            "        899154, 229268, 716038,  44118, 129105, 690628, 687528, 112449, 965014,\n",
            "        822257, 655778, 576333, 265444, 560282]) tensor([ 0, 57, 58, 43, 39, 53, 57, 44, 57,  1, 58, 59,  1, 50, 37, 52,  1, 39,\n",
            "        14, 58, 42, 33, 53,  6, 43, 46, 59, 21,  1, 43, 58,  1])\n",
            "start tensor([980902, 785734, 407087,  22179, 885715, 260386, 594818, 707339,  69542,\n",
            "        755250, 811596, 429724,  78172, 656287, 809747, 890946, 954811, 170434,\n",
            "        621159, 199004, 323823,  58849, 515428, 278142, 935431, 924402,  62495,\n",
            "        534384,  77530, 507369, 282327, 826533]) tensor([51, 58, 57, 35, 17, 45, 39, 53, 52, 57, 43, 43,  1, 47, 47,  0, 53, 54,\n",
            "        13, 43, 53, 58, 39, 46, 46, 53, 59, 46, 33, 57, 41,  1])\n",
            "start tensor([242539, 992419, 836810, 941892, 722405, 262008, 366570, 986885, 749204,\n",
            "        687252, 633361,  32828, 125289, 338043, 785369, 412749, 893129, 468546,\n",
            "        469652, 648931,  83955, 494426, 829134, 900980, 463360, 193344, 469172,\n",
            "        473157,  27561, 917254, 589779, 323836]) tensor([ 1, 59, 52, 51, 39, 27, 18, 58, 53, 56, 10, 53, 43, 59, 61,  0, 58, 60,\n",
            "        53, 39, 31, 59, 30, 56, 47,  1,  0,  1, 39,  1,  1, 50])\n",
            "start tensor([809504,  38390, 115252,  38215, 179192, 991663, 683107, 676015, 665156,\n",
            "        971674, 293130, 162671, 255468, 595719, 145636, 271876, 185024, 224447,\n",
            "        261977,  64824, 666312, 941472, 437703, 920491,  14001, 762162, 768137,\n",
            "        602111,  33084, 137267, 131846, 130158]) tensor([46, 58, 50, 45, 21,  6, 41, 53, 63, 46, 39, 42, 51, 39, 39,  0, 46,  1,\n",
            "         1, 52, 52,  0, 57, 13, 32, 49, 57,  1, 57,  0, 58, 59])\n",
            "start tensor([ 92832,  33585, 409207, 379072, 618144, 204987, 920196,  21145, 847178,\n",
            "        425006, 243652, 800900, 625795, 978669,  76723, 637506, 480651, 931461,\n",
            "         87854, 376179, 532015, 418484, 934329, 589456, 135450, 304298,   1063,\n",
            "        415729,  78289, 638142, 596937, 327271]) tensor([52, 10, 43,  1, 41,  0, 26, 46, 46, 17, 50, 47, 53,  6, 47,  1, 50,  1,\n",
            "         1, 57, 53, 52, 27, 57, 58, 43, 47,  1, 54,  1, 41, 39])\n",
            "start tensor([256584, 881462, 325225, 875387, 714106, 365366, 594657, 501669, 302816,\n",
            "        288964, 793620, 710479, 237193, 501731, 704925, 938566, 482541, 694992,\n",
            "        223443, 806127,  99402, 352398, 578465, 254264, 323757, 191194, 757120,\n",
            "        305364, 344417, 126041, 369258, 998692]) tensor([ 1, 58,  1,  1, 44, 58, 15,  1, 60, 47, 52, 53, 42, 56,  8,  1, 57, 56,\n",
            "        28, 24,  0, 54, 53, 43, 58, 47, 56, 42, 43, 21,  0,  1])\n",
            "start tensor([891479, 688250, 958753,  23729, 154479,  21211, 357364, 151835, 513289,\n",
            "        489556, 164992,  31609, 224454, 121686, 299356, 151318,  88438, 540708,\n",
            "        846228, 133595, 234316,  68209,  70324, 928480, 604449, 643741, 384600,\n",
            "         30110, 601036, 358187, 717375, 402096]) tensor([56, 52,  0, 32, 30, 58, 47, 41,  1, 43,  1,  1, 43, 40,  2, 25,  2, 43,\n",
            "        21, 46,  1, 52, 58, 17, 42, 16, 43, 39, 43,  0, 27,  0])\n",
            "start tensor([970368, 537765, 517164, 518791, 761625, 646544, 954811, 493697, 963750,\n",
            "        757763, 728987, 790216, 761099, 454861, 143932,  90541, 926502, 704780,\n",
            "        860241, 480180, 952465, 162390, 531336, 706764, 960602, 385698, 828693,\n",
            "        788808, 992266,  27727,  29767, 698109]) tensor([57, 46, 11, 53, 61, 53, 53, 46,  0, 56, 53, 43, 50, 56, 41, 53,  1, 52,\n",
            "        42,  5, 53, 43, 57, 43, 50, 56, 38, 54,  1, 47, 45, 52])\n",
            "start tensor([706398, 158986, 752786,  69565, 980859, 769346,  25841, 637192, 319165,\n",
            "        165189, 755681, 161745, 263805, 483594,  28430, 585774, 432142, 926418,\n",
            "        116432, 344740, 671845, 280883, 494533, 396631, 808794, 651350, 327380,\n",
            "        139526, 687183, 801836, 797376, 660003]) tensor([47, 10, 46, 41, 32,  1, 47, 52, 56, 58, 52, 47, 51,  0, 50, 50,  1,  1,\n",
            "         1, 51, 42,  1, 59,  1, 52,  1, 58, 58, 39, 43, 39, 39])\n",
            "4.7352094650268555\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.2528, -0.1611, -0.4505, -1.0055,  0.5339,  0.0196,  0.8623,\n",
            "          -0.0269,  0.2696,  1.0755, -1.4398, -0.6009,  0.1445, -0.1786,\n",
            "          -0.8881,  1.5141,  1.3379, -0.1604,  0.3124,  0.9506, -1.9857,\n",
            "           0.5445,  1.4142,  0.5862,  0.1431, -1.5163, -1.1225, -0.2982,\n",
            "           0.4643, -0.8125,  1.4573,  2.4641, -0.5878, -0.2640,  0.9603,\n",
            "           0.1659,  0.0670,  1.1198, -1.2440, -0.3060, -0.5132, -0.9535,\n",
            "           0.4549, -1.5845, -1.2403,  0.4872, -0.5946, -0.7408,  1.5527,\n",
            "          -0.8934,  1.2589, -0.3022, -1.5512,  2.0149,  2.6710, -1.8360,\n",
            "           1.3626, -1.5095,  0.7610, -0.3025,  0.6869,  1.4597,  1.5169,\n",
            "          -0.4381, -0.9249]]], grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "l\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XinV8nmAnmKN"
      },
      "source": [
        "## The mathematical trick in self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukiH-NbRBhA",
        "outputId": "d981f6d4-ac08-4ec2-8284-82f5fa1e0815"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_E24uRE8kr",
        "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "86NuXX0fn7ps"
      },
      "outputs": [],
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT1hdtzXCjgL",
        "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5CvobiQ0pLr"
      },
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "4SNbLq5z3oBw"
      },
      "outputs": [],
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6I9n9IRTSo",
        "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tQx7oeRvtc",
        "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLb_odHU3iKM",
        "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpt8569BB9_f",
        "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "929ceb78-a639-41d6-aac7-12997b5c93f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633T2cmnW1uk",
        "outputId": "7720fa58-0478-4e8a-86a7-502d4cce9443"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9cK9BoXCYb",
        "outputId": "6368ece0-600e-417d-8a91-7c1e5d750ba8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "dRJH6wM_XFfU"
      },
      "outputs": [],
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      },
      "source": [
        "### Full finished code, for reference\n",
        "\n",
        "You may want to refer directly to the git repo instead though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.209729 M parameters\n",
            "step 0: train loss 4.4116, val loss 4.4022\n",
            "step 100: train loss 2.6568, val loss 2.6670\n",
            "step 200: train loss 2.5090, val loss 2.5058\n",
            "step 300: train loss 2.4195, val loss 2.4337\n",
            "step 400: train loss 2.3500, val loss 2.3560\n",
            "step 500: train loss 2.2965, val loss 2.3129\n",
            "step 600: train loss 2.2411, val loss 2.2499\n",
            "step 700: train loss 2.2050, val loss 2.2186\n",
            "step 800: train loss 2.1640, val loss 2.1873\n",
            "step 900: train loss 2.1242, val loss 2.1506\n",
            "step 1000: train loss 2.1029, val loss 2.1304\n",
            "step 1100: train loss 2.0686, val loss 2.1168\n",
            "step 1200: train loss 2.0389, val loss 2.0803\n",
            "step 1300: train loss 2.0262, val loss 2.0656\n",
            "step 1400: train loss 1.9937, val loss 2.0375\n",
            "step 1500: train loss 1.9696, val loss 2.0298\n",
            "step 1600: train loss 1.9632, val loss 2.0470\n",
            "step 1700: train loss 1.9414, val loss 2.0127\n",
            "step 1800: train loss 1.9085, val loss 1.9946\n",
            "step 1900: train loss 1.9126, val loss 1.9906\n",
            "step 2000: train loss 1.8852, val loss 1.9939\n",
            "step 2100: train loss 1.8704, val loss 1.9736\n",
            "step 2200: train loss 1.8607, val loss 1.9626\n",
            "step 2300: train loss 1.8562, val loss 1.9507\n",
            "step 2400: train loss 1.8430, val loss 1.9458\n",
            "step 2500: train loss 1.8153, val loss 1.9429\n",
            "step 2600: train loss 1.8269, val loss 1.9393\n",
            "step 2700: train loss 1.8113, val loss 1.9324\n",
            "step 2800: train loss 1.8053, val loss 1.9218\n",
            "step 2900: train loss 1.8079, val loss 1.9341\n",
            "step 3000: train loss 1.7956, val loss 1.9200\n",
            "step 3100: train loss 1.7695, val loss 1.9212\n",
            "step 3200: train loss 1.7525, val loss 1.9067\n",
            "step 3300: train loss 1.7589, val loss 1.9080\n",
            "step 3400: train loss 1.7556, val loss 1.8959\n",
            "step 3500: train loss 1.7399, val loss 1.9016\n",
            "step 3600: train loss 1.7281, val loss 1.8894\n",
            "step 3700: train loss 1.7331, val loss 1.8888\n",
            "step 3800: train loss 1.7217, val loss 1.8933\n",
            "step 3900: train loss 1.7243, val loss 1.8767\n",
            "step 4000: train loss 1.7129, val loss 1.8624\n",
            "step 4100: train loss 1.7130, val loss 1.8737\n",
            "step 4200: train loss 1.7068, val loss 1.8653\n",
            "step 4300: train loss 1.7010, val loss 1.8506\n",
            "step 4400: train loss 1.7108, val loss 1.8738\n",
            "step 4500: train loss 1.6927, val loss 1.8552\n",
            "step 4600: train loss 1.6860, val loss 1.8336\n",
            "step 4700: train loss 1.6841, val loss 1.8455\n",
            "step 4800: train loss 1.6687, val loss 1.8464\n",
            "step 4900: train loss 1.6703, val loss 1.8425\n",
            "step 4999: train loss 1.6681, val loss 1.8270\n",
            "\n",
            "And they bridle.\n",
            "\n",
            "STALLO:\n",
            "Gost\n",
            "be some obe to take Our my called\n",
            "My art that us hath but redilancate away, my fears,\n",
            "You her heavens, to that I commil;\n",
            "Intonceed is ensengmintlation, drevits, and the now on you son like dreeas.\n",
            "His my news: speak; ands hew you love.\n",
            "In Badieter'dle to thy would that\n",
            "mont-not what evily we arm dour wish now;\n",
            "But poor of his butt kingntstupt for treagint must with all on,\n",
            "That Prive my of.\n",
            "\n",
            "HENRY BOLINGS:\n",
            "You ardsabed.\n",
            "\n",
            "EDWARY:\n",
            "Ithen to mark, your his chan you!\n",
            "My firds anks mary.\n",
            "You contranthmes have myse.-\n",
            "And, by wear throady that such sween the begget,\n",
            "Thund frieds stime might. \n",
            "CLARENCE:\n",
            "My worst seeed Peed me: veter my asscator:\n",
            "And meety some so upon surpe enderal not.\n",
            "\n",
            "GLORD;\n",
            "Mest is good God tone.\n",
            "\n",
            "VORINIUS:\n",
            "But a would fravius with some. Down long made to lack.\n",
            "\n",
            "PRAKINGA:\n",
            "Welcome, if I it hithour\n",
            "no not with tyrees I dood time tondship the summon.\n",
            "\n",
            "QUEEN ELIZBY:\n",
            "What the silees Bliest madon so hate! you, drawatch the\n",
            "out the setcurty thee, out The can it weepeals,\n",
            "When Henrieght. My that I would not\n",
            "tands and to yet or words, not weep mines sweet worting.\n",
            "\n",
            "GRUMIO:\n",
            "On much a wordnds,\n",
            "And fries\n",
            "He bringenIniom tale the call as ince any in\n",
            "Becovel me attely true singes. Cingine.\n",
            "\n",
            "DUKE VINCENTI:\n",
            "But doth adfull, we\n",
            "that Tumselflow! thy coldreress just deer of through here.\n",
            "Her maste but, is stide, flatty dong'dlate,\n",
            "I beholds beguare to nigne, the town,\n",
            "Whave boeld holds; we it.\n",
            "\n",
            "Rumporm This beeny, delet!\n",
            "How sso, I have tall wholess enemins; I soke Hewrand pedry, say, as If has denaka-lates were not my been;\n",
            "Our servaling strard, thy Blourbeints dest try.\n",
            "\n",
            "BENVOLIO:\n",
            "His.\n",
            "Fromse manrward. Pratier as I was that becomfort,\n",
            "May oather'd;\n",
            "\n",
            "That the have blay not, these wife\n",
            "Or must that heave courcity, of munge,\n",
            "As lates her at repeses before? But I have but the peepose: life he pusict\n",
            "With is a scause; whom now.\n",
            "\n",
            "CORIOLANUMNIA:\n",
            "And now wonse than a brauge comforn,\n",
            "Wilth woarding: Inkin shown's fity.\n",
            "\n",
            "LARY VI:\n",
            "Where he wongra\n",
            "Marr\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
