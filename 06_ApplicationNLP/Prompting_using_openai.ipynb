{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15404,"status":"ok","timestamp":1720060565478,"user":{"displayName":"­송무호 / 학생 / 데이터사이언스학과","userId":"12688132877606962582"},"user_tz":-540},"id":"PdN6jjSXJJLI","outputId":"4e74b7ff-b7a1-4721-f3d7-bc981318a042"},"outputs":[],"source":["!pip install --upgrade openai"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1537,"status":"ok","timestamp":1720060567013,"user":{"displayName":"­송무호 / 학생 / 데이터사이언스학과","userId":"12688132877606962582"},"user_tz":-540},"id":"VPuwwA2KJrgr"},"outputs":[],"source":["from openai import OpenAI\n","private_key = \"\" # Your own secret key\n","organization_id = \"\" # SKI-ML Organization ID\n","project_id = \"\" # Samsung AI_Expert project ID\n","\n","client = OpenAI(api_key=private_key,\n","                organization=organization_id,\n","               )"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1720060567013,"user":{"displayName":"­송무호 / 학생 / 데이터사이언스학과","userId":"12688132877606962582"},"user_tz":-540},"id":"uBBEOZbZJ6ts"},"outputs":[],"source":["from openai import OpenAI\n","\n","def chatgpt(prompt: str) -> str:\n","  completion = client.chat.completions.create(\n","    model=\"gpt-3.5-turbo\",\n","    messages=[\n","      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","      {\"role\": \"user\", \"content\": prompt}\n","    ]\n","  )\n","  return completion.choices[0].message.content.strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":382},"executionInfo":{"elapsed":3,"status":"error","timestamp":1720060567013,"user":{"displayName":"­송무호 / 학생 / 데이터사이언스학과","userId":"12688132877606962582"},"user_tz":-540},"id":"qbFE37TrKNb4","outputId":"ef19367b-9f3f-4f1e-eac5-492177a9482e"},"outputs":[],"source":["### Test\n","\n","chatgpt(\"hello\")"]},{"cell_type":"markdown","metadata":{"id":"CkK6isM0LrV6"},"source":["# Zero-shot prompting\n","Large language models (LLMs) today, such as GPT-3.5 Turbo, GPT-4, and Claude 3, are tuned to follow instructions and are trained on large amounts of data. Large-scale training makes these models capable of performing some tasks in a \"zero-shot\" manner. Zero-shot prompting means that the prompt used to interact with the model won't contain examples or demonstrations. The zero-shot prompt directly instructs the model to perform a task without any additional examples to steer it."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1720060567013,"user":{"displayName":"­송무호 / 학생 / 데이터사이언스학과","userId":"12688132877606962582"},"user_tz":-540},"id":"7S210nuRKRca"},"outputs":[],"source":["ZERO_SHOT=\"\"\"Classify the text into neutral, negative or positive.\n","\n","Text: I think the vacation is okay.\n","Sentiment:\"\"\"\n","chatgpt(ZERO_SHOT)"]},{"cell_type":"markdown","metadata":{"id":"G7Ore2ymr_Ov"},"source":["Write your own zero shot prompt (in English or Korean)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1720060567013,"user":{"displayName":"­송무호 / 학생 / 데이터사이언스학과","userId":"12688132877606962582"},"user_tz":-540},"id":"ChDCCH0Rr3DT"},"outputs":[],"source":["YOUR_ZERO_SHOT_PROMPT=\"\"\"\n","\"\"\"\n","chatgpt(YOUR_ZERO_SHOT_PROMPT)"]},{"cell_type":"markdown","metadata":{"id":"FZ7pdAcWL21e"},"source":["# Few-shot prompting\n","While large-language models demonstrate remarkable zero-shot capabilities, they still fall short on more complex tasks when using the zero-shot setting. Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance. The demonstrations serve as conditioning for subsequent examples where we would like the model to generate a response."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1720060567013,"user":{"displayName":"­송무호 / 학생 / 데이터사이언스학과","userId":"12688132877606962582"},"user_tz":-540},"id":"i4Myh8QrL0Hj"},"outputs":[],"source":["FEW_SHOT=\"\"\"\n","Input: This is awesome!\n","Output: Positive\n","\n","Input: This is bad!\n","Output: Negative\n","\n","Input: Wow that movie was rad!\n","Output: Positive\n","\n","Input: What a horrible show!\n","Output: \"\"\"\n","chatgpt(FEW_SHOT)"]},{"cell_type":"markdown","metadata":{"id":"F5-WfOgftuUn"},"source":["Write your own few shot prompt (in English)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1720060567013,"user":{"displayName":"­송무호 / 학생 / 데이터사이언스학과","userId":"12688132877606962582"},"user_tz":-540},"id":"nYLXpyoHttQY"},"outputs":[],"source":["ZERO_SHOT=\"\"\"Find the next number in the sequence. Each number is doubled to get the next number.\n","Input: 2, 4, 8, 16\n","Output:\n","\"\"\"\n","print(chatgpt(ZERO_SHOT))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1720060567014,"user":{"displayName":"­송무호 / 학생 / 데이터사이언스학과","userId":"12688132877606962582"},"user_tz":-540},"id":"KS06WmSavQJr"},"outputs":[],"source":["YOUR_FEW_SHOT_PROMPT=\"\"\"Find the next number in the sequence. Each number is doubled to get the next number.\n","\"\"\"\n","print(chatgpt(YOUR_FEW_SHOT_PROMPT))"]},{"cell_type":"markdown","metadata":{"id":"pdTSu43uMrPs"},"source":["# Chain-of-Thought Prompting\n","Introduced in Wei et al. (2022), chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17914,"status":"aborted","timestamp":1720060567014,"user":{"displayName":"­송무호 / 학생 / 데이터사이언스학과","userId":"12688132877606962582"},"user_tz":-540},"id":"xJiKgYhnMO9c"},"outputs":[],"source":["COT=\"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n","A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n","\n","The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n","A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n","\n","The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n","A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n","\n","The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n","A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n","\n","The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n","A:\"\"\"\n","chatgpt(COT)"]},{"cell_type":"markdown","metadata":{"id":"FlBXCmZTNSFt"},"source":["## Zero-shot CoT"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17914,"status":"aborted","timestamp":1720060567014,"user":{"displayName":"­송무호 / 학생 / 데이터사이언스학과","userId":"12688132877606962582"},"user_tz":-540},"id":"GyVsdSJ3NT0N"},"outputs":[],"source":["NORMAL_PROMPT=(\"I went to the market and bought 10 apples. \"\n","               \"I gave 2 apples to the neighbor and 2 to the repairman. \"\n","               \"I then went and bought 5 more apples and ate 1. \"\n","               \"How many apples did I remain with?\")\n","chatgpt(NORMAL_PROMPT)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1720060567591,"user":{"displayName":"­송무호 / 학생 / 데이터사이언스학과","userId":"12688132877606962582"},"user_tz":-540},"id":"paSom8G1Nlqq"},"outputs":[],"source":["ZERO_SHOT_COT=(\"I went to the market and bought 10 apples. \"\n","               \"I gave 2 apples to the neighbor and 2 to the repairman. \"\n","               \"I then went and bought 5 more apples and ate 1. \"\n","               \"How many apples did I remain with? \"\n","               \"Let's think step by step.\")\n","chatgpt(ZERO_SHOT_COT)"]},{"cell_type":"markdown","metadata":{"id":"aWAyjRk7OMp4"},"source":["# Prompt Chaining\n","To improve the reliability and performance of LLMs, one of the important prompt engineering techniques is to break tasks into its subtasks. Once those subtasks have been identified, the LLM is prompted with a subtask and then its response is used as input to another prompt. This is what's referred to as prompt chaining, where a task is split into subtasks with the idea to create a chain of prompt operations."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1720060567592,"user":{"displayName":"­송무호 / 학생 / 데이터사이언스학과","userId":"12688132877606962582"},"user_tz":-540},"id":"UYTBxZMlAo41"},"outputs":[],"source":["# Define the first prompt\n","prompt_1 = \"What are the key ingredients of a successful marketing campaign?\"\n","\n","# Get the response from the first prompt\n","response_1 = chatgpt(prompt_1)\n","\n","# Define the second prompt, using the response from the first prompt as input\n","prompt_2 = f\"Based on the key ingredients you mentioned, provide specific examples of how a business can implement them in their marketing campaign. Use the following format: \\n\\nIngredient: \\nExample:\\n\\n{response_1}\"\n","\n","# Get the response from the second prompt\n","response_2 = chatgpt(prompt_2)\n","\n","# Print the responses from both prompts\n","print(f\"Response to prompt 1: {response_1}\")\n","print(f\"\\nResponse to prompt 2: {response_2}\")"]},{"cell_type":"markdown","metadata":{"id":"JzThrJgGv2xO"},"source":["Write your own chaining prompt (in English or Korean)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1720060567592,"user":{"displayName":"­송무호 / 학생 / 데이터사이언스학과","userId":"12688132877606962582"},"user_tz":-540},"id":"hEe2GLfOv2EB"},"outputs":[],"source":["prompt_1 = \"\"\n","\n","response_1 = chatgpt(prompt_1)\n","\n","prompt_2 = f\"{response_1}\"\n","\n","response_2 = chatgpt(prompt_2)\n","\n","print(f\"Response to prompt 1: {response_1}\")\n","print(f\"\\nResponse to prompt 2: {response_2}\")"]},{"cell_type":"markdown","metadata":{"id":"RAIbbs_rORgC"},"source":["# Directional Stimulus Prompting\n","Li et al., (2023) proposes a new prompting technique to better guide the LLM in generating the desired summary.\n","\n","A tuneable policy LM is trained to generate the stimulus/hint. Seeing more use of RL to optimize LLMs.\n","\n","The figure below shows how Directional Stimulus Prompting compares with standard prompting. The policy LM can be small and optimized to generate the hints that guide a black-box frozen LLM.\n","\n","![image](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fdsp.27a0005f.jpeg&w=3840&q=75)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1720060567592,"user":{"displayName":"­송무호 / 학생 / 데이터사이언스학과","userId":"12688132877606962582"},"user_tz":-540},"id":"BWKMJ6q-DtZE"},"outputs":[],"source":["!pip install rouge-score"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1720060567592,"user":{"displayName":"­송무호 / 학생 / 데이터사이언스학과","userId":"12688132877606962582"},"user_tz":-540},"id":"nnqYfdrDCzOi"},"outputs":[],"source":["from rouge_score import rouge_scorer\n","\n","def calculate_rouge1(system_summary, reference_summary):\n","    # Create a ROUGE scorer for ROUGE-1\n","    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n","    # Calculate the score\n","    scores = scorer.score(reference_summary, system_summary)\n","    return scores['rouge1']"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1720060567592,"user":{"displayName":"­송무호 / 학생 / 데이터사이언스학과","userId":"12688132877606962582"},"user_tz":-540},"id":"FaVkNbSfA3fR"},"outputs":[],"source":["input_text = \"\"\"Article: (CNN) For the first time in eight years, a TV legend returned to doing what he does best. Contestants told to \"come on down!\" on the April 1 edition of \"The Price Is Right\" encountered not host Drew Carey but another familiar face in charge of the proceedings. Instead, there was Bob Barker, who hosted the TV game show for 35 years before stepping down in 2007. Looking spry at 91, Barker handled the first price-guessing game of the show, the classic \"Lucky Seven,\" before turning hosting duties over to Carey, who finished up. Despite being away from the show for most of the past eight years, Barker didn't seem to miss a beat.\"\"\"\n","reference_text = \"\"\"Bob Barker returned to host \"The Price Is Right\" on Wednesday. Barker, 91, had retired as host in 2007.\"\"\"\n","NORMAL_PROMPT = f\"\"\"{input_text}\\n\n","Q: Summarize the above article briefly in 2-3 sentences.\n","\n","Output:\n","\"\"\"\n","DSP_PROMPT = f\"\"\"{input_text}\\n\n","Q: Summarize the above article briefly in 2-3 sentences based on the hint.\n","Hint: Bob Barker; TV; April 1; \"The Price Is Right\"; 2007; 91.\n","\n","Output:\n","\"\"\"\n","normal_summary = chatgpt(NORMAL_PROMPT)\n","dsp_summary = chatgpt(DSP_PROMPT)\n","normal_rouge1_score = calculate_rouge1(normal_summary, reference_text)\n","dsp_rouge1_score = calculate_rouge1(dsp_summary, reference_text)\n","print(\"ROUGE-1 Score (Normal Prompt):\", normal_rouge1_score)\n","print(\"ROUGE-1 Score (DSP Prompt):\", dsp_rouge1_score)\n"]},{"cell_type":"markdown","metadata":{"id":"zmqQAqQxOimL"},"source":["# ReACT Prompting\n","ReAct is inspired by the synergies between \"acting\" and \"reasoning\" which allow humans to learn new tasks and make decisions or reasoning.\n","\n","Chain-of-thought (CoT) prompting has shown the capabilities of LLMs to carry out reasoning traces to generate answers to questions involving arithmetic and commonsense reasoning, among other tasks (Wei et al., 2022). But its lack of access to the external world or inability to update its knowledge can lead to issues like fact hallucination and error propagation.\n","\n","ReAct is a general paradigm that combines reasoning and acting with LLMs. ReAct prompts LLMs to generate verbal reasoning traces and actions for a task. This allows the system to perform dynamic reasoning to create, maintain, and adjust plans for acting while also enabling interaction to external environments (e.g., Wikipedia) to incorporate additional information into the reasoning. The figure below shows an example of ReAct and the different steps involved to perform question answering."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1720060567592,"user":{"displayName":"­송무호 / 학생 / 데이터사이언스학과","userId":"12688132877606962582"},"user_tz":-540},"id":"ebqaRjR6FuTg"},"outputs":[],"source":["!pip install -U langchain-community langchain-core\n","!pip install -U google-search-results\n","!pip install -U langchain-openai\n","!pip install -U wikipedia"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1720060567592,"user":{"displayName":"­송무호 / 학생 / 데이터사이언스학과","userId":"12688132877606962582"},"user_tz":-540},"id":"Pz7saQkENDDc"},"outputs":[],"source":["import os\n","from langchain_openai import ChatOpenAI\n","from langchain_community.utilities import GoogleSerperAPIWrapper\n","from langchain.agents import load_tools\n","from langchain.agents import initialize_agent, Tool"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1720060567592,"user":{"displayName":"­송무호 / 학생 / 데이터사이언스학과","userId":"12688132877606962582"},"user_tz":-540},"id":"ZmAqCZZwyOIK"},"outputs":[],"source":["from langchain.agents.react.base import DocstoreExplorer\n","from langchain.agents import AgentType\n","from langchain import Wikipedia\n","docstore=DocstoreExplorer(Wikipedia())\n","tools = [\n","    Tool(\n","        name=\"Search\",\n","        func=docstore.search,\n","        description=\"useful for when you need to ask with search\"\n","    ),\n","    Tool(\n","        name=\"Lookup\",\n","        func=docstore.lookup,\n","        description=\"useful for when you need to ask with lookup\"\n","    )\n","]\n","\n","llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", api_key=private_key, organization=organization_id)\n","react = initialize_agent(tools, llm, agent=AgentType.REACT_DOCSTORE, verbose=True, handle_parsing_errors=True)\n","\n","react.run(\"Author David Chanoff has collaborated with a U.S. Navy admiral who served as the ambassador to the United Kingdom under which President?\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"aborted","timestamp":1720060567593,"user":{"displayName":"­송무호 / 학생 / 데이터사이언스학과","userId":"12688132877606962582"},"user_tz":-540},"id":"Rk-Hiu7eNzdf"},"outputs":[],"source":["## Print template used by LangChain\n","print(react.agent.llm_chain.prompt.template)"]}],"metadata":{"colab":{"provenance":[{"file_id":"17D3eUcaAjNw-5zKRFUVo4XCmU1MOBHKX","timestamp":1719812835723}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
