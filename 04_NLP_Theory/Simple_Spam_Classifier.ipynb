{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skCfWpwydiYY"
      },
      "source": [
        "# Week1: Simple Spam Mail Classifier\n",
        "\n",
        "## 1. Introduction to NLP\n",
        "\n",
        "Let's have a taste of NLP by exploring how ChatGPT, which is widely used these days, began the era of Large Language Models.\n",
        "\n",
        "Using GPT2 to understand how ChatGPT works internally.\n",
        "\n",
        "## 2. Spam Mail Classifier\n",
        "\n",
        "Create a spam mail classifier based on what's learned in the lecture.\n",
        "\n",
        "Building the model in the order presented in the lecture and observing the results.\n",
        "\n",
        "#### a. Rule-Based Classifier without NLP\n",
        "\n",
        "Create a classifier using a simple rule without NLP.\n",
        "\n",
        "#### 0. Evaluation\n",
        "\n",
        "Evaluate the results of the model created earlier.\n",
        "\n",
        "#### b. Basic Word Processing!\n",
        "\n",
        "Enhance the classifier using simple NLP techniques.\n",
        "\n",
        "#### c. Word Preprocessing\n",
        "\n",
        "Implementing how to preprocess words according to what's learned in the lecture, so that classification can be as accurate as possible without exceptions.\n",
        "\n",
        "#### d. Similarity-Based Processing\n",
        "\n",
        "Further improve the NLP-based model using similarity functions.\n",
        "\n",
        "## 3. [Extra] Tokenization with BERT\n",
        "\n",
        "Represent the tokenization from 2-c. as vectors using BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hK0v2tmOB7L"
      },
      "source": [
        "#### Package Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wjm4FsGoOAnT"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet gdown pandas nltk fast-edit-distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDXARMJYqqyu",
        "outputId": "8afc4642-bbdb-483d-aa8f-d176a91808d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/devrok/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/devrok/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/devrok/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# nltk package init for lemmatization, stemming\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77dSU75spGmO"
      },
      "source": [
        "## 1. Introduction to NLP\n",
        "\n",
        "Natural Language Processing (NLP) stands at the forefront of modern artificial intelligence, enabling machines to understand, interpret, and generate human language. In this section, we embark on a journey to explore the fundamentals of NLP and its practical applications, with a focus on ChatGPT, one of the most widely used models in the field.\n",
        "\n",
        "### Understanding the Significance of NLP\n",
        "\n",
        "NLP has revolutionized various domains, including machine translation, sentiment analysis, question answering systems, and text summarization. Its applications range from enhancing search engines to enabling virtual assistants like Siri and Alexa to interact with users in natural language.\n",
        "\n",
        "### ChatGPT: A Glimpse into the Era of Large Language Models (LLMs)\n",
        "\n",
        "We delve into the evolution of NLP with the advent of Large Language Models (LLMs) like ChatGPT. These models, powered by vast amounts of data and advanced neural network architectures such as GPT-2 and GPT-3, have pushed the boundaries of natural language understanding and generation.\n",
        "\n",
        "### Objectives of the Section\n",
        "\n",
        "- Introduce the fundamental concepts of NLP.\n",
        "- Highlight the transformative impact of NLP on various industries.\n",
        "- Explore the role of ChatGPT in advancing NLP technology.\n",
        "- Familiarize ourselves with the workings of GPT-2 as a precursor to ChatGPT.\n",
        "\n",
        "### Key Topics Covered\n",
        "\n",
        "- Overview of NLP: Basic concepts and applications.\n",
        "- Evolution of Large Language Models: From GPT-2 to ChatGPT.\n",
        "- Exploring GPT-2: Understanding its architecture and capabilities.\n",
        "\n",
        "### Why It Matters\n",
        "\n",
        "Understanding the basics of NLP and the underlying mechanisms of LLMs like ChatGPT is crucial for anyone venturing into the field of artificial intelligence, data science, or natural language understanding. This section sets the stage for the practical experimentation and model building exercises that follow, laying a solid foundation for further exploration into the exciting realm of NLP.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9jwriPtSNWl"
      },
      "source": [
        "### Experimenting with GPT2\n",
        "\n",
        "To gain insights into the workings of ChatGPT, we leverage GPT-2, a precursor to ChatGPT, to explore its internal mechanisms. By understanding how GPT-2 processes text and generates responses, we lay the groundwork for building our own NLP-based solutions in subsequent sections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NTRFSeWUpFhO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# model, tokenization init.\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\") # model과\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") # tokenizer 다운"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "C0rYOde3pXKT"
      },
      "outputs": [],
      "source": [
        "prompt = \"GPT2 is a model developed by OpenAI.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLSUfJZ_pYRL",
        "outputId": "6419041f-49c9-4b49-8c19-60935d7148da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[   38, 11571,    17,   318,   257,  2746,  4166,   416,  4946, 20185,\n",
              "            13]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# string -> vector\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids # return_tensors=\"pt\" : tokenizer를 pytorch로 변환\n",
        "input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg_cjvTOQ2XU",
        "outputId": "6e779c8a-7c9b-4fd2-cfe3-88d5af4bb728"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('G', 'PT', '2', ' is')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tokenizer 확인\n",
        "tokenizer.decode(38), tokenizer.decode(11571), tokenizer.decode(17), tokenizer.decode(318)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at7K4TZSpZx8",
        "outputId": "db0e6917-7c5a-481f-dd18-92ccf79c764d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[   38, 11571,    17,   318,   257,  2746,  4166,   416,  4946, 20185,\n",
              "            13,   383,  3061,   286,   428,  3348,   318,   284,  2423,   262,\n",
              "          1459,  1181,   286,  3725,  5115, 17019,  7686,   290,   703,   484,\n",
              "           460,   307,  5625,   284,   584,  4959,    13,   770,  3348, 12932,\n",
              "           284,  2148,   257,   517,  9432,  6764,   286,   262,  4096, 10838,\n",
              "           416,   543,   428,   318,  5625,   284, 17019,  7686,    13,   770,\n",
              "          3164,   468,   587,  9713,   287,   428,  3348,   867,  1661,    13,\n",
              "           383,  4031,   286,   428,  3348,   318,   284, 11589,  2209,   262,\n",
              "          2276,  2428,  4376,   290,   284, 18077,   257,   517,  6496,  3164,\n",
              "            13, 50256]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# vector -> GPT2 -> vector\n",
        "gen_tokens = model.generate(\n",
        "    input_ids, # token화된 문장\n",
        "    do_sample=True, # sampling을 이용하여 단어를 무작위로 선택\n",
        "    temperature=0.9, # 1에 가까울수록 확률이 높은 단어 선택\n",
        "    max_length=100,\n",
        ")\n",
        "\n",
        "gen_tokens\n",
        "#[38, 11571, 17, 318, 257, 2746, 4166, 416, 4946, 20185, 13, .... ]을 보고 뒷부분을 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "SJu7ejtppbKr",
        "outputId": "4d7e2619-b0f4-4a33-82f3-d7d8b29f7cb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'GPT2 is a model developed by OpenAI. The goal of this paper is to review the current state of knowledge regarding neural networks and how they can be applied to other purposes. This paper seeks to provide a more objective description of the basic concepts by which this is applied to neural networks. This approach has been studied in this paper many times. The aim of this paper is to briefly address the general issues raised and to propose a more detailed approach.<|endoftext|>'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# vector -> string\n",
        "gen_text = tokenizer.batch_decode(gen_tokens)[0] # list 자료 형태 [0] 없어도 됨\n",
        "gen_text # 그럴듯한 문장 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUkdVAgvp84F"
      },
      "source": [
        "### Experimenting with ChatGPT\n",
        "\n",
        "We conduct hands-on experiments with ChatGPT to gain practical insights into its capabilities and behavior. By interacting with ChatGPT and observing its responses, we deepen our understanding of its language processing abilities and explore its potential applications in various scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HjsYrxStqM8Y"
      },
      "outputs": [],
      "source": [
        "# 각각의 gpt가 답변한 내용들\n",
        "gpt3_5 = \"Close! It’s actually GPT-3, the third iteration of the Generative Pre-trained Transformer model, developed by OpenAI. GPT-2 is the predecessor to GPT-3.\"\n",
        "\n",
        "gpt4 = \"\"\"\n",
        "Yes, that's correct!\n",
        "GPT-2, or the second version of the Generative Pre-trained Transformer, is a model developed by OpenAI.\n",
        "It's an advanced language processing AI model that uses a transformer-based machine learning technique to generate human-like text.\n",
        "GPT-2 can comprehend and generate coherent and contextually relevant text based on a given prompt, making it useful for a variety of applications like text completion, summarization, and even generating creative writing.\n",
        "\"\"\"\n",
        "\n",
        "gpt4o = \"\"\"\n",
        "GPT-2 is a language model developed by OpenAI. It is part of a family of Generative Pre-trained Transformers (GPT) that use deep learning techniques to generate human-like text based on the input provided. Here are some key points about GPT-2:\n",
        "\n",
        "Release Date: GPT-2 was introduced by OpenAI in February 2019.\n",
        "Architecture: It uses a transformer architecture, which is a type of deep learning model designed for sequence-to-sequence tasks, such as translation and text generation.\n",
        "Training Data: GPT-2 was trained on a diverse dataset of internet text. The dataset is large and varied, enabling the model to generate coherent and contextually relevant text across a wide range of topics.\n",
        "Size Variants: GPT-2 comes in several size variants, with different numbers of parameters (the weights in the model):\n",
        "117 million parameters\n",
        "345 million parameters\n",
        "762 million parameters\n",
        "1.5 billion parameters (the largest version)\n",
        "Capabilities: GPT-2 can perform a variety of natural language processing tasks, such as text completion, translation, summarization, and question answering, without needing task-specific training data.\n",
        "Controversy and Concerns: Upon its release, there were concerns about the potential misuse of GPT-2, such as generating misleading news or spam. This led OpenAI to initially withhold the full model and release it gradually, starting with smaller versions and finally releasing the full model after thorough evaluation of its impact.\n",
        "Successor: GPT-2 was succeeded by GPT-3, which is significantly larger and more powerful, with 175 billion parameters. GPT-3 further enhanced the capabilities demonstrated by GPT-2.\n",
        "Overall, GPT-2 marked a significant advancement in the field of natural language processing and paved the way for subsequent developments in AI language models.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoNoJUm8TktA"
      },
      "source": [
        "## 2. Spam Mail Classifier\n",
        "\n",
        "In this section, we focus on building a spam mail classifier using various techniques learned in the lecture. We follow a structured approach to develop and evaluate the classifier's performance, starting from simple rule-based methods to more sophisticated NLP-based models. Through this process, we aim to understand the effectiveness of different approaches in accurately identifying and classifying spam emails.\n",
        "\n",
        "### Objectives:\n",
        "- Develop a spam mail classifier based on lecture materials.\n",
        "- Evaluate classifier performance using different techniques.\n",
        "- Compare effectiveness of rule-based and NLP-based approaches.\n",
        "\n",
        "### Key Steps:\n",
        "1. **Rule-Based Classifier without NLP:** Implement a basic classifier using predefined rules.\n",
        "2. **Basic Word Processing:** Enhance classifier using simple NLP techniques.\n",
        "3. **Word Preprocessing:** Implement advanced word preprocessing methods for improved classification.\n",
        "4. **Similarity-Based Processing:** Further refine classifier using similarity functions and NLP-based techniques.\n",
        "\n",
        "### Evaluation:\n",
        "- Assess classifier performance at each stage.\n",
        "- Compare results to determine effectiveness of different approaches.\n",
        "- Identify strengths and limitations of rule-based and NLP-based methods.\n",
        "\n",
        "### Importance:\n",
        "Developing a spam mail classifier provides practical insights into applying NLP techniques for text classification tasks. This section enables us to understand the challenges involved in spam detection and the role of NLP in addressing them, contributing to our overall understanding of natural language processing applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDt3E8OcgRsy"
      },
      "source": [
        "RAG(Real-time Abstractive Generation)는 최근 자연어 처리(NLP) 분야에서 사용되는 기술 중 하나입니다. 이 기술은 주어진 질문에 대해 실시간으로 관련된 정보를 검색하여 요약하고, 이를 바탕으로 응답을 생성하는 방식으로 동작합니다. RAG의 주요 구성 요소는 다음과 같습니다:\n",
        "\n",
        "Retriever: 주어진 질문에 대한 관련 문서를 검색하는 역할을 합니다. 검색 엔진이나 데이터베이스에서 관련 정보를 찾아냅니다.\n",
        "Generator: 검색된 문서를 바탕으로 응답을 생성하는 역할을 합니다. GPT-3와 같은 대규모 언어 모델을 사용하여 자연스럽고 일관성 있는 텍스트를 생성합니다.\n",
        "이러한 RAG 모델은 특히 대화형 AI, 질문 응답 시스템, 챗봇 등에서 유용하게 사용될 수 있습니다. 예를 들어, 특정 주제에 대한 질문이 주어지면, 먼저 관련 정보를 검색하고, 그 정보를 요약하여 사용자에게 적절한 응답을 제공하는 방식입니다.\n",
        "\n",
        "RAG는 기존의 단순한 정보 검색 모델보다 더 정교하고, 사용자가 원하는 정보를 더 정확하게 제공할 수 있다는 장점이 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqhJow8-zQAX"
      },
      "source": [
        "### Prepare Dataset\n",
        "\n",
        "* Spam Mail Dataset\n",
        "[Dataset Reference](https://www.kaggle.com/datasets/venky73/spam-mails-dataset?resource=download&SSORegistrationToken=CfDJ8B5GsGLMFaFLm6_4BsA80RS4BAT0LRVbH6iNxvo5aSOHX-1pg5QGq6ge5mtJU8nMIokHC1zedaT2IXaj98xFPqxsKGkKG4FktgUQIoWDCGIz6XGDNs5_0Y4gPevVU07z61T2d0z6mY1g98ljAzOE-DlkPT8k1zBPqyzX_d7yQJMR_49mt5pxs-_6WdFhdSu1B9pFDEnajFKk3QF791dEJp_4ok6NubwwwPyfQQ-O6yBAi9Z9VpkXVBtMIzXMO7_FeAidYdW5uSivFrnz2hmfv0yxmfq-GgNYXfPoAMvasqdAktgKRiFSI-GZATODMfnjFq1q40TDOQu7ZGg8b_9pwx40zhTxCNU9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "sruK3HY1EW5U",
        "outputId": "716143b2-68af-4b3e-de04-f3f9da416581"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Ua1q2hOfN25B6H8PDWXl4yViE_cNj3mp\n",
            "To: /home/devrok/aiexpert/ai-nlp/04_NLP_Theory/spam_dataset.tsv\n",
            "100%|██████████| 5.50M/5.50M [00:00<00:00, 16.3MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'spam_dataset.tsv'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# download spam/ham dataset\n",
        "import gdown\n",
        "from pathlib import Path\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1Ua1q2hOfN25B6H8PDWXl4yViE_cNj3mp'\n",
        "dataset_path = Path('./spam_dataset.tsv')\n",
        "gdown.download(url, str(dataset_path), quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn8-WynHzWvI"
      },
      "source": [
        "### Dataset Inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "w_uCP6J0fxRW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(dataset_path, sep=\"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "sjG8K10dP4RX",
        "outputId": "fd1aa98b-1620-42b0-d311-7cbec3822f50"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>label_num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>605</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2349</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3624</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4685</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2030</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5166</th>\n",
              "      <td>1518</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: put the 10 on the ft\\r\\nthe transport...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5167</th>\n",
              "      <td>404</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: 3 / 4 / 2000 and following noms\\r\\nhp...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5168</th>\n",
              "      <td>2933</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: calpine daily gas nomination\\r\\n&gt;\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5169</th>\n",
              "      <td>1409</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: industrial worksheets for august 2000...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5170</th>\n",
              "      <td>4807</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: important online banking alert\\r\\ndea...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5171 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0 label                                               text  \\\n",
              "0            605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
              "1           2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
              "2           3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
              "3           4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
              "4           2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
              "...          ...   ...                                                ...   \n",
              "5166        1518   ham  Subject: put the 10 on the ft\\r\\nthe transport...   \n",
              "5167         404   ham  Subject: 3 / 4 / 2000 and following noms\\r\\nhp...   \n",
              "5168        2933   ham  Subject: calpine daily gas nomination\\r\\n>\\r\\n...   \n",
              "5169        1409   ham  Subject: industrial worksheets for august 2000...   \n",
              "5170        4807  spam  Subject: important online banking alert\\r\\ndea...   \n",
              "\n",
              "      label_num  \n",
              "0             0  \n",
              "1             0  \n",
              "2             0  \n",
              "3             1  \n",
              "4             0  \n",
              "...         ...  \n",
              "5166          0  \n",
              "5167          0  \n",
              "5168          0  \n",
              "5169          0  \n",
              "5170          1  \n",
              "\n",
              "[5171 rows x 4 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check how data looks like\n",
        "df # label_num -> 0 : ham, 1 : spam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "S0H7hv53Ss1j",
        "outputId": "fc4c35c1-7d03-42d2-bac6-9b24adf56074"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @title label\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "df.groupby('label').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB6jb1Ojk4RA",
        "outputId": "4f145407-e8f5-4f4f-c545-8fc02d86cd0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5171 entries, 0 to 5170\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   Unnamed: 0  5171 non-null   int64 \n",
            " 1   label       5171 non-null   object\n",
            " 2   text        5171 non-null   object\n",
            " 3   label_num   5171 non-null   int64 \n",
            "dtypes: int64(2), object(2)\n",
            "memory usage: 161.7+ KB\n"
          ]
        }
      ],
      "source": [
        "# check types\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "cdhiqZbYSUZP",
        "outputId": "e526efc3-4869-4b47-ad62-447d5f90f3ea"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# distribution inspection\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "df.groupby('label').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "A9-W9YhDSpN8",
        "outputId": "de005df6-558d-48db-c05d-82b002fdd9a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Subject: photoshop , windows , office . cheap . main trending\\r\\nabasements darer prudently fortuitous undergone\\r\\nlighthearted charm orinoco taster\\r\\nrailroad affluent pornographic cuvier\\r\\nirvin parkhouse blameworthy chlorophyll\\r\\nrobed diagrammatic fogarty clears bayda\\r\\ninconveniencing managing represented smartness hashish\\r\\nacademies shareholders unload badness\\r\\ndanielson pure caffein\\r\\nspaniard chargeable levin\\r\\n'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# text_example\n",
        "df.iloc[3][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qr9py3GCE2b"
      },
      "source": [
        "### a. Rule-Based Classifier without NLP\n",
        "\n",
        "In this subsection, we begin by implementing a basic spam mail classifier using a rule-based approach without relying on NLP techniques. We define simple rules or criteria based on known characteristics of spam emails, such as **length** of the email. By employing this straightforward method, we aim to establish a baseline for spam classification and evaluate its effectiveness in distinguishing between spam and ham emails.\n",
        "\n",
        "### Objectives:\n",
        "- Develop a rule-based classifier for spam mail detection.\n",
        "- Define rules based on common characteristics of spam emails.\n",
        "\n",
        "### Key Steps:\n",
        "1. **Rule Definition:** Identify key features or patterns indicative of spam emails.\n",
        "2. **Classifier Implementation:** Translate rules into algorithmic logic for classification.\n",
        "\n",
        "\n",
        "### Importance:\n",
        "Building a rule-based classifier provides a fundamental understanding of spam detection methods and serves as a benchmark for comparing more sophisticated approaches. This subsection lays the groundwork for exploring advanced NLP techniques in subsequent sections, contributing to our overall understanding of spam mail classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "v0XR6GJjSzsG",
        "outputId": "b729da49-ae1d-4294-f227-201625be6785"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>label_num</th>\n",
              "      <th>model_without_nlp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>605</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2349</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3624</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4685</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2030</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5166</th>\n",
              "      <td>1518</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: put the 10 on the ft\\r\\nthe transport...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5167</th>\n",
              "      <td>404</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: 3 / 4 / 2000 and following noms\\r\\nhp...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5168</th>\n",
              "      <td>2933</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: calpine daily gas nomination\\r\\n&gt;\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5169</th>\n",
              "      <td>1409</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: industrial worksheets for august 2000...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5170</th>\n",
              "      <td>4807</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: important online banking alert\\r\\ndea...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5171 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0 label                                               text  \\\n",
              "0            605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
              "1           2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
              "2           3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
              "3           4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
              "4           2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
              "...          ...   ...                                                ...   \n",
              "5166        1518   ham  Subject: put the 10 on the ft\\r\\nthe transport...   \n",
              "5167         404   ham  Subject: 3 / 4 / 2000 and following noms\\r\\nhp...   \n",
              "5168        2933   ham  Subject: calpine daily gas nomination\\r\\n>\\r\\n...   \n",
              "5169        1409   ham  Subject: industrial worksheets for august 2000...   \n",
              "5170        4807  spam  Subject: important online banking alert\\r\\ndea...   \n",
              "\n",
              "      label_num  model_without_nlp  \n",
              "0             0                  0  \n",
              "1             0                  0  \n",
              "2             0                  1  \n",
              "3             1                  0  \n",
              "4             0                  0  \n",
              "...         ...                ...  \n",
              "5166          0                  0  \n",
              "5167          0                  1  \n",
              "5168          0                  0  \n",
              "5169          0                  0  \n",
              "5170          1                  1  \n",
              "\n",
              "[5171 rows x 5 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# rule definition\n",
        "# 규칙기반으로 spam mail 분류\n",
        "length_threshold = 750\n",
        "\n",
        "df[\"model_without_nlp\"] = df[\"text\"].apply(lambda txt: 1 if len(txt) > length_threshold else 0) # 1 : sapm, 0 : not spam\n",
        "# 특정 길이 이상은 spam으로 분류\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "kI1MkLGJUcMw",
        "outputId": "a39e89b1-049c-4d9f-d2b5-a6471f1016fb"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# distribution check\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "df.groupby('model_without_nlp').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phKaybdbzPSA"
      },
      "source": [
        "### b. Evaluation Metrics\n",
        "Implement followings as function.\n",
        "\n",
        "#### Precision\n",
        "Precision quantifies the proportion of correctly classified spam emails out of all emails classified as spam. It is calculated as the ratio of true positives (correctly classified spam emails) to the sum of true positives and false positives (legitimate emails incorrectly classified as spam). Precision helps assess the classifier's ability to avoid misclassifying legitimate emails as spam.\n",
        "\n",
        "#### Recall\n",
        "Recall, also known as sensitivity, measures the proportion of correctly classified spam emails out of all actual spam emails. It is calculated as the ratio of true positives to the sum of true positives and false negatives (spam emails incorrectly classified as legitimate). Recall indicates the classifier's ability to identify all instances of spam.\n",
        "\n",
        "#### F1 Score\n",
        "The F1 score is the harmonic mean of precision and recall, providing a balanced evaluation metric that considers both false positives and false negatives. It is calculated as the weighted average of precision and recall, with a higher value indicating better overall performance. The F1 score is particularly useful when dealing with imbalanced datasets, where precision and recall may provide conflicting insights.\n",
        "\n",
        "#### Importance:\n",
        "Evaluating the classifier using multiple metrics, including F1 score, ensures a comprehensive understanding of its performance across different aspects of classification. By considering precision, recall, and their harmonic mean, we gain insights into the classifier's ability to accurately detect spam while minimizing false positives and false negatives.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SE7VDyYqFsK",
        "outputId": "9e2c2459-8790-45e8-d768-708364b03ca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<zip object at 0x73d6e068f580>\n",
            "('a', 1) ('b', 2) ('c', 3)\n"
          ]
        }
      ],
      "source": [
        "# zip test\n",
        "a1 = [\"a\", \"b\", \"c\"]\n",
        "b1 = [1, 2, 3]\n",
        "\n",
        "print(zip(a1,b1))\n",
        "print(*zip(a1,b1))\n",
        "# tuple쌍으로 만들어주는 iterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Zxjr8paGV4pV"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "def confusion_matrix(reals: List[int], predicts: List[int]): #def confusion_matrix(arg1 name: arg1 type, ...):\n",
        "  tp, fp, fn, tn = 0, 0, 0, 0\n",
        "  for r_val, p_val in zip(reals, predicts):\n",
        "    if r_val == 1:\n",
        "      if p_val == 1:\n",
        "        tp += 1\n",
        "      else:\n",
        "        fn += 1\n",
        "    else:\n",
        "      if p_val == 1:\n",
        "        fp += 1\n",
        "      else:\n",
        "        tn += 1\n",
        "  print(f\"total: {tp + fp + fn + tn}\")\n",
        "  return tp, fp, fn, tn\n",
        "\n",
        "# !!! be aware of zero-division !!!\n",
        "def precision(tp, fp, fn, tn):\n",
        "  return tp / (tp + fp) if (tp + fp) != 0 else 0\n",
        "\n",
        "# !!! be aware of zero-division !!!\n",
        "def recall(tp, fp, fn, tn):\n",
        "  return tp / (tp + fn) if (tp + fn) != 0 else 0\n",
        "\n",
        "# !!! be aware of zero-division !!!\n",
        "def f1_score(tp, fp, fn, tn):\n",
        "  p = precision(tp, fp, fn, tn)\n",
        "  r = recall(tp, fp, fn, tn)\n",
        "  return 2*p*r / (p + r) if (p + r) != 0 else 0\n",
        "\n",
        "def print_metrics(tp, fp, fn, tn):\n",
        "  p = precision(tp, fp, fn, tn)\n",
        "  r = recall(tp, fp, fn, tn)\n",
        "  f1 = f1_score(tp, fp, fn, tn)\n",
        "\n",
        "  print(f\"tp: {tp}, fp: {fp}, fn: {fn}, tn: {tn}\")\n",
        "  print(f\"precision: {p}, recall: {r}, f1: {f1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rMCCu5beZE6",
        "outputId": "fbb920b1-ae11-4b10-a9a4-a57b679d5740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total: 5171\n",
            "tp: 595, fp: 1442, fn: 904, tn: 2230\n",
            "precision: 0.2920962199312715, recall: 0.39693128752501666, f1: 0.33653846153846156\n"
          ]
        }
      ],
      "source": [
        "# evaluation\n",
        "real_labels = df[\"label_num\"].tolist() # 정답 label\n",
        "without_nlp_predicts = df[\"model_without_nlp\"].tolist() # rule based로 예측된 label\n",
        "\n",
        "print_metrics(*confusion_matrix(real_labels, without_nlp_predicts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtKQyD_BkZ6N",
        "outputId": "dd452670-7b1b-4b54-fcef-4d343042ba47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total: 5171\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(595, 1442, 904, 2230)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "confusion_matrix(real_labels, without_nlp_predicts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yes75rSjzYq_"
      },
      "source": [
        "### c. Basic Word Processing\n",
        "\n",
        "In this subsection, we enhance the spam mail classifier by incorporating basic natural language processing (NLP) techniques. By leveraging simple NLP methods, such as tokenization and stemming, we aim to improve the classifier's ability to understand and process textual data. This enhancement allows us to capture more nuanced features of spam emails and refine the classification process for better accuracy and effectiveness.\n",
        "\n",
        "### Objectives:\n",
        "- Apply basic NLP techniques to preprocess textual data.\n",
        "- Enhance the classifier's ability to capture relevant features of spam emails.\n",
        "- Improve classification accuracy and effectiveness compared to the rule-based approach.\n",
        "\n",
        "### Key Steps:\n",
        "1. **Keywords:** Check whether the keyword is in the mail. If the keyword is in the mail filter as spam and otherwise ham.\n",
        "2. **Regex:** Use regex to catch the keywords.\n",
        "3. **Edit Distance:**: Utilizing the edit distance to catch some words that is modified.\n",
        "4. **NLTK**: Use NLTK to improve method 1-3.\n",
        "  1. **Text Tokenization:** Break down email text into individual tokens or words to facilitate analysis and processing.\n",
        "  2. **Stemming or Lemmatization:** Reduce words to their root form to standardize vocabulary and improve feature extraction.\n",
        "  3. **Stopword Removal:** Eliminate common words (stopwords) that carry little semantic meaning to focus on informative content.\n",
        "\n",
        "### Evaluation:\n",
        "- Assess classifier performance using the same evaluation metrics as in Section 2-a (Accuracy, Precision, Recall, F1 Score).\n",
        "- Compare results with the rule-based classifier to measure improvement in classification accuracy and effectiveness.\n",
        "\n",
        "### Importance:\n",
        "Incorporating basic NLP techniques enhances the classifier's ability to understand and process textual data, leading to more accurate spam classification. This subsection lays the foundation for exploring advanced NLP methods in subsequent sections, further improving the classifier's performance and contributing to our understanding of text classification tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e47i9T46W8RS"
      },
      "source": [
        "#### c-1. Keywords\n",
        "Implement the following logic:\n",
        "If keyword is in the mail, classify it as spam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiJQEWJUxxKC",
        "outputId": "bcd409b2-ade9-4688-9bf5-1e8e0c56b214"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Keyword Reference: https://lix-it.com/blog/spam-trigger-words/#:~:text=Some%20common%20spam%20trigger%20words,entice%20the%20recipient%20into%20taking\n",
        "keywords = ['$', '$$$', 'affordable', 'amazing', 'apply', 'bargain', 'bonus', 'buy', 'buy', 'call', 'cards', 'cash', 'cash', 'certified', 'chance', 'cheap', 'clearance', 'click', 'compare', 'credit', 'deal', 'discount', 'fantastic', 'free', 'great', 'instant', 'lowest price', 'marketing solutions', 'no questions asked', 'open', 'order', 'prize', 'sale', 'sales', 'sample', 'satisfaction', 'save', 'sign up', 'solution', 'special ', 'subscribe', 'success', 'trial', 'unlimited', 'win', 'winner', 'act', 'brand', 'congratulations', 'consultation', 'expires', 'free', 'gift', 'guaranteed', 'lifetime', 'limited', 'membership', 'millions', 'new', 'obligation', 'offer', 'offer', 'only', 'order', 'pay', 'percent', 'price', 'promotion', 'rates', 'refund', 'sales', 'satisfaction', 'satisfied', 'supplies', 'trial', 'urgent', 'winner']\n",
        "# spam mail에 들어있는 keyword\n",
        "keywords = list(set(keywords)) # set 중복요소 제거\n",
        "len(keywords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFQ3p0nZrfoM",
        "outputId": "574a0c99-51c8-47b9-ca5e-de3813a822ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['click',\n",
              " '$$$',\n",
              " 'lifetime',\n",
              " 'rates',\n",
              " 'sales',\n",
              " 'clearance',\n",
              " 'certified',\n",
              " 'obligation',\n",
              " 'consultation',\n",
              " 'sample']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "keywords[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "LGKkm12tfoGu",
        "outputId": "3374ae8b-d886-4281-8aa9-999329ea7269"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>label_num</th>\n",
              "      <th>model_without_nlp</th>\n",
              "      <th>model_rule_based</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>605</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2349</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3624</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4685</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2030</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5166</th>\n",
              "      <td>1518</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: put the 10 on the ft\\r\\nthe transport...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5167</th>\n",
              "      <td>404</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: 3 / 4 / 2000 and following noms\\r\\nhp...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5168</th>\n",
              "      <td>2933</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: calpine daily gas nomination\\r\\n&gt;\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5169</th>\n",
              "      <td>1409</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: industrial worksheets for august 2000...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5170</th>\n",
              "      <td>4807</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: important online banking alert\\r\\ndea...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5171 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0 label                                               text  \\\n",
              "0            605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
              "1           2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
              "2           3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
              "3           4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
              "4           2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
              "...          ...   ...                                                ...   \n",
              "5166        1518   ham  Subject: put the 10 on the ft\\r\\nthe transport...   \n",
              "5167         404   ham  Subject: 3 / 4 / 2000 and following noms\\r\\nhp...   \n",
              "5168        2933   ham  Subject: calpine daily gas nomination\\r\\n>\\r\\n...   \n",
              "5169        1409   ham  Subject: industrial worksheets for august 2000...   \n",
              "5170        4807  spam  Subject: important online banking alert\\r\\ndea...   \n",
              "\n",
              "      label_num  model_without_nlp  model_rule_based  \n",
              "0             0                  0                 1  \n",
              "1             0                  0                 0  \n",
              "2             0                  1                 1  \n",
              "3             1                  0                 1  \n",
              "4             0                  0                 1  \n",
              "...         ...                ...               ...  \n",
              "5166          0                  0                 1  \n",
              "5167          0                  1                 1  \n",
              "5168          0                  0                 0  \n",
              "5169          0                  0                 1  \n",
              "5170          1                  1                 1  \n",
              "\n",
              "[5171 rows x 6 columns]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# logic\n",
        "def keyword_in_text(text):\n",
        "  return 1 if any(keyword in text for keyword in keywords) else 0\n",
        "  #for keyword in keywords:\n",
        "  #  if keyword in text:\n",
        "  #    return 1\n",
        "  #return 0\n",
        "\n",
        "df[\"model_rule_based\"] = df[\"text\"].apply(lambda txt: keyword_in_text(txt))\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8v9WuEAqsBY4",
        "outputId": "cc78c4a2-84e5-4882-ead1-f4e85da266bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(True, False)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# membership 연산자 in\n",
        "a = [1,2,3]\n",
        "1 in a, 5 in a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MiywXyFksGV",
        "outputId": "59f5dd75-5b63-4746-acf0-ca61d91a4f92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total: 5171\n",
            "tp: 1246, fp: 2722, fn: 253, tn: 950\n",
            "precision: 0.31401209677419356, recall: 0.8312208138759173, f1: 0.4558258642765685\n"
          ]
        }
      ],
      "source": [
        "# evaluation\n",
        "rule_based_predicts = df[\"model_rule_based\"].tolist()\n",
        "\n",
        "print_metrics(*confusion_matrix(real_labels, rule_based_predicts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd9Rznlq0C1b"
      },
      "source": [
        "#### c-2. Regex\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5xOmNrTqKov",
        "outputId": "09399e9c-67ee-4692-f6b8-5dad923fbb2b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<re.Match object; span=(20, 30), match='worksheets'>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# example of regex\n",
        "import re\n",
        "# 대소문자 구분 : re.IGNORECASE\n",
        "a = re.search(\"work\\D+\", \"Subject: industrial worksheets\", re.IGNORECASE)\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "YqhPHjtMpu8T",
        "outputId": "044315b1-8cfd-4b4b-f68d-c8c8958fbcc3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>label_num</th>\n",
              "      <th>model_without_nlp</th>\n",
              "      <th>model_rule_based</th>\n",
              "      <th>model_regex</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>605</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2349</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3624</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4685</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2030</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5166</th>\n",
              "      <td>1518</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: put the 10 on the ft\\r\\nthe transport...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5167</th>\n",
              "      <td>404</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: 3 / 4 / 2000 and following noms\\r\\nhp...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5168</th>\n",
              "      <td>2933</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: calpine daily gas nomination\\r\\n&gt;\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5169</th>\n",
              "      <td>1409</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: industrial worksheets for august 2000...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5170</th>\n",
              "      <td>4807</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: important online banking alert\\r\\ndea...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5171 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0 label                                               text  \\\n",
              "0            605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
              "1           2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
              "2           3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
              "3           4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
              "4           2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
              "...          ...   ...                                                ...   \n",
              "5166        1518   ham  Subject: put the 10 on the ft\\r\\nthe transport...   \n",
              "5167         404   ham  Subject: 3 / 4 / 2000 and following noms\\r\\nhp...   \n",
              "5168        2933   ham  Subject: calpine daily gas nomination\\r\\n>\\r\\n...   \n",
              "5169        1409   ham  Subject: industrial worksheets for august 2000...   \n",
              "5170        4807  spam  Subject: important online banking alert\\r\\ndea...   \n",
              "\n",
              "      label_num  model_without_nlp  model_rule_based  model_regex  \n",
              "0             0                  0                 1            1  \n",
              "1             0                  0                 0            0  \n",
              "2             0                  1                 1            1  \n",
              "3             1                  0                 1            1  \n",
              "4             0                  0                 1            1  \n",
              "...         ...                ...               ...          ...  \n",
              "5166          0                  0                 1            1  \n",
              "5167          0                  1                 1            1  \n",
              "5168          0                  0                 0            0  \n",
              "5169          0                  0                 1            1  \n",
              "5170          1                  1                 1            1  \n",
              "\n",
              "[5171 rows x 7 columns]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# logic\n",
        "def keyword_regex(text):\n",
        "    return 1 if any(re.search(f\"{keyword}\\D+\", text, re.IGNORECASE) for keyword in keywords) else 0\n",
        "#  for keyword in keywords:\n",
        "#    if re.search(f\"{keyword}\\D+\", text, re.IGNORECASE):\n",
        "#      return 1\n",
        "#  return 0\n",
        "\n",
        "df[\"model_regex\"] = df[\"text\"].apply(lambda txt: keyword_regex(txt))\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bpCZtPGq5Ad",
        "outputId": "4a761658-f89a-4188-bc03-52794df50633"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total: 5171\n",
            "tp: 1352, fp: 2719, fn: 147, tn: 953\n",
            "precision: 0.3321051338737411, recall: 0.9019346230820547, f1: 0.4854578096947935\n"
          ]
        }
      ],
      "source": [
        "# evaluation\n",
        "regex_predicts = df[\"model_regex\"].tolist()\n",
        "\n",
        "print_metrics(*confusion_matrix(real_labels, regex_predicts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM52iV-cKhfz"
      },
      "source": [
        "#### c-3. Edit Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "LGiak9f6j-j4"
      },
      "outputs": [],
      "source": [
        "# implementation of edit distance\n",
        "# sentance간 distance 측정\n",
        "def edit_distance(str1, str2, m, n):\n",
        "    # Reference: https://www.geeksforgeeks.org/edit-distance-dp-5/\n",
        "    # If first string is empty, the only option is to\n",
        "    # insert all characters of second string into first\n",
        "    if m == 0:\n",
        "        return n\n",
        "\n",
        "    # If second string is empty, the only option is to\n",
        "    # remove all characters of first string\n",
        "    if n == 0:\n",
        "        return m\n",
        "\n",
        "    # If last characters of two strings are same, nothing\n",
        "    # much to do. Ignore last characters and get count for\n",
        "    # remaining strings.\n",
        "    if str1[m-1] == str2[n-1]:\n",
        "        return edit_distance(str1, str2, m-1, n-1)\n",
        "\n",
        "    # If last characters are not same, consider all three\n",
        "    # operations on last character of first string, recursively\n",
        "    # compute minimum cost for all three operations and take\n",
        "    # minimum of three values.\n",
        "    return 1 + min(\n",
        "        edit_distance(str1, str2, m, n-1),    # Insert\n",
        "        edit_distance(str1, str2, m-1, n),    # Remove\n",
        "        edit_distance(str1, str2, m-1, n-1)    # Replace\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aII9TYzkOYH",
        "outputId": "e8e95ad9-84eb-4ca9-87ca-f3158e6353f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        }
      ],
      "source": [
        "# edit distance example\n",
        "str1 = \"GEEXSFRGEEKKS\"\n",
        "#          P  I   R\n",
        "str2 = \"GEEKSFORGEEKS\"\n",
        "print(edit_distance(str1, str2, len(str1), len(str2)))\n",
        "# 3부분의 차이가 발생"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5wEVS1Uk_x-"
      },
      "source": [
        "##### Logic for Edit Distance based Filter\n",
        "1. Split the email into list of words\n",
        "2. Measure edit distance between each word in email and each keyword.\n",
        "3. Classify the email with similarity higher than threshold as spam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTXI6ojPtAdB",
        "outputId": "af29b9bc-2b2b-49eb-c193-274e56c9ca90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<map object at 0x73d6e073a260>\n",
            "4 9 12\n"
          ]
        }
      ],
      "source": [
        "# map -> func를 받아 iterator 처리\n",
        "print(map(lambda x, y: x+y, [1,2,3], [3,7,9])) # iterator\n",
        "print(*map(lambda x, y: x+y, [1,2,3], [3,7,9])) # unpack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jkU7b2muD9Z",
        "outputId": "c36af56b-21ee-41ad-c0c1-5a224700d3b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Subject:',\n",
              " 'hpl',\n",
              " 'nom',\n",
              " 'for',\n",
              " 'january',\n",
              " '9',\n",
              " ',',\n",
              " '2001',\n",
              " '(',\n",
              " 'see',\n",
              " 'attached',\n",
              " 'file',\n",
              " ':',\n",
              " 'hplnol',\n",
              " '09',\n",
              " '.',\n",
              " 'xls',\n",
              " ')',\n",
              " '-',\n",
              " 'hplnol',\n",
              " '09',\n",
              " '.',\n",
              " 'xls']"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[1, \"text\"].split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "5k19d1mBlu-Z",
        "outputId": "11652bc3-fb72-48bd-e280-cbd43d6bf5a0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>label_num</th>\n",
              "      <th>model_without_nlp</th>\n",
              "      <th>model_rule_based</th>\n",
              "      <th>model_regex</th>\n",
              "      <th>words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>605</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, enron, methanol, ;, meter, #, :, 98...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2349</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[subject:, hpl, nom, for, january, 9, ,, 2001,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3624</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, neon, retreat, ho, ho, ho, ,, we, '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4685</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, photoshop, ,, windows, ,, office, ....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2030</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, re, :, indian, springs, this, deal,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5166</th>\n",
              "      <td>1518</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: put the 10 on the ft\\r\\nthe transport...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, put, the, 10, on, the, ft, the, tra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5167</th>\n",
              "      <td>404</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: 3 / 4 / 2000 and following noms\\r\\nhp...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, 3, /, 4, /, 2000, and, following, n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5168</th>\n",
              "      <td>2933</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: calpine daily gas nomination\\r\\n&gt;\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[subject:, calpine, daily, gas, nomination, &gt;,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5169</th>\n",
              "      <td>1409</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: industrial worksheets for august 2000...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, industrial, worksheets, for, august...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5170</th>\n",
              "      <td>4807</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: important online banking alert\\r\\ndea...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, important, online, banking, alert, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5171 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0 label                                               text  \\\n",
              "0            605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
              "1           2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
              "2           3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
              "3           4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
              "4           2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
              "...          ...   ...                                                ...   \n",
              "5166        1518   ham  Subject: put the 10 on the ft\\r\\nthe transport...   \n",
              "5167         404   ham  Subject: 3 / 4 / 2000 and following noms\\r\\nhp...   \n",
              "5168        2933   ham  Subject: calpine daily gas nomination\\r\\n>\\r\\n...   \n",
              "5169        1409   ham  Subject: industrial worksheets for august 2000...   \n",
              "5170        4807  spam  Subject: important online banking alert\\r\\ndea...   \n",
              "\n",
              "      label_num  model_without_nlp  model_rule_based  model_regex  \\\n",
              "0             0                  0                 1            1   \n",
              "1             0                  0                 0            0   \n",
              "2             0                  1                 1            1   \n",
              "3             1                  0                 1            1   \n",
              "4             0                  0                 1            1   \n",
              "...         ...                ...               ...          ...   \n",
              "5166          0                  0                 1            1   \n",
              "5167          0                  1                 1            1   \n",
              "5168          0                  0                 0            0   \n",
              "5169          0                  0                 1            1   \n",
              "5170          1                  1                 1            1   \n",
              "\n",
              "                                                  words  \n",
              "0     [subject:, enron, methanol, ;, meter, #, :, 98...  \n",
              "1     [subject:, hpl, nom, for, january, 9, ,, 2001,...  \n",
              "2     [subject:, neon, retreat, ho, ho, ho, ,, we, '...  \n",
              "3     [subject:, photoshop, ,, windows, ,, office, ....  \n",
              "4     [subject:, re, :, indian, springs, this, deal,...  \n",
              "...                                                 ...  \n",
              "5166  [subject:, put, the, 10, on, the, ft, the, tra...  \n",
              "5167  [subject:, 3, /, 4, /, 2000, and, following, n...  \n",
              "5168  [subject:, calpine, daily, gas, nomination, >,...  \n",
              "5169  [subject:, industrial, worksheets, for, august...  \n",
              "5170  [subject:, important, online, banking, alert, ...  \n",
              "\n",
              "[5171 rows x 8 columns]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 각열의 text를 받아 split 후 lower\n",
        "df[\"words\"] = df[\"text\"].apply(lambda txt: list(map(lambda w: w.lower(), txt.split())))\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIoCT2yrZj45"
      },
      "source": [
        "#### Stopword Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4L60NyukZn18",
        "outputId": "f4ff9545-0c04-4203-aba3-1d5a4db42029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "179\n",
            "['will', 'as', 'of', 'doesn', 'there', 'why', 'd', \"aren't\", 'our', 'so']\n",
            "[':', '/', '?', '!', '{', '(', '>', '*', '+', '-']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "STOPWORDS = set(stopwords.words(\"english\"))\n",
        "# 영어에서 큰 의미가 없는 word집합\n",
        "\n",
        "from string import punctuation\n",
        "PUNCTUATIONS = set(punctuation)\n",
        "# 특수기호\n",
        "\n",
        "print(len(STOPWORDS))\n",
        "print(list(STOPWORDS)[:10])\n",
        "print(list(PUNCTUATIONS)[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "AHU7J6aD2mE7",
        "outputId": "581443c5-a20e-4a34-db54-ad0c1eab918f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'  hello,    world!.  fff'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"  hello,    world!.  fff.\".strip(\"\".join([\"!\", \".\", \",\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4caprnRJ3EvO",
        "outputId": "99c96985-9613-4ca7-859f-2c10a30a4f25"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!.,'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\".join([\"!\", \".\", \",\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzwMVowY8PkH",
        "outputId": "3a353ad6-9dff-4622-8fde-b3c6eb36a202"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', 'This', 'is', 'an', 'example']\n"
          ]
        }
      ],
      "source": [
        "tokens = ['Hello', ',', 'world', '!', 'This', 'is', 'an', 'example', '.']\n",
        "PUNCTUATIONS = {'He', '.', '!', '?'}\n",
        "\n",
        "# PUNCTUATIONS에 포함된 문장 부호를 제거\n",
        "tokens = [t for t in tokens if t not in PUNCTUATIONS]\n",
        "\n",
        "print(tokens)  # ['Hello', 'world', 'This', 'is', 'an', 'example']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn1VjtNy8ctV",
        "outputId": "a4f266c5-6b9d-4758-e35e-d2b44903cfe3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"b\" not in \"abcd\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "_1u9S0hrqDFh"
      },
      "outputs": [],
      "source": [
        "# logic\n",
        "# 불용문자 제거\n",
        "def regularize_tokens(tokens: List[str]):\n",
        "    tokens = [t.strip() for t in tokens] # 문자열의 시작과 끝에서 공백 제거\n",
        "    tokens = [t.strip(\"\".join(PUNCTUATIONS)) for t in tokens] # 문자열의 시작과 끝에서 \"\".join(PUNCTUATIONS)에 해당하는 공백제거\n",
        "    tokens = [t for t in tokens if len(t) > 1] # 문자열 길이가 2 이상인 문자만 저장\n",
        "    tokens = [t for t in tokens if t not in STOPWORDS] # STOPWORDS가 아닌 경우만 저장\n",
        "    tokens = [t for t in tokens if t not in PUNCTUATIONS] # PUNCTUATIONS가 아닌 경우만 저장\n",
        "    tokens = [t for t in tokens if not re.match(r\"^\\d+?\\.\\d+?$\", t)]  # e.g., 1.23 숫자 제거\n",
        "    tokens = [t for t in tokens if not re.match(r\"^\\d+?\\,\\d+?$\", t)]  # e.g., 1,234 숫자제거\n",
        "    tokens = [t for t in tokens if not t.isnumeric()]  # e.g., 123 숫자제거\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "NhTbj0KvqgXw",
        "outputId": "d5017183-be63-4bf2-fdf7-f9235f77579e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>label_num</th>\n",
              "      <th>model_without_nlp</th>\n",
              "      <th>model_rule_based</th>\n",
              "      <th>model_regex</th>\n",
              "      <th>words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>605</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, nron, methanol, meter, follow, th, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2349</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[subject:, hpl, nom, january, attached, fil, h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3624</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, neon, retreat, ho, ho, ho, around, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4685</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, photoshop, windows, offic, cheap, m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2030</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, indian, springs, deal, book, th, te...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5166</th>\n",
              "      <td>1518</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: put the 10 on the ft\\r\\nthe transport...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, put, th, th, ft, th, transport, vol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5167</th>\n",
              "      <td>404</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: 3 / 4 / 2000 and following noms\\r\\nhp...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, following, noms, hpl, tak, th, xtra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5168</th>\n",
              "      <td>2933</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: calpine daily gas nomination\\r\\n&gt;\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[subject:, calpin, daily, gas, nomination, jul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5169</th>\n",
              "      <td>1409</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: industrial worksheets for august 2000...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, industrial, worksheets, august, act...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5170</th>\n",
              "      <td>4807</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: important online banking alert\\r\\ndea...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, important, onlin, banking, alert, d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5171 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0 label                                               text  \\\n",
              "0            605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
              "1           2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
              "2           3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
              "3           4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
              "4           2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
              "...          ...   ...                                                ...   \n",
              "5166        1518   ham  Subject: put the 10 on the ft\\r\\nthe transport...   \n",
              "5167         404   ham  Subject: 3 / 4 / 2000 and following noms\\r\\nhp...   \n",
              "5168        2933   ham  Subject: calpine daily gas nomination\\r\\n>\\r\\n...   \n",
              "5169        1409   ham  Subject: industrial worksheets for august 2000...   \n",
              "5170        4807  spam  Subject: important online banking alert\\r\\ndea...   \n",
              "\n",
              "      label_num  model_without_nlp  model_rule_based  model_regex  \\\n",
              "0             0                  0                 1            1   \n",
              "1             0                  0                 0            0   \n",
              "2             0                  1                 1            1   \n",
              "3             1                  0                 1            1   \n",
              "4             0                  0                 1            1   \n",
              "...         ...                ...               ...          ...   \n",
              "5166          0                  0                 1            1   \n",
              "5167          0                  1                 1            1   \n",
              "5168          0                  0                 0            0   \n",
              "5169          0                  0                 1            1   \n",
              "5170          1                  1                 1            1   \n",
              "\n",
              "                                                  words  \n",
              "0     [subject:, nron, methanol, meter, follow, th, ...  \n",
              "1     [subject:, hpl, nom, january, attached, fil, h...  \n",
              "2     [subject:, neon, retreat, ho, ho, ho, around, ...  \n",
              "3     [subject:, photoshop, windows, offic, cheap, m...  \n",
              "4     [subject:, indian, springs, deal, book, th, te...  \n",
              "...                                                 ...  \n",
              "5166  [subject:, put, th, th, ft, th, transport, vol...  \n",
              "5167  [subject:, following, noms, hpl, tak, th, xtra...  \n",
              "5168  [subject:, calpin, daily, gas, nomination, jul...  \n",
              "5169  [subject:, industrial, worksheets, august, act...  \n",
              "5170  [subject:, important, onlin, banking, alert, d...  \n",
              "\n",
              "[5171 rows x 8 columns]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# logic\n",
        "df[\"words\"] = df[\"words\"].apply(lambda ws: regularize_tokens(ws)) #불용어 제거\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNjjVfHpYUEt",
        "outputId": "bfd49e54-a7fc-4807-ebaa-7f12f1b4bf6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from fast_edit_distance import edit_distance as f_edit_distance # 단어간 distance를 계산하는 module\n",
        "f_edit_distance(\"hello\", \"hsseello\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1VTH_ib_GK1",
        "outputId": "14f161ee-9e0d-4adc-82d6-273b4e0ea587"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipdb in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (0.13.13)\n",
            "Requirement already satisfied: ipython>=7.31.1 in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from ipdb) (8.26.0)\n",
            "Requirement already satisfied: tomli in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from ipdb) (2.0.1)\n",
            "Requirement already satisfied: decorator in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from ipdb) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (0.1.7)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (3.0.47)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (2.18.0)\n",
            "Requirement already satisfied: stack-data in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (0.6.2)\n",
            "Requirement already satisfied: traitlets>=5.13.0 in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (5.14.3)\n",
            "Requirement already satisfied: exceptiongroup in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (4.12.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.31.1->ipdb) (0.2.13)\n",
            "Requirement already satisfied: executing>=1.2.0 in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from stack-data->ipython>=7.31.1->ipdb) (2.0.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from stack-data->ipython>=7.31.1->ipdb) (2.4.1)\n",
            "Requirement already satisfied: pure-eval in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from stack-data->ipython>=7.31.1->ipdb) (0.2.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=7.31.1->ipdb) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ipdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7geByCCa_yTU",
        "outputId": "c7f1d40d-c516-4db0-c2dd-19ec6260ec46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#chance\n",
        "#subject:\n",
        "f_edit_distance(\"chance\", \"subject:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "collapsed": true,
        "id": "ZMVl3PfEkpKV",
        "outputId": "804d5b73-058e-4563-8a2e-d37716b54f01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Measuring edit distance: 100%|██████████| 5171/5171 [00:17<00:00, 293.44it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>label_num</th>\n",
              "      <th>model_without_nlp</th>\n",
              "      <th>model_rule_based</th>\n",
              "      <th>model_regex</th>\n",
              "      <th>words</th>\n",
              "      <th>edit_distance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>605</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, nron, methanol, meter, follow, th, ...</td>\n",
              "      <td>6.818311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2349</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[subject:, hpl, nom, january, attached, fil, h...</td>\n",
              "      <td>6.775000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3624</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, neon, retreat, ho, ho, ho, around, ...</td>\n",
              "      <td>6.692322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4685</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, photoshop, windows, offic, cheap, m...</td>\n",
              "      <td>7.801136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2030</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, indian, springs, deal, book, th, te...</td>\n",
              "      <td>6.767004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5166</th>\n",
              "      <td>1518</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: put the 10 on the ft\\r\\nthe transport...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, put, th, th, ft, th, transport, vol...</td>\n",
              "      <td>6.779789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5167</th>\n",
              "      <td>404</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: 3 / 4 / 2000 and following noms\\r\\nhp...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, following, noms, hpl, tak, th, xtra...</td>\n",
              "      <td>6.668791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5168</th>\n",
              "      <td>2933</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: calpine daily gas nomination\\r\\n&gt;\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[subject:, calpin, daily, gas, nomination, jul...</td>\n",
              "      <td>6.828676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5169</th>\n",
              "      <td>1409</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: industrial worksheets for august 2000...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, industrial, worksheets, august, act...</td>\n",
              "      <td>7.225840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5170</th>\n",
              "      <td>4807</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: important online banking alert\\r\\ndea...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, important, onlin, banking, alert, d...</td>\n",
              "      <td>6.909502</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5171 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0 label                                               text  \\\n",
              "0            605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
              "1           2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
              "2           3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
              "3           4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
              "4           2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
              "...          ...   ...                                                ...   \n",
              "5166        1518   ham  Subject: put the 10 on the ft\\r\\nthe transport...   \n",
              "5167         404   ham  Subject: 3 / 4 / 2000 and following noms\\r\\nhp...   \n",
              "5168        2933   ham  Subject: calpine daily gas nomination\\r\\n>\\r\\n...   \n",
              "5169        1409   ham  Subject: industrial worksheets for august 2000...   \n",
              "5170        4807  spam  Subject: important online banking alert\\r\\ndea...   \n",
              "\n",
              "      label_num  model_without_nlp  model_rule_based  model_regex  \\\n",
              "0             0                  0                 1            1   \n",
              "1             0                  0                 0            0   \n",
              "2             0                  1                 1            1   \n",
              "3             1                  0                 1            1   \n",
              "4             0                  0                 1            1   \n",
              "...         ...                ...               ...          ...   \n",
              "5166          0                  0                 1            1   \n",
              "5167          0                  1                 1            1   \n",
              "5168          0                  0                 0            0   \n",
              "5169          0                  0                 1            1   \n",
              "5170          1                  1                 1            1   \n",
              "\n",
              "                                                  words  edit_distance  \n",
              "0     [subject:, nron, methanol, meter, follow, th, ...       6.818311  \n",
              "1     [subject:, hpl, nom, january, attached, fil, h...       6.775000  \n",
              "2     [subject:, neon, retreat, ho, ho, ho, around, ...       6.692322  \n",
              "3     [subject:, photoshop, windows, offic, cheap, m...       7.801136  \n",
              "4     [subject:, indian, springs, deal, book, th, te...       6.767004  \n",
              "...                                                 ...            ...  \n",
              "5166  [subject:, put, th, th, ft, th, transport, vol...       6.779789  \n",
              "5167  [subject:, following, noms, hpl, tak, th, xtra...       6.668791  \n",
              "5168  [subject:, calpin, daily, gas, nomination, jul...       6.828676  \n",
              "5169  [subject:, industrial, worksheets, august, act...       7.225840  \n",
              "5170  [subject:, important, onlin, banking, alert, d...       6.909502  \n",
              "\n",
              "[5171 rows x 9 columns]"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# logic\n",
        "import random\n",
        "#import ipdb\n",
        "from fast_edit_distance import edit_distance as f_edit_distance\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"Measuring edit distance\")\n",
        "\n",
        "def edit_distance_per_keyword(ws: List[str]):\n",
        "  # return normalized total sum of edit distance of each word in ws.\n",
        "  # normalization: consider the length of both keywords and ws.\n",
        "  distance = 0\n",
        "  # keywords = ['$', '$$$', 'affordable', 'amazing',,,,, 스팸이 포함하는 keyword\n",
        "  #ipdb.set_trace()\n",
        "  for keyword in keywords:\n",
        "    for w in ws:\n",
        "      distance += f_edit_distance(keyword, w)\n",
        "  return distance / (len(keywords) * len(ws)) # distance의 평균\n",
        "\n",
        "\n",
        "df[\"edit_distance\"] = df[\"words\"].progress_apply(lambda txts: edit_distance_per_keyword(txts))\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "AXikvkpqo53G",
        "outputId": "de30d4d1-12d7-4858-8b7a-99a2c2026c3f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAGzCAYAAADOnwhmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA71UlEQVR4nO3deVxWdd7/8fcleLEYYC5shbiU5kKpNBGVW5qIjFnalLlhkpphLrSQd+aS94TLjGnL5DiPhBZKc3JsRksF11LaMCK1yJ0cAS2VSzFZz++Pflx3V+DCJavn9Xw8zuPmfM/3nPP5eoJ53+d8z3VZDMMwBAAAYGKN6roAAACAukYgAgAApkcgAgAApkcgAgAApkcgAgAApkcgAgAApkcgAgAApkcgAgAApkcgAgAApkcgAlBrtm7dKovFoq1bt9rbxowZo9atW1+V5wXQcBCIANQr586d0+zZsx3CS32xd+9ezZ49W4cPH67rUgBUM9e6LgCAuf3jH/9QWVmZff3cuXOaM2eOJKl37961dt7LsXfvXs2ZM0e9e/fm7hJwlSEQAahTjRs3NtV5AdRPPDIDUGX//e9/NXbsWPn5+cnNzU2dO3fW8uXLHfocPXpU9913n5o0aSJfX19NmzZNhYWFFY7127k8hw8fVsuWLSVJc+bMkcVikcVi0ezZsy+7NmfOW27FihUKDQ2Vl5eXvL29FRISoiVLlkiSkpKS9Kc//UmS1KdPH3tt5Y/2PvzwQ0VFRSkwMFBubm5q166d5s6dq9LSUodz9O7dW126dNHevXvVp08feXp66rrrrtOCBQsq1Hj+/HnNnj1b7du3l7u7uwICAjRkyBAdOHDA3qesrEyLFy9W586d5e7uLj8/P02YMEGnTp267H8zANwhAlBFeXl5uv3222WxWDRp0iS1bNlSH3/8sWJiYmSz2TR16lT98ssv6tu3r7KzszV58mQFBgbq7bff1ubNmy967JYtW+r111/XxIkTdf/992vIkCGSpJtvvvmyanP2vJKUkpKihx9+WH379tX8+fMlSd9995127NihKVOmqGfPnpo8ebJefvll/c///I86duwoSfb/m5SUpGuuuUZxcXG65pprtHnzZs2cOVM2m00LFy50ONepU6c0YMAADRkyRA8++KD++c9/Kj4+XiEhIYqMjJQklZaW6o9//KM2bdqkYcOGacqUKTpz5oxSUlK0e/dutWvXTpI0YcIEJSUl6ZFHHtHkyZN16NAhvfrqq/r666+1Y8cO7oQBl8sAgCqIiYkxAgICjJ9++smhfdiwYYaPj49x7tw5Y/HixYYk4/3337dvLygoMG644QZDkrFlyxZ7e3R0tBEcHGxfP3HihCHJmDVrVpVru5LzTpkyxfD29jZKSkouePxVq1ZVOE65c+fOVWibMGGC4enpaZw/f97e1qtXL0OS8dZbb9nbCgsLDX9/f2Po0KH2tuXLlxuSjEWLFlU4bllZmWEYhvHJJ58Ykozk5GSH7evXr6+0HcCF8cgMwGUzDEMffPCBBg0aJMMw9NNPP9mXiIgI5efna9euXfroo48UEBCgBx54wL6vp6enxo8fX6P1Xcl5mzZtqoKCAqWkpDh1bg8PD/vPZ86c0U8//aQePXro3Llz+v777x36XnPNNRo5cqR93Wq16rbbbtPBgwftbR988IFatGihJ554osK5LBaLJGnVqlXy8fHRPffc43AtQkNDdc0112jLli1OjQUwIx6ZAbhsJ06c0OnTp7Vs2TItW7as0j7Hjx/XkSNHdMMNN9j/h7tchw4darS+Kznv448/rvfff1+RkZG67rrr1L9/fz344IMaMGDAZZ17z549mjFjhjZv3iybzeawLT8/32H9+uuvr1Djtddeq8zMTPv6gQMH1KFDB7m6XvjP9L59+5Sfny9fX99Ktx8/fvyyagdAIAJQBeWvqY8cOVLR0dGV9rnc+T71ja+vrzIyMrRhwwZ9/PHH+vjjj5WYmKjRo0frzTffvOi+p0+fVq9eveTt7a0XXnhB7dq1k7u7u3bt2qX4+PgKr/e7uLhUehzDMKpUc1lZmXx9fZWcnFzp9vIJ6gAujUAE4LK1bNlSXl5eKi0tVb9+/S7YLzg4WLt375ZhGA53QrKysi55jt/fOamKKzmv9Oujq0GDBmnQoEEqKyvT448/rr///e96/vnnK73zVG7r1q36+eeftXr1avXs2dPefujQIafH0q5dO33++ecqLi6+4MTodu3aKTU1VXfeeafDIzsAVcccIgCXzcXFRUOHDtUHH3yg3bt3V9h+4sQJSdLAgQN17Ngx/fOf/7RvO3fu3AUfs/2Wp6enpF/vulTVlZz3559/dlhv1KiR/W5X+Wv7TZo0qbS28js+v73DU1RUpL/97W9VHkO5oUOH6qefftKrr75aYVv5eR588EGVlpZq7ty5FfqUlJQ49W8ImBV3iABUybx587RlyxaFhYVp3Lhx6tSpk06ePKldu3YpNTVVJ0+e1Lhx4/Tqq69q9OjRSk9PV0BAgN5++2172LkYDw8PderUSStXrlT79u3VrFkzdenSRV26dLnkvldy3kcffVQnT57U3Xffreuvv15HjhzRK6+8oq5du9pfre/atatcXFw0f/585efny83NTXfffbfuuOMOXXvttYqOjtbkyZNlsVj09ttvV/kR2G+NHj1ab731luLi4vTFF1+oR48eKigoUGpqqh5//HENHjxYvXr10oQJE5SQkKCMjAz1799fjRs31r59+7Rq1SotWbLEYYI5gIuouxfcADRUeXl5RmxsrBEUFGQ0btzY8Pf3N/r27WssW7bM3ufIkSPGvffea3h6ehotWrQwpkyZYn8d/GKvvxuGYezcudMIDQ01rFZrlV/Bd/a8//znP43+/fsbvr6+htVqNVq1amVMmDDByMnJcTj+P/7xD6Nt27aGi4uLwzF37Nhh3H777YaHh4cRGBhoPPPMM8aGDRsqnLdXr15G586dK9Rd2b/DuXPnjOeee85o06aN/d/5gQceMA4cOODQb9myZUZoaKjh4eFheHl5GSEhIcYzzzxjHDt27LL/3QCzsxjGFfy/MAAAAFcB5hABAADTYw4RgHqvqKhIJ0+evGgfHx8f3rQC4DQCEYB6b+fOnerTp89F+yQmJmrMmDG1UxCAqw5ziADUe6dOnVJ6evpF+3Tu3FkBAQG1VBGAqw2BCAAAmB6TqgEAgOkRiC6DYRiy2WxX9CFrAACg/iIQXYYzZ87Ix8dHZ86cqetSAABADSAQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA06vTQJSQkKA//OEP8vLykq+vr+677z5lZWU59Dl//rxiY2PVvHlzXXPNNRo6dKjy8vIc+mRnZysqKkqenp7y9fXV008/rZKSEoc+W7duVffu3eXm5qYbbrhBSUlJNT08AADQQNRpINq2bZtiY2P12WefKSUlRcXFxerfv78KCgrsfaZNm6b//Oc/WrVqlbZt26Zjx45pyJAh9u2lpaWKiopSUVGRdu7cqTfffFNJSUmaOXOmvc+hQ4cUFRWlPn36KCMjQ1OnTtWjjz6qDRs21Op4AQBA/WQx6tFXuJ84cUK+vr7atm2bevbsqfz8fLVs2VLvvvuuHnjgAUnS999/r44dOyotLU233367Pv74Y/3xj3/UsWPH5OfnJ0launSp4uPjdeLECVmtVsXHx2vdunXavXu3/VzDhg3T6dOntX79+kvWZbPZ5OPjo/z8fHl7e9fM4AEAQJ2pV3OI8vPzJUnNmjWTJKWnp6u4uFj9+vWz97npppvUqlUrpaWlSZLS0tIUEhJiD0OSFBERIZvNpj179tj7/PYY5X3Kj/F7hYWFstlsDgsAALh61ZtAVFZWpqlTp+rOO+9Uly5dJEm5ubmyWq1q2rSpQ18/Pz/l5uba+/w2DJVvL992sT42m02//PJLhVoSEhLk4+NjX4KCgqpljAAAoH6qN4EoNjZWu3fv1ooVK+q6FE2fPl35+fn25ccff6zrkgAAQA1yresCJGnSpElau3attm/fruuvv97e7u/vr6KiIp0+fdrhLlFeXp78/f3tfb744guH45W/hfbbPr9/My0vL0/e3t7y8PCoUI+bm5vc3NyqZWxXq9bPrquxYx+eF1VjxwYAoDJ1eofIMAxNmjRJ//rXv7R582a1adPGYXtoaKgaN26sTZs22duysrKUnZ2t8PBwSVJ4eLi+/fZbHT9+3N4nJSVF3t7e6tSpk73Pb49R3qf8GAAAwNzq9A5RbGys3n33XX344Yfy8vKyz/nx8fGRh4eHfHx8FBMTo7i4ODVr1kze3t564oknFB4erttvv12S1L9/f3Xq1EmjRo3SggULlJubqxkzZig2NtZ+l+exxx7Tq6++qmeeeUZjx47V5s2b9f7772vdupq7ywEAABqOOn3t3mKxVNqemJioMWPGSPr1gxmffPJJvffeeyosLFRERIT+9re/2R+HSdKRI0c0ceJEbd26VU2aNFF0dLTmzZsnV9f/y3tbt27VtGnTtHfvXl1//fV6/vnn7ee4FF67r4hHZgCAq0m9+hyi+opAVBGBCABwNak3b5kBAADUFQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwvToNRNu3b9egQYMUGBgoi8WiNWvWOGy3WCyVLgsXLrT3ad26dYXt8+bNczhOZmamevToIXd3dwUFBWnBggW1MTwAANBA1GkgKigo0C233KLXXnut0u05OTkOy/Lly2WxWDR06FCHfi+88IJDvyeeeMK+zWazqX///goODlZ6eroWLlyo2bNna9myZTU6NgAA0HC41uXJIyMjFRkZecHt/v7+Dusffvih+vTpo7Zt2zq0e3l5VehbLjk5WUVFRVq+fLmsVqs6d+6sjIwMLVq0SOPHj7/yQQAAgAavwcwhysvL07p16xQTE1Nh27x589S8eXN169ZNCxcuVElJiX1bWlqaevbsKavVam+LiIhQVlaWTp06Vem5CgsLZbPZHBYAAHD1qtM7RFXx5ptvysvLS0OGDHFonzx5srp3765mzZpp586dmj59unJycrRo0SJJUm5urtq0aeOwj5+fn33btddeW+FcCQkJmjNnTg2NBAAA1DcNJhAtX75cI0aMkLu7u0N7XFyc/eebb75ZVqtVEyZMUEJCgtzc3Jw61/Tp0x2Oa7PZFBQU5FzhAACg3msQgeiTTz5RVlaWVq5cecm+YWFhKikp0eHDh9WhQwf5+/srLy/PoU/5+oXmHbm5uTkdpgAAQMPTIOYQvfHGGwoNDdUtt9xyyb4ZGRlq1KiRfH19JUnh4eHavn27iouL7X1SUlLUoUOHSh+XAQAA86nTQHT27FllZGQoIyNDknTo0CFlZGQoOzvb3sdms2nVqlV69NFHK+yflpamxYsX65tvvtHBgweVnJysadOmaeTIkfawM3z4cFmtVsXExGjPnj1auXKllixZ4vBIDAAAmFudPjL76quv1KdPH/t6eUiJjo5WUlKSJGnFihUyDEMPP/xwhf3d3Ny0YsUKzZ49W4WFhWrTpo2mTZvmEHZ8fHy0ceNGxcbGKjQ0VC1atNDMmTN55R4AANhZDMMw6rqI+s5ms8nHx0f5+fny9vau63LqhdbPrquxYx+eF1VjxwYAoDINYg4RAABATSIQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA0yMQAQAA06vTQLR9+3YNGjRIgYGBslgsWrNmjcP2MWPGyGKxOCwDBgxw6HPy5EmNGDFC3t7eatq0qWJiYnT27FmHPpmZmerRo4fc3d0VFBSkBQsW1PTQAABAA1KngaigoEC33HKLXnvttQv2GTBggHJycuzLe++957B9xIgR2rNnj1JSUrR27Vpt375d48ePt2+32Wzq37+/goODlZ6eroULF2r27NlatmxZjY0LAAA0LK51efLIyEhFRkZetI+bm5v8/f0r3fbdd99p/fr1+vLLL3XrrbdKkl555RUNHDhQf/nLXxQYGKjk5GQVFRVp+fLlslqt6ty5szIyMrRo0SKH4AQAAMyr3s8h2rp1q3x9fdWhQwdNnDhRP//8s31bWlqamjZtag9DktSvXz81atRIn3/+ub1Pz549ZbVa7X0iIiKUlZWlU6dOVXrOwsJC2Ww2hwUAAFy96nUgGjBggN566y1t2rRJ8+fP17Zt2xQZGanS0lJJUm5urnx9fR32cXV1VbNmzZSbm2vv4+fn59CnfL28z+8lJCTIx8fHvgQFBVX30AAAQD1Sp4/MLmXYsGH2n0NCQnTzzTerXbt22rp1q/r27Vtj550+fbri4uLs6zabjVAEAMBVrF7fIfq9tm3bqkWLFtq/f78kyd/fX8ePH3foU1JSopMnT9rnHfn7+ysvL8+hT/n6heYmubm5ydvb22EBAABXrwYViI4ePaqff/5ZAQEBkqTw8HCdPn1a6enp9j6bN29WWVmZwsLC7H22b9+u4uJie5+UlBR16NBB1157be0OAAAA1Et1GojOnj2rjIwMZWRkSJIOHTqkjIwMZWdn6+zZs3r66af12Wef6fDhw9q0aZMGDx6sG264QREREZKkjh07asCAARo3bpy++OIL7dixQ5MmTdKwYcMUGBgoSRo+fLisVqtiYmK0Z88erVy5UkuWLHF4JAYAAMytTgPRV199pW7duqlbt26SpLi4OHXr1k0zZ86Ui4uLMjMzde+996p9+/aKiYlRaGioPvnkE7m5udmPkZycrJtuukl9+/bVwIEDdddddzl8xpCPj482btyoQ4cOKTQ0VE8++aRmzpzJK/cAAMDOYhiGUddF1Hc2m00+Pj7Kz89nPtH/1/rZdTV27MPzomrs2AAAVKZBzSECAACoCQQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgenUaiLZv365BgwYpMDBQFotFa9assW8rLi5WfHy8QkJC1KRJEwUGBmr06NE6duyYwzFat24ti8XisMybN8+hT2Zmpnr06CF3d3cFBQVpwYIFtTE8AADQQNRpICooKNAtt9yi1157rcK2c+fOadeuXXr++ee1a9curV69WllZWbr33nsr9H3hhReUk5NjX5544gn7NpvNpv79+ys4OFjp6elauHChZs+erWXLltXo2AAAQMPhWpcnj4yMVGRkZKXbfHx8lJKS4tD26quv6rbbblN2drZatWplb/fy8pK/v3+lx0lOTlZRUZGWL18uq9Wqzp07KyMjQ4sWLdL48eOrbzAAAKDBalBziPLz82WxWNS0aVOH9nnz5ql58+bq1q2bFi5cqJKSEvu2tLQ09ezZU1ar1d4WERGhrKwsnTp1qtLzFBYWymazOSwAAODqVad3iKri/Pnzio+P18MPPyxvb297++TJk9W9e3c1a9ZMO3fu1PTp05WTk6NFixZJknJzc9WmTRuHY/n5+dm3XXvttRXOlZCQoDlz5tTgaAAAQH3SIAJRcXGxHnzwQRmGoddff91hW1xcnP3nm2++WVarVRMmTFBCQoLc3NycOt/06dMdjmuz2RQUFORc8QAAoN6r94GoPAwdOXJEmzdvdrg7VJmwsDCVlJTo8OHD6tChg/z9/ZWXl+fQp3z9QvOO3NzcnA5TAACg4anXc4jKw9C+ffuUmpqq5s2bX3KfjIwMNWrUSL6+vpKk8PBwbd++XcXFxfY+KSkp6tChQ6WPywAAgPk4FYgOHjxYLSc/e/asMjIylJGRIUk6dOiQMjIylJ2dreLiYj3wwAP66quvlJycrNLSUuXm5io3N1dFRUWSfp0wvXjxYn3zzTc6ePCgkpOTNW3aNI0cOdIedoYPHy6r1aqYmBjt2bNHK1eu1JIlSxweiQEAAHOzGIZhVHWnRo0aqVevXoqJidEDDzwgd3d3p06+detW9enTp0J7dHS0Zs+eXWEydLktW7aod+/e2rVrlx5//HF9//33KiwsVJs2bTRq1CjFxcU5PPLKzMxUbGysvvzyS7Vo0UJPPPGE4uPjL7tOm80mHx8f5efnX/KRnVm0fnZdjR378LyoGjs2AACVcSoQZWRkKDExUe+9956Kior00EMPKSYmRrfddltN1FjnCEQVEYgAAFcTpx6Zde3aVUuWLNGxY8e0fPly5eTk6K677lKXLl20aNEinThxorrrBAAAqDFXNKna1dVVQ4YM0apVqzR//nzt379fTz31lIKCgjR69Gjl5ORUV50AAAA15ooC0VdffaXHH39cAQEBWrRokZ566ikdOHBAKSkpOnbsmAYPHlxddQIAANQYpz6HaNGiRUpMTFRWVpYGDhyot956SwMHDlSjRr/mqzZt2igpKUmtW7euzloBAABqhFOB6PXXX9fYsWM1ZswYBQQEVNrH19dXb7zxxhUVBwAAUBucCkT79u27ZB+r1aro6GhnDg8AAFCrnJpDlJiYqFWrVlVoX7Vqld58880rLgoAAKA2ORWIEhIS1KJFiwrtvr6+evHFF6+4KAAAgNrkVCDKzs6u9FOkg4ODlZ2dfcVFAQAA1Can5hD5+voqMzOzwltk33zzzWV9AStwMTX1Kdh8AjYA4EKcukP08MMPa/LkydqyZYtKS0tVWlqqzZs3a8qUKRo2bFh11wgAAFCjnLpDNHfuXB0+fFh9+/aVq+uvhygrK9Po0aOZQwQAABocpwKR1WrVypUrNXfuXH3zzTfy8PBQSEiIgoODq7s+AACAGudUICrXvn17tW/fvrpqAQAAqBNOBaLS0lIlJSVp06ZNOn78uMrKyhy2b968uVqKAwAAqA1OBaIpU6YoKSlJUVFR6tKliywWS3XXBQAAUGucCkQrVqzQ+++/r4EDB1Z3PQAAALXOqdfurVarbrjhhuquBQAAoE44FYiefPJJLVmyRIZhVHc9AAAAtc6pR2affvqptmzZoo8//lidO3dW48aNHbavXr26WooDAACoDU4FoqZNm+r++++v7loAAADqhFOBKDExsbrrAAAAqDNOzSGSpJKSEqWmpurvf/+7zpw5I0k6duyYzp49W23FAQAA1Aan7hAdOXJEAwYMUHZ2tgoLC3XPPffIy8tL8+fPV2FhoZYuXVrddQIAANQYp+4QTZkyRbfeeqtOnTolDw8Pe/v999+vTZs2VVtxAAAAtcGpO0SffPKJdu7cKavV6tDeunVr/fe//62WwgAAAGqLU3eIysrKVFpaWqH96NGj8vLyuuKiAAAAapNTgah///5avHixfd1isejs2bOaNWsWX+cBAAAaHKcemf31r39VRESEOnXqpPPnz2v48OHat2+fWrRooffee6+6awQAAKhRTgWi66+/Xt98841WrFihzMxMnT17VjExMRoxYoTDJGsAAICGwKlAJEmurq4aOXJkddYCAABQJ5wKRG+99dZFt48ePdqpYgAAAOqCU4FoypQpDuvFxcU6d+6crFarPD09CUQAAKBBceots1OnTjksZ8+eVVZWlu66664qTarevn27Bg0apMDAQFksFq1Zs8Zhu2EYmjlzpgICAuTh4aF+/fpp3759Dn1OnjypESNGyNvbW02bNlVMTEyFrw/JzMxUjx495O7urqCgIC1YsMCZYQMAgKuU099l9ns33nij5s2bV+Hu0cUUFBTolltu0WuvvVbp9gULFujll1/W0qVL9fnnn6tJkyaKiIjQ+fPn7X1GjBihPXv2KCUlRWvXrtX27ds1fvx4+3abzab+/fsrODhY6enpWrhwoWbPnq1ly5Y5P1gAAHBVcXpSdaUHc3XVsWPHLrt/ZGSkIiMjK91mGIYWL16sGTNmaPDgwZJ+nbvk5+enNWvWaNiwYfruu++0fv16ffnll7r11lslSa+88ooGDhyov/zlLwoMDFRycrKKioq0fPlyWa1Wde7cWRkZGVq0aJFDcAIAAOblVCD697//7bBuGIZycnL06quv6s4776yWwg4dOqTc3Fz169fP3ubj46OwsDClpaVp2LBhSktLU9OmTe1hSJL69eunRo0a6fPPP9f999+vtLQ09ezZ0+FrRiIiIjR//nydOnVK1157bYVzFxYWqrCw0L5us9mqZUwAAKB+cioQ3XfffQ7rFotFLVu21N13362//vWv1VGXcnNzJUl+fn4O7X5+fvZtubm58vX1ddju6uqqZs2aOfRp06ZNhWOUb6ssECUkJGjOnDnVMg4AAFD/ORWIysrKqruOemX69OmKi4uzr9tsNgUFBdVhRQAAoCZV26Tq6ubv7y9JysvLc2jPy8uzb/P399fx48cdtpeUlOjkyZMOfSo7xm/P8Xtubm7y9vZ2WAAAwNXLqTtEv717cimLFi1y5hRq06aN/P39tWnTJnXt2lXSr3dqPv/8c02cOFGSFB4ertOnTys9PV2hoaGSpM2bN6usrExhYWH2Ps8995yKi4vVuHFjSVJKSoo6dOhQ6eMyAABgPk4Foq+//lpff/21iouL1aFDB0nSDz/8IBcXF3Xv3t3ez2KxXPQ4Z8+e1f79++3rhw4dUkZGhpo1a6ZWrVpp6tSp+t///V/deOONatOmjZ5//nkFBgba5zB17NhRAwYM0Lhx47R06VIVFxdr0qRJGjZsmAIDAyVJw4cP15w5cxQTE6P4+Hjt3r1bS5Ys0UsvveTM0AEAwFXIqUA0aNAgeXl56c0337TfZTl16pQeeeQR9ejRQ08++eRlHeerr75Snz597Ovld56io6OVlJSkZ555RgUFBRo/frxOnz6tu+66S+vXr5e7u7t9n+TkZE2aNEl9+/ZVo0aNNHToUL388sv27T4+Ptq4caNiY2MVGhqqFi1aaObMmbxyDwAA7CyGYRhV3em6667Txo0b1blzZ4f23bt3q3///lX6LKKGwGazycfHR/n5+cwn+v9aP7uurkuossPzouq6BABAPeXUpGqbzaYTJ05UaD9x4oTOnDlzxUUBAADUJqcC0f33369HHnlEq1ev1tGjR3X06FF98MEHiomJ0ZAhQ6q7RgAAgBrl1ByipUuX6qmnntLw4cNVXFz864FcXRUTE6OFCxdWa4EAAAA1zak5ROUKCgp04MABSVK7du3UpEmTaiusPmEOUUXMIQIAXE2u6IMZc3JylJOToxtvvFFNmjTRFWQrAACAOuNUIPr555/Vt29ftW/fXgMHDlROTo4kKSYm5rJfuQcAAKgvnJpDNG3aNDVu3FjZ2dnq2LGjvf2hhx5SXFxctX3BK65MQ3ysBQBAXXAqEG3cuFEbNmzQ9ddf79B+44036siRI9VSGAAAQG1x6pFZQUGBPD09K7SfPHlSbm5uV1wUAABAbXIqEPXo0UNvvfWWfd1isaisrEwLFixw+CoOAACAhsCpR2YLFixQ37599dVXX6moqEjPPPOM9uzZo5MnT2rHjh3VXSMAAECNcuoOUZcuXfTDDz/orrvu0uDBg1VQUKAhQ4bo66+/Vrt27aq7RgAAgBpV5TtExcXFGjBggJYuXarnnnuuJmoCAACoVVW+Q9S4cWNlZmbWRC0AAAB1wqlHZiNHjtQbb7xR3bUAAADUCacmVZeUlGj58uVKTU1VaGhohe8wW7RoUbUUBwAAUBuqFIgOHjyo1q1ba/fu3erevbsk6YcffnDoY7FYqq86AACAWlClQHTjjTcqJydHW7ZskfTrV3W8/PLL8vPzq5HiAAAAakOV5hD9/tvsP/74YxUUFFRrQQAAALXNqUnV5X4fkAAAABqiKgUii8VSYY4Qc4YAAEBDV6U5RIZhaMyYMfYvcD1//rwee+yxCm+ZrV69uvoqBAAAqGFVCkTR0dEO6yNHjqzWYgAAAOpClQJRYmJiTdUBAABQZ65oUjUAAMDVgEAEAABMj0AEAABMj0AEAABMj0AEAABMj0AEAABMj0AEAABMj0AEAABMj0AEAABMj0AEAABMr94HotatW8tisVRYYmNjJUm9e/eusO2xxx5zOEZ2draioqLk6ekpX19fPf300yopKamL4QAAgHqoSt9lVhe+/PJLlZaW2td3796te+65R3/605/sbePGjdMLL7xgX/f09LT/XFpaqqioKPn7+2vnzp3KycnR6NGj1bhxY7344ou1MwgAAFCv1ftA1LJlS4f1efPmqV27durVq5e9zdPTU/7+/pXuv3HjRu3du1epqany8/NT165dNXfuXMXHx2v27NmyWq0V9iksLFRhYaF93WazVdNoAABAfVTvH5n9VlFRkd555x2NHTtWFovF3p6cnKwWLVqoS5cumj59us6dO2fflpaWppCQEPn5+dnbIiIiZLPZtGfPnkrPk5CQIB8fH/sSFBRUc4MCAAB1rt7fIfqtNWvW6PTp0xozZoy9bfjw4QoODlZgYKAyMzMVHx+vrKwsrV69WpKUm5vrEIYk2ddzc3MrPc/06dMVFxdnX7fZbIQiAACuYg0qEL3xxhuKjIxUYGCgvW38+PH2n0NCQhQQEKC+ffvqwIEDateunVPncXNzk5ub2xXXCwAAGoYG88jsyJEjSk1N1aOPPnrRfmFhYZKk/fv3S5L8/f2Vl5fn0Kd8/ULzjgAAgLk0mECUmJgoX19fRUVFXbRfRkaGJCkgIECSFB4erm+//VbHjx+390lJSZG3t7c6depUY/UCAICGo0E8MisrK1NiYqKio6Pl6vp/JR84cEDvvvuuBg4cqObNmyszM1PTpk1Tz549dfPNN0uS+vfvr06dOmnUqFFasGCBcnNzNWPGDMXGxvJYDAAASGoggSg1NVXZ2dkaO3asQ7vValVqaqoWL16sgoICBQUFaejQoZoxY4a9j4uLi9auXauJEycqPDxcTZo0UXR0tMPnFgEAAHOzGIZh1HUR9Z3NZpOPj4/y8/Pl7e1d1+VcttbPrqvrEuqVw/Mu/rgVAGBeDWYOEQAAQE0hEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEAANMjEAEAANNzresCgNrS+tl1NXbsw/OiauzYAICaxx0iAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgegQiAABgevU6EM2ePVsWi8Vhuemmm+zbz58/r9jYWDVv3lzXXHONhg4dqry8PIdjZGdnKyoqSp6envL19dXTTz+tkpKS2h4KAACox+r9V3d07txZqamp9nVX1/8redq0aVq3bp1WrVolHx8fTZo0SUOGDNGOHTskSaWlpYqKipK/v7927typnJwcjR49Wo0bN9aLL75Y62MBAAD1U70PRK6urvL396/Qnp+frzfeeEPvvvuu7r77bklSYmKiOnbsqM8++0y33367Nm7cqL179yo1NVV+fn7q2rWr5s6dq/j4eM2ePVtWq7W2hwMAAOqhev3ITJL27dunwMBAtW3bViNGjFB2drYkKT09XcXFxerXr5+970033aRWrVopLS1NkpSWlqaQkBD5+fnZ+0RERMhms2nPnj0XPGdhYaFsNpvDAgAArl71OhCFhYUpKSlJ69ev1+uvv65Dhw6pR48eOnPmjHJzc2W1WtW0aVOHffz8/JSbmytJys3NdQhD5dvLt11IQkKCfHx87EtQUFD1DgwAANQr9fqRWWRkpP3nm2++WWFhYQoODtb7778vDw+PGjvv9OnTFRcXZ1+32WyEIgAArmL1+g7R7zVt2lTt27fX/v375e/vr6KiIp0+fdqhT15enn3Okb+/f4W3zsrXK5uXVM7NzU3e3t4OCwAAuHo1qEB09uxZHThwQAEBAQoNDVXjxo21adMm+/asrCxlZ2crPDxckhQeHq5vv/1Wx48ft/dJSUmRt7e3OnXqVOv1AwCA+qlePzJ76qmnNGjQIAUHB+vYsWOaNWuWXFxc9PDDD8vHx0cxMTGKi4tTs2bN5O3trSeeeELh4eG6/fbbJUn9+/dXp06dNGrUKC1YsEC5ubmaMWOGYmNj5ebmVsejAwAA9UW9DkRHjx7Vww8/rJ9//lktW7bUXXfdpc8++0wtW7aUJL300ktq1KiRhg4dqsLCQkVEROhvf/ubfX8XFxetXbtWEydOVHh4uJo0aaLo6Gi98MILdTUkAABQD1kMwzDquoj6zmazycfHR/n5+Q1qPlHrZ9fVdQmmcXheVF2XAAC4Ag1qDhEAAEBNIBABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTIxABAADTq9eBKCEhQX/4wx/k5eUlX19f3XfffcrKynLo07t3b1ksFoflsccec+iTnZ2tqKgoeXp6ytfXV08//bRKSkpqcygAAKAec63rAi5m27Ztio2N1R/+8AeVlJTof/7nf9S/f3/t3btXTZo0sfcbN26cXnjhBfu6p6en/efS0lJFRUXJ399fO3fuVE5OjkaPHq3GjRvrxRdfrNXxAACA+qleB6L169c7rCclJcnX11fp6enq2bOnvd3T01P+/v6VHmPjxo3au3evUlNT5efnp65du2ru3LmKj4/X7NmzZbVaa3QMAACg/qvXj8x+Lz8/X5LUrFkzh/bk5GS1aNFCXbp00fTp03Xu3Dn7trS0NIWEhMjPz8/eFhERIZvNpj179lR6nsLCQtlsNocFAABcver1HaLfKisr09SpU3XnnXeqS5cu9vbhw4crODhYgYGByszMVHx8vLKysrR69WpJUm5urkMYkmRfz83NrfRcCQkJmjNnTg2NBAAA1DcNJhDFxsZq9+7d+vTTTx3ax48fb/85JCREAQEB6tu3rw4cOKB27do5da7p06crLi7Ovm6z2RQUFORc4QAAoN5rEI/MJk2apLVr12rLli26/vrrL9o3LCxMkrR//35Jkr+/v/Ly8hz6lK9faN6Rm5ubvL29HRYAAHD1qteByDAMTZo0Sf/617+0efNmtWnT5pL7ZGRkSJICAgIkSeHh4fr22291/Phxe5+UlBR5e3urU6dONVI3AABoWOr1I7PY2Fi9++67+vDDD+Xl5WWf8+Pj4yMPDw8dOHBA7777rgYOHKjmzZsrMzNT06ZNU8+ePXXzzTdLkvr3769OnTpp1KhRWrBggXJzczVjxgzFxsbKzc2tLocHAADqCYthGEZdF3EhFoul0vbExESNGTNGP/74o0aOHKndu3eroKBAQUFBuv/++zVjxgyHx1xHjhzRxIkTtXXrVjVp0kTR0dGaN2+eXF0vLw/abDb5+PgoPz+/QT0+a/3surouAVfo8Lyoui4BAEyhXt8hulRWCwoK0rZt2y55nODgYH300UfVVRYAALjK1Os5RAAAALWBQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEyPQAQAAEzPta4LAHBhrZ9dV2PHPjwvqsaODQANDXeIAACA6RGIAACA6fHIrB6oycciAADg0rhDBAAATI9ABAAATI9ABAAATI9ABAAATI9ABAAATI9ABAAATI9ABAAATI9ABAAATI9ABAAATI9PqgZQrfhCWgANEYEIMCm+MgYA/o+pHpm99tprat26tdzd3RUWFqYvvviirksCAAD1gGkC0cqVKxUXF6dZs2Zp165duuWWWxQREaHjx4/XdWkAAKCOmSYQLVq0SOPGjdMjjzyiTp06aenSpfL09NTy5cvrujQAAFDHTDGHqKioSOnp6Zo+fbq9rVGjRurXr5/S0tIq9C8sLFRhYaF9PT8/X5Jks9lqpL6ywnM1clzgatNq2qoaOe7uORE1clxJ6jJrQ40ctyZrrkk19e9RkxrqvzX+j5eXlywWy0X7mCIQ/fTTTyotLZWfn59Du5+fn77//vsK/RMSEjRnzpwK7UFBQTVWI4C647O4riuouoZYc0PFv3XDl5+fL29v74v2MUUgqqrp06crLi7Ovl5WVqaTJ0+qefPml0yYDZXNZlNQUJB+/PHHS/5HczVgvFcvM41VYrxXMzONVarZ8Xp5eV2yjykCUYsWLeTi4qK8vDyH9ry8PPn7+1fo7+bmJjc3N4e2pk2b1mSJ9Ya3t7cpfvHKMd6rl5nGKjHeq5mZxirV3XhNManaarUqNDRUmzZtsreVlZVp06ZNCg8Pr8PKAABAfWCKO0SSFBcXp+joaN1666267bbbtHjxYhUUFOiRRx6p69IAAEAdM00geuihh3TixAnNnDlTubm56tq1q9avX19horVZubm5adasWRUeFV6tGO/Vy0xjlRjv1cxMY5XqfrwWwzCMOjkzAABAPWGKOUQAAAAXQyACAACmRyACAACmRyACAACmRyACAACmRyAyif/+978aOXKkmjdvLg8PD4WEhOirr766YP+tW7fKYrFUWHJzc2uxaue0bt260tpjY2MvuM+qVat00003yd3dXSEhIfroo49qseIrU9XxJiUlVejr7u5ey1U7p7S0VM8//7zatGkjDw8PtWvXTnPnztWlXpbdunWrunfvLjc3N91www1KSkqqnYKvkDPjbci/u5J05swZTZ06VcHBwfLw8NAdd9yhL7/88qL7NNTrW9WxNqRru337dg0aNEiBgYGyWCxas2aNw3bDMDRz5kwFBATIw8ND/fr10759+y553Ndee02tW7eWu7u7wsLC9MUXX1Rf0QaueidPnjSCg4ONMWPGGJ9//rlx8OBBY8OGDcb+/fsvuM+WLVsMSUZWVpaRk5NjX0pLS2uxcuccP37coeaUlBRDkrFly5ZK++/YscNwcXExFixYYOzdu9eYMWOG0bhxY+Pbb7+t3cKdVNXxJiYmGt7e3g775Obm1m7RTvrzn/9sNG/e3Fi7dq1x6NAhY9WqVcY111xjLFmy5IL7HDx40PD09DTi4uKMvXv3Gq+88orh4uJirF+/vhYrd44z423Iv7uGYRgPPvig0alTJ2Pbtm3Gvn37jFmzZhne3t7G0aNHK+3fkK9vVcfakK7tRx99ZDz33HPG6tWrDUnGv/71L4ft8+bNM3x8fIw1a9YY33zzjXHvvfcabdq0MX755ZcLHnPFihWG1Wo1li9fbuzZs8cYN26c0bRpUyMvL69aaiYQmUB8fLxx1113VWmf8l+8U6dO1UxRtWjKlClGu3btjLKyskq3P/jgg0ZUVJRDW1hYmDFhwoTaKK/aXWq8iYmJho+PT+0WVU2ioqKMsWPHOrQNGTLEGDFixAX3eeaZZ4zOnTs7tD300ENGREREjdRYnZwZb0P+3T137pzh4uJirF271qG9e/fuxnPPPVfpPg31+joz1oZ6bX8fiMrKygx/f39j4cKF9rbTp08bbm5uxnvvvXfB49x2221GbGysfb20tNQIDAw0EhISqqVOHpmZwL///W/deuut+tOf/iRfX19169ZN//jHPy5r365duyogIED33HOPduzYUcOVVr+ioiK98847Gjt2rCwWS6V90tLS1K9fP4e2iIgIpaWl1UaJ1epyxitJZ8+eVXBwsIKCgjR48GDt2bOnFqt03h133KFNmzbphx9+kCR98803+vTTTxUZGXnBfRry9XVmvOUa4u9uSUmJSktLKzzC9fDw0KefflrpPg31+joz1nIN8dr+1qFDh5Sbm+tw3Xx8fBQWFnbB61ZUVKT09HSHfRo1aqR+/fpV27UmEJnAwYMH9frrr+vGG2/Uhg0bNHHiRE2ePFlvvvnmBfcJCAjQ0qVL9cEHH+iDDz5QUFCQevfurV27dtVi5VduzZo1On36tMaMGXPBPrm5uRW+wsXPz69ePpe/lMsZb4cOHbR8+XJ9+OGHeuedd1RWVqY77rhDR48erb1CnfTss89q2LBhuummm9S4cWN169ZNU6dO1YgRIy64z4Wur81m0y+//FLTJV8RZ8bbkH93vby8FB4errlz5+rYsWMqLS3VO++8o7S0NOXk5FS6T0O9vs6MtSFf298q/9talb+7P/30k0pLS2v0b7VpvsvMzMrKynTrrbfqxRdflCR169ZNu3fv1tKlSxUdHV3pPh06dFCHDh3s63fccYcOHDigl156SW+//Xat1F0d3njjDUVGRiowMLCuS6kVlzPe8PBwhYeH29fvuOMOdezYUX//+981d+7c2ijTae+//76Sk5P17rvvqnPnzsrIyNDUqVMVGBh4wf+WGzJnxtvQf3fffvttjR07Vtddd51cXFzUvXt3Pfzww0pPT6/r0qpdVcfa0K9tfccdIhMICAhQp06dHNo6duyo7OzsKh3ntttu0/79+6uztBp15MgRpaam6tFHH71oP39/f+Xl5Tm05eXlyd/fvybLq3aXO97fK7/z0BCu7dNPP22/axISEqJRo0Zp2rRpSkhIuOA+F7q+3t7e8vDwqOmSr4gz461MQ/rdbdeunbZt26azZ8/qxx9/1BdffKHi4mK1bdu20v4N+fpWdayVaUjXtlz539aq/N1t0aKFXFxcavRvNYHIBO68805lZWU5tP3www8KDg6u0nEyMjIUEBBQnaXVqMTERPn6+ioqKuqi/cLDw7Vp0yaHtpSUFIe7KA3B5Y7390pLS/Xtt982iGt77tw5NWrk+GfLxcVFZWVlF9ynIV9fZ8ZbmYb2uytJTZo0UUBAgE6dOqUNGzZo8ODBlfZryNe33OWOtTIN8dq2adNG/v7+DtfNZrPp888/v+B1s1qtCg0NddinrKxMmzZtqr5rXS1Ts1GvffHFF4arq6vx5z//2di3b5+RnJxseHp6Gu+88469z7PPPmuMGjXKvv7SSy8Za9asMfbt22d8++23xpQpU4xGjRoZqampdTGEKistLTVatWplxMfHV9g2atQo49lnn7Wv79ixw3B1dTX+8pe/GN99950xa9asBvXavWFUbbxz5swxNmzYYBw4cMBIT083hg0bZri7uxt79uypzZKdEh0dbVx33XX219BXr15ttGjRwnjmmWfsfX7/33L5a9lPP/208d133xmvvfZag3kt25nxNvTf3fXr1xsff/yxcfDgQWPjxo3GLbfcYoSFhRlFRUWGYVxd17eqY21I1/bMmTPG119/bXz99deGJGPRokXG119/bRw5csQwjF9fu2/atKnx4YcfGpmZmcbgwYMrvHZ/9913G6+88op9fcWKFYabm5uRlJRk7N271xg/frzRtGnTavvYEAKRSfznP/8xunTpYri5uRk33XSTsWzZMoft0dHRRq9evezr8+fPN9q1a2e4u7sbzZo1M3r37m1s3ry5lqt23oYNG+yf1/F7vXr1MqKjox3a3n//faN9+/aG1Wo1OnfubKxbt66WKq0eVRnv1KlTjVatWhlWq9Xw8/MzBg4caOzatasWq3WezWYzpkyZYrRq1cpwd3c32rZtazz33HNGYWGhvc/v/1s2jF9fV+7atathtVqNtm3bGomJibVbuJOcGW9D/91duXKl0bZtW8NqtRr+/v5GbGyscfr0afv2q+n6VnWsDenaln9EwO+X8r9FZWVlxvPPP2/4+fkZbm5uRt++fSv8/QoODjZmzZrl0PbKK6/Y/37ddtttxmeffVZtNVsM4xIf8QoAAHCVYw4RAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwPQIRAAAwvf8HZ/fCW4+dfe8AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# check distribution\n",
        "from matplotlib import pyplot as plt\n",
        "df['edit_distance'].plot(kind='hist', bins=20, title='edit_distance')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oB29txqDo-V1",
        "outputId": "88de2faf-4047-40f4-90ba-197ef7008fea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total: 5171\n",
            "tp: 546, fp: 340, fn: 953, tn: 3332\n",
            "precision: 0.6162528216704289, recall: 0.3642428285523682, f1: 0.45786163522012574\n"
          ]
        }
      ],
      "source": [
        "# evaluation\n",
        "edit_distance_threshold = 7\n",
        "df[\"model_edit_distance\"] = df['edit_distance'] > edit_distance_threshold\n",
        "edit_distance_predicts = df[\"model_edit_distance\"].tolist()\n",
        "\n",
        "print_metrics(*confusion_matrix(real_labels, edit_distance_predicts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqyIeH74LUCl"
      },
      "source": [
        "#### c-4. NLTK\n",
        "1. Case\n",
        "2. Lemmatization\n",
        "3. Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Dy-5NaRgMVL"
      },
      "source": [
        "##### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /home/devrok/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L3elNk87gKP9",
        "outputId": "079b1690-04fb-4bb1-979c-83e9e169b077"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Measuring edit distance: 100%|██████████| 5171/5171 [01:21<00:00, 63.11it/s]\n",
            "Measuring edit distance: 100%|██████████| 5171/5171 [00:00<00:00, 33477.11it/s]\n",
            "Measuring edit distance: 100%|██████████| 5171/5171 [00:01<00:00, 4035.96it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>label_num</th>\n",
              "      <th>model_without_nlp</th>\n",
              "      <th>model_rule_based</th>\n",
              "      <th>model_regex</th>\n",
              "      <th>words</th>\n",
              "      <th>edit_distance</th>\n",
              "      <th>model_edit_distance</th>\n",
              "      <th>nltk_tokens</th>\n",
              "      <th>nltk_lower_tokens</th>\n",
              "      <th>nltk_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>605</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, nron, methanol, meter, follow, th, ...</td>\n",
              "      <td>6.818311</td>\n",
              "      <td>False</td>\n",
              "      <td>[Subject, :, enron, methanol, ;, meter, #, :, ...</td>\n",
              "      <td>[subject, :, enron, methanol, ;, meter, #, :, ...</td>\n",
              "      <td>[subject, nron, methanol, meter, follow, th, g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2349</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[subject:, hpl, nom, january, attached, fil, h...</td>\n",
              "      <td>6.775000</td>\n",
              "      <td>False</td>\n",
              "      <td>[Subject, :, hpl, nom, for, january, 9, ,, 200...</td>\n",
              "      <td>[subject, :, hpl, nom, for, january, 9, ,, 200...</td>\n",
              "      <td>[subject, hpl, nom, january, attached, fil, hp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3624</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, neon, retreat, ho, ho, ho, around, ...</td>\n",
              "      <td>6.692322</td>\n",
              "      <td>False</td>\n",
              "      <td>[Subject, :, neon, retreat, ho, ho, ho, ,, we,...</td>\n",
              "      <td>[subject, :, neon, retreat, ho, ho, ho, ,, we,...</td>\n",
              "      <td>[subject, neon, retreat, ho, ho, ho, around, w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4685</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, photoshop, windows, offic, cheap, m...</td>\n",
              "      <td>7.801136</td>\n",
              "      <td>True</td>\n",
              "      <td>[Subject, :, photoshop, ,, windows, ,, office,...</td>\n",
              "      <td>[subject, :, photoshop, ,, windows, ,, office,...</td>\n",
              "      <td>[subject, photoshop, windows, offic, cheap, ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2030</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, indian, springs, deal, book, th, te...</td>\n",
              "      <td>6.767004</td>\n",
              "      <td>False</td>\n",
              "      <td>[Subject, :, re, :, indian, springs, this, dea...</td>\n",
              "      <td>[subject, :, re, :, indian, springs, this, dea...</td>\n",
              "      <td>[subject, indian, springs, deal, book, th, tec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5166</th>\n",
              "      <td>1518</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: put the 10 on the ft\\r\\nthe transport...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, put, th, th, ft, th, transport, vol...</td>\n",
              "      <td>6.779789</td>\n",
              "      <td>False</td>\n",
              "      <td>[Subject, :, put, the, 10, on, the, ft, the, t...</td>\n",
              "      <td>[subject, :, put, the, 10, on, the, ft, the, t...</td>\n",
              "      <td>[subject, put, th, th, ft, th, transport, volu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5167</th>\n",
              "      <td>404</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: 3 / 4 / 2000 and following noms\\r\\nhp...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, following, noms, hpl, tak, th, xtra...</td>\n",
              "      <td>6.668791</td>\n",
              "      <td>False</td>\n",
              "      <td>[Subject, :, 3, /, 4, /, 2000, and, following,...</td>\n",
              "      <td>[subject, :, 3, /, 4, /, 2000, and, following,...</td>\n",
              "      <td>[subject, following, noms, hpl, tak, th, xtra,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5168</th>\n",
              "      <td>2933</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: calpine daily gas nomination\\r\\n&gt;\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[subject:, calpin, daily, gas, nomination, jul...</td>\n",
              "      <td>6.828676</td>\n",
              "      <td>False</td>\n",
              "      <td>[Subject, :, calpine, daily, gas, nomination, ...</td>\n",
              "      <td>[subject, :, calpine, daily, gas, nomination, ...</td>\n",
              "      <td>[subject, calpin, daily, gas, nomination, juli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5169</th>\n",
              "      <td>1409</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: industrial worksheets for august 2000...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, industrial, worksheets, august, act...</td>\n",
              "      <td>7.225840</td>\n",
              "      <td>True</td>\n",
              "      <td>[Subject, :, industrial, worksheets, for, augu...</td>\n",
              "      <td>[subject, :, industrial, worksheets, for, augu...</td>\n",
              "      <td>[subject, industrial, worksheets, august, acti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5170</th>\n",
              "      <td>4807</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: important online banking alert\\r\\ndea...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, important, onlin, banking, alert, d...</td>\n",
              "      <td>6.909502</td>\n",
              "      <td>False</td>\n",
              "      <td>[Subject, :, important, online, banking, alert...</td>\n",
              "      <td>[subject, :, important, online, banking, alert...</td>\n",
              "      <td>[subject, important, onlin, banking, alert, de...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5171 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0 label                                               text  \\\n",
              "0            605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
              "1           2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
              "2           3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
              "3           4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
              "4           2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
              "...          ...   ...                                                ...   \n",
              "5166        1518   ham  Subject: put the 10 on the ft\\r\\nthe transport...   \n",
              "5167         404   ham  Subject: 3 / 4 / 2000 and following noms\\r\\nhp...   \n",
              "5168        2933   ham  Subject: calpine daily gas nomination\\r\\n>\\r\\n...   \n",
              "5169        1409   ham  Subject: industrial worksheets for august 2000...   \n",
              "5170        4807  spam  Subject: important online banking alert\\r\\ndea...   \n",
              "\n",
              "      label_num  model_without_nlp  model_rule_based  model_regex  \\\n",
              "0             0                  0                 1            1   \n",
              "1             0                  0                 0            0   \n",
              "2             0                  1                 1            1   \n",
              "3             1                  0                 1            1   \n",
              "4             0                  0                 1            1   \n",
              "...         ...                ...               ...          ...   \n",
              "5166          0                  0                 1            1   \n",
              "5167          0                  1                 1            1   \n",
              "5168          0                  0                 0            0   \n",
              "5169          0                  0                 1            1   \n",
              "5170          1                  1                 1            1   \n",
              "\n",
              "                                                  words  edit_distance  \\\n",
              "0     [subject:, nron, methanol, meter, follow, th, ...       6.818311   \n",
              "1     [subject:, hpl, nom, january, attached, fil, h...       6.775000   \n",
              "2     [subject:, neon, retreat, ho, ho, ho, around, ...       6.692322   \n",
              "3     [subject:, photoshop, windows, offic, cheap, m...       7.801136   \n",
              "4     [subject:, indian, springs, deal, book, th, te...       6.767004   \n",
              "...                                                 ...            ...   \n",
              "5166  [subject:, put, th, th, ft, th, transport, vol...       6.779789   \n",
              "5167  [subject:, following, noms, hpl, tak, th, xtra...       6.668791   \n",
              "5168  [subject:, calpin, daily, gas, nomination, jul...       6.828676   \n",
              "5169  [subject:, industrial, worksheets, august, act...       7.225840   \n",
              "5170  [subject:, important, onlin, banking, alert, d...       6.909502   \n",
              "\n",
              "      model_edit_distance                                        nltk_tokens  \\\n",
              "0                   False  [Subject, :, enron, methanol, ;, meter, #, :, ...   \n",
              "1                   False  [Subject, :, hpl, nom, for, january, 9, ,, 200...   \n",
              "2                   False  [Subject, :, neon, retreat, ho, ho, ho, ,, we,...   \n",
              "3                    True  [Subject, :, photoshop, ,, windows, ,, office,...   \n",
              "4                   False  [Subject, :, re, :, indian, springs, this, dea...   \n",
              "...                   ...                                                ...   \n",
              "5166                False  [Subject, :, put, the, 10, on, the, ft, the, t...   \n",
              "5167                False  [Subject, :, 3, /, 4, /, 2000, and, following,...   \n",
              "5168                False  [Subject, :, calpine, daily, gas, nomination, ...   \n",
              "5169                 True  [Subject, :, industrial, worksheets, for, augu...   \n",
              "5170                False  [Subject, :, important, online, banking, alert...   \n",
              "\n",
              "                                      nltk_lower_tokens  \\\n",
              "0     [subject, :, enron, methanol, ;, meter, #, :, ...   \n",
              "1     [subject, :, hpl, nom, for, january, 9, ,, 200...   \n",
              "2     [subject, :, neon, retreat, ho, ho, ho, ,, we,...   \n",
              "3     [subject, :, photoshop, ,, windows, ,, office,...   \n",
              "4     [subject, :, re, :, indian, springs, this, dea...   \n",
              "...                                                 ...   \n",
              "5166  [subject, :, put, the, 10, on, the, ft, the, t...   \n",
              "5167  [subject, :, 3, /, 4, /, 2000, and, following,...   \n",
              "5168  [subject, :, calpine, daily, gas, nomination, ...   \n",
              "5169  [subject, :, industrial, worksheets, for, augu...   \n",
              "5170  [subject, :, important, online, banking, alert...   \n",
              "\n",
              "                                             nltk_words  \n",
              "0     [subject, nron, methanol, meter, follow, th, g...  \n",
              "1     [subject, hpl, nom, january, attached, fil, hp...  \n",
              "2     [subject, neon, retreat, ho, ho, ho, around, w...  \n",
              "3     [subject, photoshop, windows, offic, cheap, ma...  \n",
              "4     [subject, indian, springs, deal, book, th, tec...  \n",
              "...                                                 ...  \n",
              "5166  [subject, put, th, th, ft, th, transport, volu...  \n",
              "5167  [subject, following, noms, hpl, tak, th, xtra,...  \n",
              "5168  [subject, calpin, daily, gas, nomination, juli...  \n",
              "5169  [subject, industrial, worksheets, august, acti...  \n",
              "5170  [subject, important, onlin, banking, alert, de...  \n",
              "\n",
              "[5171 rows x 13 columns]"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "df[\"nltk_tokens\"] = df[\"text\"].progress_apply(lambda txts: word_tokenize(txts)) # 파이썬에 있는 torken module 이용\n",
        "\n",
        "def str_lower(ts: List[str]):\n",
        "  l = []\n",
        "  for t in ts:\n",
        "    l.append(t.lower())\n",
        "  return l\n",
        "\n",
        "df[\"nltk_lower_tokens\"] = df[\"nltk_tokens\"].progress_apply(lambda txts: str_lower(txts))\n",
        "df[\"nltk_words\"] = df[\"nltk_lower_tokens\"].progress_apply(lambda txts: regularize_tokens(txts)) # token를 정규화\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCo3tYNCc5kr"
      },
      "source": [
        "##### Lemmatization\n",
        "Examples of lemmatization:\n",
        "\n",
        "-> rocks : rock\n",
        "\n",
        "-> corpora : corpus\n",
        "\n",
        "-> better : good\n",
        "\n",
        "### Method1: Rule based\n",
        "Word: “walked”\n",
        "\n",
        "Rule Application: Remove “-ed”\n",
        "\n",
        "Result: “walk\n",
        "\n",
        "### Method2: Dictionary based\n",
        "\n",
        "‘running’ -> ‘run’\n",
        "\n",
        "‘better’ -> ‘good’\n",
        "\n",
        "‘went’ -> ‘go’\n",
        "\n",
        "### Method3: ML based (NLTK)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcioUi8FdCPV",
        "outputId": "b9c7a245-2e47-414d-80e8-342a9b7fad5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rocks : rock\n",
            "corpora : corpus\n",
            "daaaam : dump\n",
            "daaaam : child\n"
          ]
        }
      ],
      "source": [
        "# example\n",
        "# Reference: https://www.geeksforgeeks.org/python-lemmatization-with-nltk/\n",
        "from nltk.stem import WordNetLemmatizer # 표제어 추출 module\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZA5l7KuDwd_",
        "outputId": "1e4752d7-c90b-46bc-a337-d670d158df51"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (3.8.2)\n",
            "Requirement already satisfied: click in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from nltk) (2024.7.24)\n",
            "Requirement already satisfied: tqdm in /home/devrok/.conda/envs/nlp_pt/lib/python3.10/site-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/devrok/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "1CU5pibgb5DZ",
        "outputId": "85cbc339-ff8c-4c9c-f4bf-7e924f2a5ea5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Lemmatizing...: 100%|██████████| 5171/5171 [00:01<00:00, 3521.19it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nltk_words</th>\n",
              "      <th>lemma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[subject, nron, methanol, meter, follow, th, g...</td>\n",
              "      <td>[subject, nron, methanol, meter, follow, th, g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[subject, hpl, nom, january, attached, fil, hp...</td>\n",
              "      <td>[subject, hpl, nom, january, attached, fil, hp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[subject, neon, retreat, ho, ho, ho, around, w...</td>\n",
              "      <td>[subject, neon, retreat, ho, ho, ho, around, w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[subject, photoshop, windows, offic, cheap, ma...</td>\n",
              "      <td>[subject, photoshop, window, offic, cheap, mai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[subject, indian, springs, deal, book, th, tec...</td>\n",
              "      <td>[subject, indian, spring, deal, book, th, teco...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5166</th>\n",
              "      <td>[subject, put, th, th, ft, th, transport, volu...</td>\n",
              "      <td>[subject, put, th, th, ft, th, transport, volu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5167</th>\n",
              "      <td>[subject, following, noms, hpl, tak, th, xtra,...</td>\n",
              "      <td>[subject, following, noms, hpl, tak, th, xtra,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5168</th>\n",
              "      <td>[subject, calpin, daily, gas, nomination, juli...</td>\n",
              "      <td>[subject, calpin, daily, gas, nomination, juli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5169</th>\n",
              "      <td>[subject, industrial, worksheets, august, acti...</td>\n",
              "      <td>[subject, industrial, worksheet, august, activ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5170</th>\n",
              "      <td>[subject, important, onlin, banking, alert, de...</td>\n",
              "      <td>[subject, important, onlin, banking, alert, de...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5171 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             nltk_words  \\\n",
              "0     [subject, nron, methanol, meter, follow, th, g...   \n",
              "1     [subject, hpl, nom, january, attached, fil, hp...   \n",
              "2     [subject, neon, retreat, ho, ho, ho, around, w...   \n",
              "3     [subject, photoshop, windows, offic, cheap, ma...   \n",
              "4     [subject, indian, springs, deal, book, th, tec...   \n",
              "...                                                 ...   \n",
              "5166  [subject, put, th, th, ft, th, transport, volu...   \n",
              "5167  [subject, following, noms, hpl, tak, th, xtra,...   \n",
              "5168  [subject, calpin, daily, gas, nomination, juli...   \n",
              "5169  [subject, industrial, worksheets, august, acti...   \n",
              "5170  [subject, important, onlin, banking, alert, de...   \n",
              "\n",
              "                                                  lemma  \n",
              "0     [subject, nron, methanol, meter, follow, th, g...  \n",
              "1     [subject, hpl, nom, january, attached, fil, hp...  \n",
              "2     [subject, neon, retreat, ho, ho, ho, around, w...  \n",
              "3     [subject, photoshop, window, offic, cheap, mai...  \n",
              "4     [subject, indian, spring, deal, book, th, teco...  \n",
              "...                                                 ...  \n",
              "5166  [subject, put, th, th, ft, th, transport, volu...  \n",
              "5167  [subject, following, noms, hpl, tak, th, xtra,...  \n",
              "5168  [subject, calpin, daily, gas, nomination, juli...  \n",
              "5169  [subject, industrial, worksheet, august, activ...  \n",
              "5170  [subject, important, onlin, banking, alert, de...  \n",
              "\n",
              "[5171 rows x 2 columns]"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# logic\n",
        "from fast_edit_distance import edit_distance as f_edit_distance\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"Lemmatizing...\")\n",
        "\n",
        "\n",
        "def lemmatize(ws: List[str]):\n",
        "  return [lemmatizer.lemmatize(w) for w in ws]\n",
        "  #l = []\n",
        "  #for w in ws:\n",
        "  #  pos_tag = nltk.pos_tag([w])[0][1]\n",
        "  #  l.append(lemmatizer.lemmatize(w, pos=pos_tag.lower()))\n",
        "  #return l\n",
        "\n",
        "df[\"lemma\"] = df[\"nltk_words\"].progress_apply(lambda txts: lemmatize(txts))\n",
        "\n",
        "\n",
        "df.loc[:, [\"nltk_words\", \"lemma\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU1PxrzTdgTw"
      },
      "source": [
        "##### Stemming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utyuLXwvdfxH",
        "outputId": "aa025ce7-2ee6-4609-ba50-a71ca9374a04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "program  :  program\n",
            "programs  :  program\n",
            "programmer  :  programm\n",
            "programming  :  program\n",
            "programmers  :  programm\n"
          ]
        }
      ],
      "source": [
        "# example\n",
        "# Reference: https://www.geeksforgeeks.org/python-stemming-words-with-nltk/\n",
        "# import these modules\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# choose some words to be stemmed\n",
        "words = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmers\"]\n",
        "\n",
        "for w in words:\n",
        "    print(w, \" : \", ps.stem(w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "oBy1AdJYgF5q",
        "outputId": "f846319b-2141-4bd9-bbc9-dc223eeb0161"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stemming...: 100%|██████████| 5171/5171 [00:06<00:00, 772.15it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nltk_words</th>\n",
              "      <th>lemma</th>\n",
              "      <th>stems</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[subject, nron, methanol, meter, follow, th, g...</td>\n",
              "      <td>[subject, nron, methanol, meter, follow, th, g...</td>\n",
              "      <td>[subject, nron, methanol, meter, follow, th, g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[subject, hpl, nom, january, attached, fil, hp...</td>\n",
              "      <td>[subject, hpl, nom, january, attached, fil, hp...</td>\n",
              "      <td>[subject, hpl, nom, januari, attach, fil, hpln...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[subject, neon, retreat, ho, ho, ho, around, w...</td>\n",
              "      <td>[subject, neon, retreat, ho, ho, ho, around, w...</td>\n",
              "      <td>[subject, neon, retreat, ho, ho, ho, around, w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[subject, photoshop, windows, offic, cheap, ma...</td>\n",
              "      <td>[subject, photoshop, window, offic, cheap, mai...</td>\n",
              "      <td>[subject, photoshop, window, offic, cheap, mai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[subject, indian, springs, deal, book, th, tec...</td>\n",
              "      <td>[subject, indian, spring, deal, book, th, teco...</td>\n",
              "      <td>[subject, indian, spring, deal, book, th, teco...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5166</th>\n",
              "      <td>[subject, put, th, th, ft, th, transport, volu...</td>\n",
              "      <td>[subject, put, th, th, ft, th, transport, volu...</td>\n",
              "      <td>[subject, put, th, th, ft, th, transport, volu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5167</th>\n",
              "      <td>[subject, following, noms, hpl, tak, th, xtra,...</td>\n",
              "      <td>[subject, following, noms, hpl, tak, th, xtra,...</td>\n",
              "      <td>[subject, follow, nom, hpl, tak, th, xtra, mmc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5168</th>\n",
              "      <td>[subject, calpin, daily, gas, nomination, juli...</td>\n",
              "      <td>[subject, calpin, daily, gas, nomination, juli...</td>\n",
              "      <td>[subject, calpin, daili, ga, nomin, juli, ment...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5169</th>\n",
              "      <td>[subject, industrial, worksheets, august, acti...</td>\n",
              "      <td>[subject, industrial, worksheet, august, activ...</td>\n",
              "      <td>[subject, industri, worksheet, august, activ, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5170</th>\n",
              "      <td>[subject, important, onlin, banking, alert, de...</td>\n",
              "      <td>[subject, important, onlin, banking, alert, de...</td>\n",
              "      <td>[subject, import, onlin, bank, alert, dear, va...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5171 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             nltk_words  \\\n",
              "0     [subject, nron, methanol, meter, follow, th, g...   \n",
              "1     [subject, hpl, nom, january, attached, fil, hp...   \n",
              "2     [subject, neon, retreat, ho, ho, ho, around, w...   \n",
              "3     [subject, photoshop, windows, offic, cheap, ma...   \n",
              "4     [subject, indian, springs, deal, book, th, tec...   \n",
              "...                                                 ...   \n",
              "5166  [subject, put, th, th, ft, th, transport, volu...   \n",
              "5167  [subject, following, noms, hpl, tak, th, xtra,...   \n",
              "5168  [subject, calpin, daily, gas, nomination, juli...   \n",
              "5169  [subject, industrial, worksheets, august, acti...   \n",
              "5170  [subject, important, onlin, banking, alert, de...   \n",
              "\n",
              "                                                  lemma  \\\n",
              "0     [subject, nron, methanol, meter, follow, th, g...   \n",
              "1     [subject, hpl, nom, january, attached, fil, hp...   \n",
              "2     [subject, neon, retreat, ho, ho, ho, around, w...   \n",
              "3     [subject, photoshop, window, offic, cheap, mai...   \n",
              "4     [subject, indian, spring, deal, book, th, teco...   \n",
              "...                                                 ...   \n",
              "5166  [subject, put, th, th, ft, th, transport, volu...   \n",
              "5167  [subject, following, noms, hpl, tak, th, xtra,...   \n",
              "5168  [subject, calpin, daily, gas, nomination, juli...   \n",
              "5169  [subject, industrial, worksheet, august, activ...   \n",
              "5170  [subject, important, onlin, banking, alert, de...   \n",
              "\n",
              "                                                  stems  \n",
              "0     [subject, nron, methanol, meter, follow, th, g...  \n",
              "1     [subject, hpl, nom, januari, attach, fil, hpln...  \n",
              "2     [subject, neon, retreat, ho, ho, ho, around, w...  \n",
              "3     [subject, photoshop, window, offic, cheap, mai...  \n",
              "4     [subject, indian, spring, deal, book, th, teco...  \n",
              "...                                                 ...  \n",
              "5166  [subject, put, th, th, ft, th, transport, volu...  \n",
              "5167  [subject, follow, nom, hpl, tak, th, xtra, mmc...  \n",
              "5168  [subject, calpin, daili, ga, nomin, juli, ment...  \n",
              "5169  [subject, industri, worksheet, august, activ, ...  \n",
              "5170  [subject, import, onlin, bank, alert, dear, va...  \n",
              "\n",
              "[5171 rows x 3 columns]"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# logic\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"Stemming...\")\n",
        "\n",
        "\n",
        "def stemming(ws: List[str]):\n",
        "  return [\n",
        "      ps.stem(w)\n",
        "      for w in ws\n",
        "  ]\n",
        "\n",
        "df[\"stems\"] = df[\"lemma\"].progress_apply(lambda txts: stemming(txts))\n",
        "df.loc[:, [\"nltk_words\", \"lemma\", \"stems\"]]\n",
        "#[subject, neon, retreat, ho, ho, ho, around, -> ho : stemming이 안되는 곳"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chF0FGsNhOLo"
      },
      "source": [
        "##### Lemmatize and Stem the keywords!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MVvCCy5vhNzQ",
        "outputId": "3d66267f-00c0-4404-c1cc-b2009614ba6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['click', '$$$', 'lifetime', 'rates', 'sales', 'clearance', 'certified', 'obligation', 'consultation', 'sample']\n",
            "['solut', 'subscrib', 'click', '$$$', 'afford', 'marketing solut', 'bonu', 'no questions ask', 'guarante', 'expir']\n"
          ]
        }
      ],
      "source": [
        "lemmatized_keywords = lemmatize(keywords)\n",
        "stemmed_keywords = stemming(lemmatized_keywords)\n",
        "clean_keywords = list(set(stemmed_keywords))\n",
        "print(keywords[:10])\n",
        "print(clean_keywords[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB_hMmi2QVIH"
      },
      "source": [
        "#### c-5. apply NLTK to method c-2, c-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4SLTE_hj5yy"
      },
      "source": [
        "##### Regex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VBNn9N9bwntS",
        "outputId": "f2041eac-4d61-4c50-b065-a32107394c6d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stemming...: 100%|██████████| 5171/5171 [00:17<00:00, 288.77it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>label_num</th>\n",
              "      <th>model_without_nlp</th>\n",
              "      <th>model_rule_based</th>\n",
              "      <th>model_regex</th>\n",
              "      <th>words</th>\n",
              "      <th>edit_distance</th>\n",
              "      <th>model_edit_distance</th>\n",
              "      <th>nltk_tokens</th>\n",
              "      <th>nltk_lower_tokens</th>\n",
              "      <th>nltk_words</th>\n",
              "      <th>lemma</th>\n",
              "      <th>stems</th>\n",
              "      <th>model_clean_regex</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>605</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, nron, methanol, meter, follow, th, ...</td>\n",
              "      <td>6.818311</td>\n",
              "      <td>False</td>\n",
              "      <td>[Subject, :, enron, methanol, ;, meter, #, :, ...</td>\n",
              "      <td>[subject, :, enron, methanol, ;, meter, #, :, ...</td>\n",
              "      <td>[subject, nron, methanol, meter, follow, th, g...</td>\n",
              "      <td>[subject, nron, methanol, meter, follow, th, g...</td>\n",
              "      <td>[subject, nron, methanol, meter, follow, th, g...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2349</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[subject:, hpl, nom, january, attached, fil, h...</td>\n",
              "      <td>6.775000</td>\n",
              "      <td>False</td>\n",
              "      <td>[Subject, :, hpl, nom, for, january, 9, ,, 200...</td>\n",
              "      <td>[subject, :, hpl, nom, for, january, 9, ,, 200...</td>\n",
              "      <td>[subject, hpl, nom, january, attached, fil, hp...</td>\n",
              "      <td>[subject, hpl, nom, january, attached, fil, hp...</td>\n",
              "      <td>[subject, hpl, nom, januari, attach, fil, hpln...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3624</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, neon, retreat, ho, ho, ho, around, ...</td>\n",
              "      <td>6.692322</td>\n",
              "      <td>False</td>\n",
              "      <td>[Subject, :, neon, retreat, ho, ho, ho, ,, we,...</td>\n",
              "      <td>[subject, :, neon, retreat, ho, ho, ho, ,, we,...</td>\n",
              "      <td>[subject, neon, retreat, ho, ho, ho, around, w...</td>\n",
              "      <td>[subject, neon, retreat, ho, ho, ho, around, w...</td>\n",
              "      <td>[subject, neon, retreat, ho, ho, ho, around, w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4685</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, photoshop, windows, offic, cheap, m...</td>\n",
              "      <td>7.801136</td>\n",
              "      <td>True</td>\n",
              "      <td>[Subject, :, photoshop, ,, windows, ,, office,...</td>\n",
              "      <td>[subject, :, photoshop, ,, windows, ,, office,...</td>\n",
              "      <td>[subject, photoshop, windows, offic, cheap, ma...</td>\n",
              "      <td>[subject, photoshop, window, offic, cheap, mai...</td>\n",
              "      <td>[subject, photoshop, window, offic, cheap, mai...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2030</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, indian, springs, deal, book, th, te...</td>\n",
              "      <td>6.767004</td>\n",
              "      <td>False</td>\n",
              "      <td>[Subject, :, re, :, indian, springs, this, dea...</td>\n",
              "      <td>[subject, :, re, :, indian, springs, this, dea...</td>\n",
              "      <td>[subject, indian, springs, deal, book, th, tec...</td>\n",
              "      <td>[subject, indian, spring, deal, book, th, teco...</td>\n",
              "      <td>[subject, indian, spring, deal, book, th, teco...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5166</th>\n",
              "      <td>1518</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: put the 10 on the ft\\r\\nthe transport...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, put, th, th, ft, th, transport, vol...</td>\n",
              "      <td>6.779789</td>\n",
              "      <td>False</td>\n",
              "      <td>[Subject, :, put, the, 10, on, the, ft, the, t...</td>\n",
              "      <td>[subject, :, put, the, 10, on, the, ft, the, t...</td>\n",
              "      <td>[subject, put, th, th, ft, th, transport, volu...</td>\n",
              "      <td>[subject, put, th, th, ft, th, transport, volu...</td>\n",
              "      <td>[subject, put, th, th, ft, th, transport, volu...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5167</th>\n",
              "      <td>404</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: 3 / 4 / 2000 and following noms\\r\\nhp...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, following, noms, hpl, tak, th, xtra...</td>\n",
              "      <td>6.668791</td>\n",
              "      <td>False</td>\n",
              "      <td>[Subject, :, 3, /, 4, /, 2000, and, following,...</td>\n",
              "      <td>[subject, :, 3, /, 4, /, 2000, and, following,...</td>\n",
              "      <td>[subject, following, noms, hpl, tak, th, xtra,...</td>\n",
              "      <td>[subject, following, noms, hpl, tak, th, xtra,...</td>\n",
              "      <td>[subject, follow, nom, hpl, tak, th, xtra, mmc...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5168</th>\n",
              "      <td>2933</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: calpine daily gas nomination\\r\\n&gt;\\r\\n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[subject:, calpin, daily, gas, nomination, jul...</td>\n",
              "      <td>6.828676</td>\n",
              "      <td>False</td>\n",
              "      <td>[Subject, :, calpine, daily, gas, nomination, ...</td>\n",
              "      <td>[subject, :, calpine, daily, gas, nomination, ...</td>\n",
              "      <td>[subject, calpin, daily, gas, nomination, juli...</td>\n",
              "      <td>[subject, calpin, daily, gas, nomination, juli...</td>\n",
              "      <td>[subject, calpin, daili, ga, nomin, juli, ment...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5169</th>\n",
              "      <td>1409</td>\n",
              "      <td>ham</td>\n",
              "      <td>Subject: industrial worksheets for august 2000...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, industrial, worksheets, august, act...</td>\n",
              "      <td>7.225840</td>\n",
              "      <td>True</td>\n",
              "      <td>[Subject, :, industrial, worksheets, for, augu...</td>\n",
              "      <td>[subject, :, industrial, worksheets, for, augu...</td>\n",
              "      <td>[subject, industrial, worksheets, august, acti...</td>\n",
              "      <td>[subject, industrial, worksheet, august, activ...</td>\n",
              "      <td>[subject, industri, worksheet, august, activ, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5170</th>\n",
              "      <td>4807</td>\n",
              "      <td>spam</td>\n",
              "      <td>Subject: important online banking alert\\r\\ndea...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[subject:, important, onlin, banking, alert, d...</td>\n",
              "      <td>6.909502</td>\n",
              "      <td>False</td>\n",
              "      <td>[Subject, :, important, online, banking, alert...</td>\n",
              "      <td>[subject, :, important, online, banking, alert...</td>\n",
              "      <td>[subject, important, onlin, banking, alert, de...</td>\n",
              "      <td>[subject, important, onlin, banking, alert, de...</td>\n",
              "      <td>[subject, import, onlin, bank, alert, dear, va...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5171 rows × 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0 label                                               text  \\\n",
              "0            605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
              "1           2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
              "2           3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
              "3           4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
              "4           2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
              "...          ...   ...                                                ...   \n",
              "5166        1518   ham  Subject: put the 10 on the ft\\r\\nthe transport...   \n",
              "5167         404   ham  Subject: 3 / 4 / 2000 and following noms\\r\\nhp...   \n",
              "5168        2933   ham  Subject: calpine daily gas nomination\\r\\n>\\r\\n...   \n",
              "5169        1409   ham  Subject: industrial worksheets for august 2000...   \n",
              "5170        4807  spam  Subject: important online banking alert\\r\\ndea...   \n",
              "\n",
              "      label_num  model_without_nlp  model_rule_based  model_regex  \\\n",
              "0             0                  0                 1            1   \n",
              "1             0                  0                 0            0   \n",
              "2             0                  1                 1            1   \n",
              "3             1                  0                 1            1   \n",
              "4             0                  0                 1            1   \n",
              "...         ...                ...               ...          ...   \n",
              "5166          0                  0                 1            1   \n",
              "5167          0                  1                 1            1   \n",
              "5168          0                  0                 0            0   \n",
              "5169          0                  0                 1            1   \n",
              "5170          1                  1                 1            1   \n",
              "\n",
              "                                                  words  edit_distance  \\\n",
              "0     [subject:, nron, methanol, meter, follow, th, ...       6.818311   \n",
              "1     [subject:, hpl, nom, january, attached, fil, h...       6.775000   \n",
              "2     [subject:, neon, retreat, ho, ho, ho, around, ...       6.692322   \n",
              "3     [subject:, photoshop, windows, offic, cheap, m...       7.801136   \n",
              "4     [subject:, indian, springs, deal, book, th, te...       6.767004   \n",
              "...                                                 ...            ...   \n",
              "5166  [subject:, put, th, th, ft, th, transport, vol...       6.779789   \n",
              "5167  [subject:, following, noms, hpl, tak, th, xtra...       6.668791   \n",
              "5168  [subject:, calpin, daily, gas, nomination, jul...       6.828676   \n",
              "5169  [subject:, industrial, worksheets, august, act...       7.225840   \n",
              "5170  [subject:, important, onlin, banking, alert, d...       6.909502   \n",
              "\n",
              "      model_edit_distance                                        nltk_tokens  \\\n",
              "0                   False  [Subject, :, enron, methanol, ;, meter, #, :, ...   \n",
              "1                   False  [Subject, :, hpl, nom, for, january, 9, ,, 200...   \n",
              "2                   False  [Subject, :, neon, retreat, ho, ho, ho, ,, we,...   \n",
              "3                    True  [Subject, :, photoshop, ,, windows, ,, office,...   \n",
              "4                   False  [Subject, :, re, :, indian, springs, this, dea...   \n",
              "...                   ...                                                ...   \n",
              "5166                False  [Subject, :, put, the, 10, on, the, ft, the, t...   \n",
              "5167                False  [Subject, :, 3, /, 4, /, 2000, and, following,...   \n",
              "5168                False  [Subject, :, calpine, daily, gas, nomination, ...   \n",
              "5169                 True  [Subject, :, industrial, worksheets, for, augu...   \n",
              "5170                False  [Subject, :, important, online, banking, alert...   \n",
              "\n",
              "                                      nltk_lower_tokens  \\\n",
              "0     [subject, :, enron, methanol, ;, meter, #, :, ...   \n",
              "1     [subject, :, hpl, nom, for, january, 9, ,, 200...   \n",
              "2     [subject, :, neon, retreat, ho, ho, ho, ,, we,...   \n",
              "3     [subject, :, photoshop, ,, windows, ,, office,...   \n",
              "4     [subject, :, re, :, indian, springs, this, dea...   \n",
              "...                                                 ...   \n",
              "5166  [subject, :, put, the, 10, on, the, ft, the, t...   \n",
              "5167  [subject, :, 3, /, 4, /, 2000, and, following,...   \n",
              "5168  [subject, :, calpine, daily, gas, nomination, ...   \n",
              "5169  [subject, :, industrial, worksheets, for, augu...   \n",
              "5170  [subject, :, important, online, banking, alert...   \n",
              "\n",
              "                                             nltk_words  \\\n",
              "0     [subject, nron, methanol, meter, follow, th, g...   \n",
              "1     [subject, hpl, nom, january, attached, fil, hp...   \n",
              "2     [subject, neon, retreat, ho, ho, ho, around, w...   \n",
              "3     [subject, photoshop, windows, offic, cheap, ma...   \n",
              "4     [subject, indian, springs, deal, book, th, tec...   \n",
              "...                                                 ...   \n",
              "5166  [subject, put, th, th, ft, th, transport, volu...   \n",
              "5167  [subject, following, noms, hpl, tak, th, xtra,...   \n",
              "5168  [subject, calpin, daily, gas, nomination, juli...   \n",
              "5169  [subject, industrial, worksheets, august, acti...   \n",
              "5170  [subject, important, onlin, banking, alert, de...   \n",
              "\n",
              "                                                  lemma  \\\n",
              "0     [subject, nron, methanol, meter, follow, th, g...   \n",
              "1     [subject, hpl, nom, january, attached, fil, hp...   \n",
              "2     [subject, neon, retreat, ho, ho, ho, around, w...   \n",
              "3     [subject, photoshop, window, offic, cheap, mai...   \n",
              "4     [subject, indian, spring, deal, book, th, teco...   \n",
              "...                                                 ...   \n",
              "5166  [subject, put, th, th, ft, th, transport, volu...   \n",
              "5167  [subject, following, noms, hpl, tak, th, xtra,...   \n",
              "5168  [subject, calpin, daily, gas, nomination, juli...   \n",
              "5169  [subject, industrial, worksheet, august, activ...   \n",
              "5170  [subject, important, onlin, banking, alert, de...   \n",
              "\n",
              "                                                  stems  model_clean_regex  \n",
              "0     [subject, nron, methanol, meter, follow, th, g...                  1  \n",
              "1     [subject, hpl, nom, januari, attach, fil, hpln...                  0  \n",
              "2     [subject, neon, retreat, ho, ho, ho, around, w...                  0  \n",
              "3     [subject, photoshop, window, offic, cheap, mai...                  1  \n",
              "4     [subject, indian, spring, deal, book, th, teco...                  0  \n",
              "...                                                 ...                ...  \n",
              "5166  [subject, put, th, th, ft, th, transport, volu...                  0  \n",
              "5167  [subject, follow, nom, hpl, tak, th, xtra, mmc...                  0  \n",
              "5168  [subject, calpin, daili, ga, nomin, juli, ment...                  0  \n",
              "5169  [subject, industri, worksheet, august, activ, ...                  1  \n",
              "5170  [subject, import, onlin, bank, alert, dear, va...                  1  \n",
              "\n",
              "[5171 rows x 16 columns]"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def clean_keyword_regex(text):\n",
        "  # copy your logic on section c-2.\n",
        "  # use clean_keywords instead of keywords\n",
        "  for keyword in clean_keywords:\n",
        "    for t in text:\n",
        "      if re.search(f\"{keyword}\\D+\", t, re.IGNORECASE):\n",
        "        return 1\n",
        "  return 0\n",
        "\n",
        "df[\"model_clean_regex\"] = df[\"stems\"].progress_apply(lambda txt: clean_keyword_regex(txt))\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3Ww1dnRi0Ro",
        "outputId": "2f42644f-124d-489b-f559-7d19c6e73411"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total: 5171\n",
            "tp: 787, fp: 1344, fn: 712, tn: 2328\n",
            "precision: 0.3693101830126701, recall: 0.5250166777851901, f1: 0.4336088154269972\n"
          ]
        }
      ],
      "source": [
        "# previous\n",
        "# precision: 0.3321051338737411, recall: 0.9019346230820547, f1: 0.4854578096947935\n",
        "clean_regex_predicts = df[\"model_clean_regex\"].tolist()\n",
        "\n",
        "print_metrics(*confusion_matrix(real_labels, clean_regex_predicts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnL586ILkMQ-"
      },
      "source": [
        "##### Edit distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "jS589AXUi59y",
        "outputId": "7d4fe59d-d9ca-4f64-bcc0-d03e05359512"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Measuring edit distance: 100%|██████████| 5171/5171 [00:16<00:00, 309.43it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stems</th>\n",
              "      <th>clean_edit_distance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[subject, nron, methanol, meter, follow, th, g...</td>\n",
              "      <td>6.585389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[subject, hpl, nom, januari, attach, fil, hpln...</td>\n",
              "      <td>6.661765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[subject, neon, retreat, ho, ho, ho, around, w...</td>\n",
              "      <td>6.503014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[subject, photoshop, window, offic, cheap, mai...</td>\n",
              "      <td>7.185829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[subject, indian, spring, deal, book, th, teco...</td>\n",
              "      <td>6.524816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5166</th>\n",
              "      <td>[subject, put, th, th, ft, th, transport, volu...</td>\n",
              "      <td>6.682127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5167</th>\n",
              "      <td>[subject, follow, nom, hpl, tak, th, xtra, mmc...</td>\n",
              "      <td>6.536601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5168</th>\n",
              "      <td>[subject, calpin, daili, ga, nomin, juli, ment...</td>\n",
              "      <td>6.527206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5169</th>\n",
              "      <td>[subject, industri, worksheet, august, activ, ...</td>\n",
              "      <td>6.805212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5170</th>\n",
              "      <td>[subject, import, onlin, bank, alert, dear, va...</td>\n",
              "      <td>6.591912</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5171 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  stems  clean_edit_distance\n",
              "0     [subject, nron, methanol, meter, follow, th, g...             6.585389\n",
              "1     [subject, hpl, nom, januari, attach, fil, hpln...             6.661765\n",
              "2     [subject, neon, retreat, ho, ho, ho, around, w...             6.503014\n",
              "3     [subject, photoshop, window, offic, cheap, mai...             7.185829\n",
              "4     [subject, indian, spring, deal, book, th, teco...             6.524816\n",
              "...                                                 ...                  ...\n",
              "5166  [subject, put, th, th, ft, th, transport, volu...             6.682127\n",
              "5167  [subject, follow, nom, hpl, tak, th, xtra, mmc...             6.536601\n",
              "5168  [subject, calpin, daili, ga, nomin, juli, ment...             6.527206\n",
              "5169  [subject, industri, worksheet, august, activ, ...             6.805212\n",
              "5170  [subject, import, onlin, bank, alert, dear, va...             6.591912\n",
              "\n",
              "[5171 rows x 2 columns]"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"Measuring edit distance\")\n",
        "\n",
        "\n",
        "def clean_edit_distance_per_keyword(ws: List[str]):\n",
        "  # copy your logic on section c-3.\n",
        "  # use clean_keywords instead of keywords\n",
        "  distance = 0\n",
        "  for keyword in keywords:\n",
        "    for w in ws:\n",
        "      distance += f_edit_distance(keyword, w)\n",
        "  return distance / (len(keywords) * len(ws))\n",
        "\n",
        "df[\"clean_edit_distance\"] = df[\"stems\"].progress_apply(lambda txts: clean_edit_distance_per_keyword(txts))\n",
        "df.loc[:, [\"stems\", \"clean_edit_distance\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "gKRpsNIqjRjR",
        "outputId": "02487fc9-477b-4e31-edb4-0fdff3191f83"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "df['clean_edit_distance'].plot(kind='hist', bins=20, title='clean_edit_distance')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCSMk_d_jT4t",
        "outputId": "cc0e138a-dd87-4575-fb34-63dcf197dac4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total: 5171\n",
            "tp: 1499, fp: 3672, fn: 0, tn: 0\n",
            "precision: 0.2898859021465867, recall: 1.0, f1: 0.4494752623688156\n"
          ]
        }
      ],
      "source": [
        "# previous\n",
        "# precision: 0.5656108597285068, recall: 0.41694462975316876, f1: 0.4800307219662059\n",
        "\n",
        "edit_distance_threshold = 6\n",
        "df[\"model_clean_edit_distance\"] = df['clean_edit_distance'] > edit_distance_threshold\n",
        "clean_edit_distance_predicts = df[\"model_clean_edit_distance\"].tolist()\n",
        "\n",
        "print_metrics(*confusion_matrix(real_labels, clean_edit_distance_predicts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiU7TjkVAJZD"
      },
      "source": [
        "[4, 3, 1 ...] -> 각 단어간의 등장횟수 vector -> 특정벡터는 label 1로 판단"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SptSzgPe8IpE"
      },
      "source": [
        "## Implement with scikit-learn!\n",
        "\n",
        "* In this section we are going to use **counter vectorizer** to build a classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-K40nTa8HFQ",
        "outputId": "26fdd097-e378-47b0-85b8-e280986f8c66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on class CountVectorizer in module sklearn.feature_extraction.text:\n",
            "\n",
            "class CountVectorizer(_VectorizerMixin, sklearn.base.BaseEstimator)\n",
            " |  CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
            " |  \n",
            " |  Convert a collection of text documents to a matrix of token counts.\n",
            " |  \n",
            " |  This implementation produces a sparse representation of the counts using\n",
            " |  scipy.sparse.csr_matrix.\n",
            " |  \n",
            " |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
            " |  that does some kind of feature selection then the number of features will\n",
            " |  be equal to the vocabulary size found by analyzing the data.\n",
            " |  \n",
            " |  For an efficiency comparison of the different feature extractors, see\n",
            " |  :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  input : {'filename', 'file', 'content'}, default='content'\n",
            " |      - If `'filename'`, the sequence passed as an argument to fit is\n",
            " |        expected to be a list of filenames that need reading to fetch\n",
            " |        the raw content to analyze.\n",
            " |  \n",
            " |      - If `'file'`, the sequence items must have a 'read' method (file-like\n",
            " |        object) that is called to fetch the bytes in memory.\n",
            " |  \n",
            " |      - If `'content'`, the input is expected to be a sequence of items that\n",
            " |        can be of type string or byte.\n",
            " |  \n",
            " |  encoding : str, default='utf-8'\n",
            " |      If bytes or files are given to analyze, this encoding is used to\n",
            " |      decode.\n",
            " |  \n",
            " |  decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
            " |      Instruction on what to do if a byte sequence is given to analyze that\n",
            " |      contains characters not of the given `encoding`. By default, it is\n",
            " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
            " |      values are 'ignore' and 'replace'.\n",
            " |  \n",
            " |  strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
            " |      Remove accents and perform other character normalization\n",
            " |      during the preprocessing step.\n",
            " |      'ascii' is a fast method that only works on characters that have\n",
            " |      a direct ASCII mapping.\n",
            " |      'unicode' is a slightly slower method that works on any characters.\n",
            " |      None (default) means no character normalization is performed.\n",
            " |  \n",
            " |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
            " |      :func:`unicodedata.normalize`.\n",
            " |  \n",
            " |  lowercase : bool, default=True\n",
            " |      Convert all characters to lowercase before tokenizing.\n",
            " |  \n",
            " |  preprocessor : callable, default=None\n",
            " |      Override the preprocessing (strip_accents and lowercase) stage while\n",
            " |      preserving the tokenizing and n-grams generation steps.\n",
            " |      Only applies if ``analyzer`` is not callable.\n",
            " |  \n",
            " |  tokenizer : callable, default=None\n",
            " |      Override the string tokenization step while preserving the\n",
            " |      preprocessing and n-grams generation steps.\n",
            " |      Only applies if ``analyzer == 'word'``.\n",
            " |  \n",
            " |  stop_words : {'english'}, list, default=None\n",
            " |      If 'english', a built-in stop word list for English is used.\n",
            " |      There are several known issues with 'english' and you should\n",
            " |      consider an alternative (see :ref:`stop_words`).\n",
            " |  \n",
            " |      If a list, that list is assumed to contain stop words, all of which\n",
            " |      will be removed from the resulting tokens.\n",
            " |      Only applies if ``analyzer == 'word'``.\n",
            " |  \n",
            " |      If None, no stop words will be used. In this case, setting `max_df`\n",
            " |      to a higher value, such as in the range (0.7, 1.0), can automatically detect\n",
            " |      and filter stop words based on intra corpus document frequency of terms.\n",
            " |  \n",
            " |  token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
            " |      Regular expression denoting what constitutes a \"token\", only used\n",
            " |      if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
            " |      or more alphanumeric characters (punctuation is completely ignored\n",
            " |      and always treated as a token separator).\n",
            " |  \n",
            " |      If there is a capturing group in token_pattern then the\n",
            " |      captured group content, not the entire match, becomes the token.\n",
            " |      At most one capturing group is permitted.\n",
            " |  \n",
            " |  ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
            " |      The lower and upper boundary of the range of n-values for different\n",
            " |      word n-grams or char n-grams to be extracted. All values of n such\n",
            " |      such that min_n <= n <= max_n will be used. For example an\n",
            " |      ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
            " |      unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
            " |      Only applies if ``analyzer`` is not callable.\n",
            " |  \n",
            " |  analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
            " |      Whether the feature should be made of word n-gram or character\n",
            " |      n-grams.\n",
            " |      Option 'char_wb' creates character n-grams only from text inside\n",
            " |      word boundaries; n-grams at the edges of words are padded with space.\n",
            " |  \n",
            " |      If a callable is passed it is used to extract the sequence of features\n",
            " |      out of the raw, unprocessed input.\n",
            " |  \n",
            " |      .. versionchanged:: 0.21\n",
            " |  \n",
            " |      Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
            " |      first read from the file and then passed to the given callable\n",
            " |      analyzer.\n",
            " |  \n",
            " |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
            " |      When building the vocabulary ignore terms that have a document\n",
            " |      frequency strictly higher than the given threshold (corpus-specific\n",
            " |      stop words).\n",
            " |      If float, the parameter represents a proportion of documents, integer\n",
            " |      absolute counts.\n",
            " |      This parameter is ignored if vocabulary is not None.\n",
            " |  \n",
            " |  min_df : float in range [0.0, 1.0] or int, default=1\n",
            " |      When building the vocabulary ignore terms that have a document\n",
            " |      frequency strictly lower than the given threshold. This value is also\n",
            " |      called cut-off in the literature.\n",
            " |      If float, the parameter represents a proportion of documents, integer\n",
            " |      absolute counts.\n",
            " |      This parameter is ignored if vocabulary is not None.\n",
            " |  \n",
            " |  max_features : int, default=None\n",
            " |      If not None, build a vocabulary that only consider the top\n",
            " |      `max_features` ordered by term frequency across the corpus.\n",
            " |      Otherwise, all features are used.\n",
            " |  \n",
            " |      This parameter is ignored if vocabulary is not None.\n",
            " |  \n",
            " |  vocabulary : Mapping or iterable, default=None\n",
            " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
            " |      indices in the feature matrix, or an iterable over terms. If not\n",
            " |      given, a vocabulary is determined from the input documents. Indices\n",
            " |      in the mapping should not be repeated and should not have any gap\n",
            " |      between 0 and the largest index.\n",
            " |  \n",
            " |  binary : bool, default=False\n",
            " |      If True, all non zero counts are set to 1. This is useful for discrete\n",
            " |      probabilistic models that model binary events rather than integer\n",
            " |      counts.\n",
            " |  \n",
            " |  dtype : dtype, default=np.int64\n",
            " |      Type of the matrix returned by fit_transform() or transform().\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  vocabulary_ : dict\n",
            " |      A mapping of terms to feature indices.\n",
            " |  \n",
            " |  fixed_vocabulary_ : bool\n",
            " |      True if a fixed vocabulary of term to indices mapping\n",
            " |      is provided by the user.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  HashingVectorizer : Convert a collection of text documents to a\n",
            " |      matrix of token counts.\n",
            " |  \n",
            " |  TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
            " |      of TF-IDF features.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from sklearn.feature_extraction.text import CountVectorizer\n",
            " |  >>> corpus = [\n",
            " |  ...     'This is the first document.',\n",
            " |  ...     'This document is the second document.',\n",
            " |  ...     'And this is the third one.',\n",
            " |  ...     'Is this the first document?',\n",
            " |  ... ]\n",
            " |  >>> vectorizer = CountVectorizer()\n",
            " |  >>> X = vectorizer.fit_transform(corpus)\n",
            " |  >>> vectorizer.get_feature_names_out()\n",
            " |  array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
            " |         'this'], ...)\n",
            " |  >>> print(X.toarray())\n",
            " |  [[0 1 1 1 0 0 1 0 1]\n",
            " |   [0 2 0 1 0 1 1 0 1]\n",
            " |   [1 0 0 1 1 0 1 1 1]\n",
            " |   [0 1 1 1 0 0 1 0 1]]\n",
            " |  >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
            " |  >>> X2 = vectorizer2.fit_transform(corpus)\n",
            " |  >>> vectorizer2.get_feature_names_out()\n",
            " |  array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
            " |         'second document', 'the first', 'the second', 'the third', 'third one',\n",
            " |         'this document', 'this is', 'this the'], ...)\n",
            " |   >>> print(X2.toarray())\n",
            " |   [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
            " |   [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
            " |   [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
            " |   [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      CountVectorizer\n",
            " |      _VectorizerMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
            " |      sklearn.utils._metadata_requests._MetadataRequester\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, *, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, raw_documents, y=None)\n",
            " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      raw_documents : iterable\n",
            " |          An iterable which generates either str, unicode or file objects.\n",
            " |      \n",
            " |      y : None\n",
            " |          This parameter is ignored.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Fitted vectorizer.\n",
            " |  \n",
            " |  fit_transform(self, raw_documents, y=None)\n",
            " |      Learn the vocabulary dictionary and return document-term matrix.\n",
            " |      \n",
            " |      This is equivalent to fit followed by transform, but more efficiently\n",
            " |      implemented.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      raw_documents : iterable\n",
            " |          An iterable which generates either str, unicode or file objects.\n",
            " |      \n",
            " |      y : None\n",
            " |          This parameter is ignored.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X : array of shape (n_samples, n_features)\n",
            " |          Document-term matrix.\n",
            " |  \n",
            " |  get_feature_names_out(self, input_features=None)\n",
            " |      Get output feature names for transformation.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      input_features : array-like of str or None, default=None\n",
            " |          Not used, present here for API consistency by convention.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      feature_names_out : ndarray of str objects\n",
            " |          Transformed feature names.\n",
            " |  \n",
            " |  inverse_transform(self, X)\n",
            " |      Return terms per document with nonzero entries in X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          Document-term matrix.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X_inv : list of arrays of shape (n_samples,)\n",
            " |          List of arrays of terms.\n",
            " |  \n",
            " |  set_fit_request(self: sklearn.feature_extraction.text.CountVectorizer, *, raw_documents: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.feature_extraction.text.CountVectorizer\n",
            " |      Request metadata passed to the ``fit`` method.\n",
            " |      \n",
            " |      Note that this method is only relevant if\n",
            " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            " |      mechanism works.\n",
            " |      \n",
            " |      The options for each parameter are:\n",
            " |      \n",
            " |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
            " |      \n",
            " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
            " |      \n",
            " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            " |      \n",
            " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            " |      \n",
            " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            " |      existing request. This allows you to change the request for some\n",
            " |      parameters and not others.\n",
            " |      \n",
            " |      .. versionadded:: 1.3\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method is only relevant if this estimator is used as a\n",
            " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      raw_documents : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            " |          Metadata routing for ``raw_documents`` parameter in ``fit``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          The updated object.\n",
            " |  \n",
            " |  set_transform_request(self: sklearn.feature_extraction.text.CountVectorizer, *, raw_documents: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.feature_extraction.text.CountVectorizer\n",
            " |      Request metadata passed to the ``transform`` method.\n",
            " |      \n",
            " |      Note that this method is only relevant if\n",
            " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            " |      mechanism works.\n",
            " |      \n",
            " |      The options for each parameter are:\n",
            " |      \n",
            " |      - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided.\n",
            " |      \n",
            " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``.\n",
            " |      \n",
            " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            " |      \n",
            " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            " |      \n",
            " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            " |      existing request. This allows you to change the request for some\n",
            " |      parameters and not others.\n",
            " |      \n",
            " |      .. versionadded:: 1.3\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method is only relevant if this estimator is used as a\n",
            " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      raw_documents : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            " |          Metadata routing for ``raw_documents`` parameter in ``transform``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          The updated object.\n",
            " |  \n",
            " |  transform(self, raw_documents)\n",
            " |      Transform documents to document-term matrix.\n",
            " |      \n",
            " |      Extract token counts out of raw text documents using the vocabulary\n",
            " |      fitted with fit or the one provided to the constructor.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      raw_documents : iterable\n",
            " |          An iterable which generates either str, unicode or file objects.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X : sparse matrix of shape (n_samples, n_features)\n",
            " |          Document-term matrix.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from _VectorizerMixin:\n",
            " |  \n",
            " |  build_analyzer(self)\n",
            " |      Return a callable to process input data.\n",
            " |      \n",
            " |      The callable handles preprocessing, tokenization, and n-grams generation.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      analyzer: callable\n",
            " |          A function to handle preprocessing, tokenization\n",
            " |          and n-grams generation.\n",
            " |  \n",
            " |  build_preprocessor(self)\n",
            " |      Return a function to preprocess the text before tokenization.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      preprocessor: callable\n",
            " |            A function to preprocess the text before tokenization.\n",
            " |  \n",
            " |  build_tokenizer(self)\n",
            " |      Return a function that splits a string into a sequence of tokens.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      tokenizer: callable\n",
            " |            A function to split a string into a sequence of tokens.\n",
            " |  \n",
            " |  decode(self, doc)\n",
            " |      Decode the input into a string of unicode symbols.\n",
            " |      \n",
            " |      The decoding strategy depends on the vectorizer parameters.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      doc : bytes or str\n",
            " |          The string to decode.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      doc: str\n",
            " |          A string of unicode symbols.\n",
            " |  \n",
            " |  get_stop_words(self)\n",
            " |      Build or fetch the effective stop words list.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      stop_words: list or None\n",
            " |              A list of stop words.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from _VectorizerMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  __sklearn_clone__(self)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : dict\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
            " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
            " |      possible to update each component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : estimator instance\n",
            " |          Estimator instance.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
            " |  \n",
            " |  get_metadata_routing(self)\n",
            " |      Get metadata routing of this object.\n",
            " |      \n",
            " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
            " |      mechanism works.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      routing : MetadataRequest\n",
            " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
            " |          routing information.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
            " |  \n",
            " |  __init_subclass__(**kwargs) from builtins.type\n",
            " |      Set the ``set_{method}_request`` methods.\n",
            " |      \n",
            " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
            " |      looks for the information available in the set default values which are\n",
            " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
            " |      from method signatures.\n",
            " |      \n",
            " |      The ``__metadata_request__*`` class attributes are used when a method\n",
            " |      does not explicitly accept a metadata through its arguments or if the\n",
            " |      developer would like to specify a request value for those metadata\n",
            " |      which are different from the default ``None``.\n",
            " |      \n",
            " |      References\n",
            " |      ----------\n",
            " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "help(CountVectorizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "o6i6x1jL8LA3"
      },
      "outputs": [],
      "source": [
        "# as we are trying to use regression, which requires training, we will split the dataset into train and test.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_X, test_X, train_y, test_y = train_test_split(df[\"text\"],df[\"label_num\"], test_size=0.2, random_state=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVFHht4nBt_C",
        "outputId": "d4ff2535-e7ef-4755-a702-30372cf0e66c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4136.8"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df)*0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkNLmruHA4dU",
        "outputId": "da5e4185-354d-44eb-a67d-b68c0d3a4208"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4164    Subject: enron / hpl actuals for sept . 11 , 2...\n",
              "113     Subject: mitchell gas services 2 / 00\\r\\n- - -...\n",
              "2697    Subject: jan . 01 sale to texas general land o...\n",
              "737     Subject: fw : crosstex energy , driscoll ranch...\n",
              "4214    Subject: no more anxiety ! valium % xanax are ...\n",
              "                              ...                        \n",
              "1180    Subject: re : may activity survey\\r\\nthanks , ...\n",
              "3441    Subject: hi paliourg all available meds . avai...\n",
              "1344    Subject: abazis @ iit . demokritos . gr : new ...\n",
              "4623    Subject: re : flow volumes at oxy gladewater ,...\n",
              "1289    Subject: calpine daily gas nomination\\r\\n>\\r\\n...\n",
              "Name: text, Length: 4136, dtype: object"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL3RZnkrA78U",
        "outputId": "15c0558d-edf6-4d3a-9c18-6fea34d31f65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4164    0\n",
              "113     0\n",
              "2697    0\n",
              "737     0\n",
              "4214    1\n",
              "       ..\n",
              "1180    0\n",
              "3441    1\n",
              "1344    1\n",
              "4623    0\n",
              "1289    0\n",
              "Name: label_num, Length: 4136, dtype: int64"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_y # spam, non spam label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "QGP5fhN28MGG"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(stop_words='english', lowercase=True) # stop_words : 불용어 제거 언어 선택, lowercase : 대소문자 구분 없이"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb-yZD_ULSs9",
        "outputId": "7e001c32-2184-4e11-84e1-68d98071538c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['213423' '243' 'f24fsf245' 'fsdf' 'sdfsdf']\n"
          ]
        }
      ],
      "source": [
        "vectorizer.fit([\"fsdf 213423 sdfsdf 243 f24fsf245\"])\n",
        "print(vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihHhohST8M6x",
        "outputId": "45f5dd38-b42d-4d9f-b7c3-d6f27217c671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['00' '000' '0000' '000000' '000000000002858' '000080' '0001' '00018'\n",
            " '00020608' '0004' '0005' '0008' '001' '0010' '0012' '002' '0022' '00221'\n",
            " '0025' '0027']\n",
            "['zxgwvpiadobe' 'zxgwvpihere' 'zxgwvpiimg' 'zxgwvpimacromedia'\n",
            " 'zxgwvpimicrosoft' 'zxgwvpinorton' 'zxjcxz' 'zxklh' 'zxzmcnbf' 'zyban'\n",
            " 'zykfe' 'zyl' 'zynve' 'zyrtec' 'zyyqywp' 'zzn' 'zzo' 'zzocb' 'zzso'\n",
            " 'zzsyt']\n"
          ]
        }
      ],
      "source": [
        "# build vocabulary with training data and see what's in it.\n",
        "vectorizer.fit(train_X)\n",
        "print(vectorizer.get_feature_names_out()[0:20]) # get_feature_names_out()를 사용하면 CountVectorizer가 텍스트 데이터에서 추출한 고유 단어들의 목록 추출\n",
        "print(vectorizer.get_feature_names_out()[-20:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekc4oebACJi7",
        "outputId": "9d49490c-ea21-4bea-ca89-253c1e0336d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "44268"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "KRJo4YTp8NxJ"
      },
      "outputs": [],
      "source": [
        "# vecotrize both train and test with the fit vectorizer\n",
        "train_X_vector = vectorizer.transform(train_X)\n",
        "test_X_vector = vectorizer.transform(test_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr27Yq3n8OlC",
        "outputId": "92e8c99f-4327-4601-c0a4-780c82bf1c04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4164    Subject: enron / hpl actuals for sept . 11 , 2...\n",
            "113     Subject: mitchell gas services 2 / 00\\r\\n- - -...\n",
            "2697    Subject: jan . 01 sale to texas general land o...\n",
            "737     Subject: fw : crosstex energy , driscoll ranch...\n",
            "4214    Subject: no more anxiety ! valium % xanax are ...\n",
            "                              ...                        \n",
            "1180    Subject: re : may activity survey\\r\\nthanks , ...\n",
            "3441    Subject: hi paliourg all available meds . avai...\n",
            "1344    Subject: abazis @ iit . demokritos . gr : new ...\n",
            "4623    Subject: re : flow volumes at oxy gladewater ,...\n",
            "1289    Subject: calpine daily gas nomination\\r\\n>\\r\\n...\n",
            "Name: text, Length: 4136, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(train_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XHgsr-RNU4D",
        "outputId": "be6a8e93-94a9-4eb0-c692-208b6180b73c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subject: enron / hpl actuals for sept . 11 , 2000\n",
            "teco tap 25 . 000 / enron ; 125 . 000 / hpl gas daily\n",
            "ls hpl lsk ic 15 . 000 / enron\n",
            "327\n"
          ]
        }
      ],
      "source": [
        "print(train_X[4164])\n",
        "print(len(train_X[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4omI1lOSP2oj",
        "outputId": "a83f6b6e-3506-49d0-c802-0465cde52683"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['document' 'new' 'second']\n",
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 8 stored elements and shape (6, 3)>\n",
            "  Coords\tValues\n",
            "  (0, 0)\t1\n",
            "  (0, 1)\t1\n",
            "  (1, 0)\t1\n",
            "  (2, 0)\t2\n",
            "  (2, 2)\t1\n",
            "  (4, 0)\t1\n",
            "  (5, 0)\t1\n",
            "  (5, 1)\t1\n",
            "[[1 1 0]\n",
            " [1 0 0]\n",
            " [2 0 1]\n",
            " [0 0 0]\n",
            " [1 0 0]\n",
            " [1 1 0]]\n"
          ]
        }
      ],
      "source": [
        "# 예시\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 예시 데이터\n",
        "ex_x = [\"This is a new document.\", \"This is the first document.\", \"This document is the second document.\", \"And this is the third one.\", \"Is this the first document?\", \"This is a new document.\"]\n",
        "\n",
        "# CountVectorizer 객체 생성 및 학습\n",
        "exvectorizer = CountVectorizer(stop_words='english', lowercase=True)\n",
        "exvectorizer.fit(ex_x)\n",
        "\n",
        "# 훈련 데이터 벡터화\n",
        "ex_x_vector = exvectorizer.transform(ex_x)\n",
        "print(exvectorizer.get_feature_names_out())\n",
        "print(ex_x_vector) # 단어 등장횟수를 벡터로\n",
        "print(ex_x_vector.toarray()) # row : 문장, col : courup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0z074dn8PXE",
        "outputId": "4a90e747-b9c7-4d73-e495-1be62c93864f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 18 stored elements and shape (1, 44268)>\n",
            "  Coords\tValues\n",
            "  (0, 1)\t3\n",
            "  (0, 341)\t1\n",
            "  (0, 438)\t1\n",
            "  (0, 689)\t1\n",
            "  (0, 967)\t1\n",
            "  (0, 1222)\t1\n",
            "  (0, 4523)\t1\n",
            "  (0, 13473)\t1\n",
            "  (0, 16671)\t3\n",
            "  (0, 19293)\t1\n",
            "  (0, 21717)\t3\n",
            "  (0, 22059)\t1\n",
            "  (0, 26206)\t1\n",
            "  (0, 26214)\t1\n",
            "  (0, 36336)\t1\n",
            "  (0, 38409)\t1\n",
            "  (0, 39106)\t1\n",
            "  (0, 39271)\t1\n",
            "[0, 3, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "print(train_X_vector[0])\n",
        "print(train_X_vector[0].toarray().tolist()[0][:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJEwptPq8Q_V",
        "outputId": "f6363c94-4bb1-4d8e-cffc-a0e218ebaebe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array(['00', '000'], dtype='<U24')]"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorizer.inverse_transform([[1, 30]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "5QP5KbDm8qqt",
        "outputId": "9123881b-6a05-44e8-bac1-90b1639a2a05"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
            ],
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# we are going to use simple model.\n",
        "# regression model draws a line based on the number of words being mentioned in the mail.\n",
        "\n",
        "# build a logistic regression model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(train_X_vector, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wztAIGpl8rsi",
        "outputId": "61fdf165-5d51-442c-a9cf-6fdbbecb8536"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 0, 0])"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred_train = model.predict(train_X_vector)\n",
        "y_pred_test = model.predict(test_X_vector)\n",
        "\n",
        "y_pred_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d8CGMtR8sc7",
        "outputId": "36267a1a-9248-4513-9d40-5c599d5904e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      2931\n",
            "           1       1.00      1.00      1.00      1205\n",
            "\n",
            "    accuracy                           1.00      4136\n",
            "   macro avg       1.00      1.00      1.00      4136\n",
            "weighted avg       1.00      1.00      1.00      4136\n",
            "\n",
            "[[2930    1]\n",
            " [   1 1204]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# training dataset score\n",
        "print(classification_report(y_pred_train, train_y))\n",
        "print(confusion_matrix(y_pred_train, train_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QP_LWghb8tNS",
        "outputId": "a49d3c62-e1dc-4c0a-f5c7-9b97cefdd184"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99       730\n",
            "           1       0.99      0.95      0.97       305\n",
            "\n",
            "    accuracy                           0.98      1035\n",
            "   macro avg       0.98      0.97      0.98      1035\n",
            "weighted avg       0.98      0.98      0.98      1035\n",
            "\n",
            "[[726   4]\n",
            " [ 15 290]]\n"
          ]
        }
      ],
      "source": [
        "# test dataset score\n",
        "print(classification_report(y_pred_test, test_y))\n",
        "print(confusion_matrix(y_pred_test, test_y))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
